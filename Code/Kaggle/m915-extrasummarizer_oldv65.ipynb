{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from heapq import heapify\nimport os\nimport json\nfrom typing import Counter\nimport pandas as pd\nimport re\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pickle\n\nimport math\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer, PorterStemmer, SnowballStemmer, LancasterStemmer\nfrom nltk.tokenize import sent_tokenize, word_tokenize\nfrom nltk import pos_tag\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport sklearn.model_selection\nfrom sklearn.model_selection import RepeatedKFold, cross_val_score, GridSearchCV\nimport sklearn.preprocessing as preproc\nfrom sklearn.feature_extraction import text\nfrom sklearn.svm import LinearSVR\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression, ElasticNet, ElasticNetCV, LassoLars, SGDRegressor\nfrom sklearn.metrics import confusion_matrix, mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, MaxAbsScaler\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.svm import SVR\n\nfrom IPython.display import FileLink, FileLinks\n\nfrom tqdm import tqdm\n\n!pip install rouge-score\nfrom rouge_score import rouge_scorer\n\nimport psutil\nimport gc\nfrom collections import Counter","metadata":{"execution":{"iopub.status.busy":"2022-08-03T20:27:07.556235Z","iopub.execute_input":"2022-08-03T20:27:07.556618Z","iopub.status.idle":"2022-08-03T20:27:24.312229Z","shell.execute_reply.started":"2022-08-03T20:27:07.556586Z","shell.execute_reply":"2022-08-03T20:27:24.310461Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"\ndef json_to_df_oldv(json_path,data_type):\n    with open(json_path, \"r\", encoding=\"utf-8\") as f: \n        lines = [eval(l) for l in f.readlines()]\n\n    # exclude lines with surrogates in their text/summary\n    surr = [ i for i,l in enumerate(lines) for k in l.keys() if k in [\"text\",\"summary\"] and re.search(r'[\\uD800-\\uDFFF]', l[k])!=None ]\n\n    lines = [ l for i,l in zip( range(len(lines)),lines ) if i not in surr ]\n\n    cols=[ \"title\",\t\"date\",\t\"text\",\t\"summary\", \"compression\", \"coverage\", \"density\", \"compression_bin\", \"coverage_bin\"]\n\n    # we need only the extractive summaries as we are building an extractive summarizer\n    data=[ [ l[k] for k in l.keys() if k in cols ] for l in lines if l[\"density_bin\"]==\"extractive\" ]\n    df = pd.DataFrame(data,columns=cols)\n\n    df.to_csv(f\"..{os.sep}Data{os.sep}DataFrames{os.sep}{data_type}_set.csv\", header=True, index=False )\n\n    return df\n\ndef json_to_df(json_path,data_type):\n    data=[]\n    for ln in open(json_path,\"r\"):\n        obj = json.loads(ln)\n        data.append(obj)\n    df=pd.DataFrame(data)\n    \n    cols=[ \"title\",\t\"date\",\t\"text\",\t\"summary\", \"compression\", \"coverage\", \"density\", \"compression_bin\", \"coverage_bin\"]\n    df=df.loc[df.density_bin==\"extractive\"].reset_index()\n    \n    df.to_csv(f\"/kaggle/working/Data{os.sep}DataFrames{os.sep}{data_type}_set.csv\", header=True, index=False )\n    df[\"summary\"].to_csv(f\"/kaggle/working/Data{os.sep}DataFrames{os.sep}{data_type}_summaries.csv\", header=True, index=False )\n    \n    return df\n\n\n# text processing functions\n\n# A list of contractions from http://stackoverflow.com/questions/19790188/expanding-english-language-contractions-in-python\ncontractions = { \"ain't\": \"am not\", \"aren't\": \"are not\", \"can't\": \"cannot\", \"can't've\": \"cannot have\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \"couldn't've\": \"could not have\", \"didn't\": \"did not\", \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hadn't've\": \"had not have\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\", \"he'd've\": \"he would have\", \"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'll\": \"how will\", \"how's\": \"how is\", \"i'd\": \"i would\", \"i'll\": \"i will\", \"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'll\": \"it will\", \"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\", \"mightn't\": \"might not\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"needn't\": \"need not\", \"oughtn't\": \"ought not\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"she'd\": \"she would\", \"she'll\": \"she will\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"that'd\": \"that would\", \"that's\": \"that is\", \"there'd\": \"there had\", \"there's\": \"there is\", \"they'd\": \"they would\", \"they'll\": \"they will\", \"they're\": \"they are\", \"they've\": \"they have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'll\": \"we will\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what're\": \"what are\", \"what's\": \"what is\", \"what've\": \"what have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"who'll\": \"who will\", \"who's\": \"who is\", \"won't\": \"will not\", \"wouldn't\": \"would not\", \"you'd\": \"you would\", \"you'll\": \"you will\", \"you're\": \"you are\" }\n\n\ndef sentence_cleaning(text, remove_stopwords = True, sub_contractions=True, stemming=True):\n    global pbar_cleaning\n    pbar_cleaning.update(1)\n    \n    # Convert words to lower case\n    text = text.lower()\n    toks = word_tokenize(text)\n      \n    # Replace contractions with their longer forms \n    if sub_contractions:\n        new_text = []\n        for word in toks:\n            if word in contractions:\n                new_text.append(contractions[word])\n            else:\n                new_text.append(word)\n    \n    text = \" \".join(new_text)\n\n    # Format words and remove unwanted characters\n    text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n    text = re.sub(r'\\<a href', ' ', text)\n    text = re.sub(r'&amp;', '', text) \n    text = re.sub(r'[_\"\\-;%()|+&=*%.,!?:#$@\\[\\]/]', ' ', text)\n    text = re.sub(r'<br />', ' ', text)\n    text = re.sub(r'\\'', ' ', text)\n\n    toks_clean = word_tokenize(text)\n    \n    if remove_stopwords:\n        stops = set(stopwords.words(\"english\"))\n        toks_clean = [w for w in toks_clean if not w in stops]\n\n    \n    if stemming: \n        stemmer=SnowballStemmer(language=\"english\")\n        toks_clean=[ stemmer.stem(w) for w in toks_clean ]\n\n    text = \" \".join(toks_clean)\n    \n    return text, toks_clean\n\n\ndef rouge_scoring(sentence,summary,sc_type=\"rougeL\",score=\"fmeasure\"):\n    global pbar\n    pbar.update(1)\n    r_scorer=rouge_scorer.RougeScorer([\"rouge1\",\"rouge2\",\"rougeL\"])\n    r_L=r_scorer.score(summary,sentence)\n    score_ind={\"precision\":0, \"recall\":1, \"fmeasure\":2}\n    \n    return r_L[sc_type][score_ind[score]]\n\n\ndef text_processing(df,data_type,df_dir,sc_type=\"rougeL\"):\n    global pbar\n    cols=[\"sentence\", \"summary\", \"text\", \"text_id\"] \n    \n    df[\"summary\"].to_csv(os.path.join(df_dir,f\"{data_type}_data_{sc_type}_summaries_grouped.csv\"), header=True, index=False)\n#     print(f\"Summary df len: {df['summary'].shape}\")\n    # new_df=pd.DataFrame()\n\n    # sentence split \n    sentences=[ sent_tokenize(t) for t in df[\"text\"].values ]\n#     print(f\"Sentence Len: {len(sentences)}\")\n\n    summaries=df[\"summary\"].values\n    sent_sum_text=[ [ s,summary,t,i  ] for i, (s_list,summary,t) in enumerate(zip( sentences, summaries, df[\"text\"] )) for s in s_list ]\n    new_df=pd.DataFrame(sent_sum_text, columns=cols)\n#     new_df[\"text_id\"]=new_df[\"text\"].factorize()[0]\n    \n#     print(len(sent_sum_text))\n#     print(set(new_df[\"text_id\"].values), set(new_df[\"t_id\"].values))\n#     print(new_df[\"text_id\"].values[-1])\n    \n    new_df[\"chosen\"]= 0\n    ind = new_df[[ s in t for s,t in zip( new_df[\"sentence\"], new_df[\"summary\"] ) ]].index\n    new_df.loc[ind,\"chosen\"]=1\n    del new_df[\"text\"]\n    # for c in new_df.columns:\n    #   new_df[c]=new_df[c].astype(str)\n\n    # labels\n    # columns -> sentence: 0, summary: 1, text: 2\n    pbar = tqdm(total=new_df.shape[0] )\n    new_df[\"rougeL\"]= new_df.apply(lambda row: rouge_scoring(row[\"sentence\"],row[\"summary\"], sc_type=sc_type, score=\"fmeasure\" ), axis=1)\n#     print(new_df[\"rougeL\"])\n\n    new_df[\"summary\"].to_csv(os.path.join(df_dir,f\"{data_type}_data_{sc_type}_summaries.csv\"), header=True, index=False)\n    del new_df[\"summary\"]\n    \n    new_df.to_csv(os.path.join(df_dir,f\"{data_type}_data_{sc_type}.csv\"), header=True, index=False)\n\n    return new_df\n\n\ndef add_chosen_text_id(df):\n    cols=[\"sentence\", \"summary\", \"text\"] \n    # new_df=pd.DataFrame()\n\n    # sentence split \n    sentences=[ sent_tokenize(t) for t in df[\"text\"].values ]\n    summaries=df[\"summary\"].values\n    \n    sent_sum_text=[ [ s,summary,t  ] for s_list,summary,t in zip( sentences, summaries, df[\"text\"] ) for s in s_list  ]\n    new_df=pd.DataFrame(sent_sum_text, columns=cols)\n    \n    new_df[\"text_id\"]=new_df[\"text\"].factorize()[0]\n    new_df[\"chosen\"]= 0\n    ind = new_df[[ s in t for s,t in zip( new_df[\"sentence\"], new_df[\"summary\"] ) ]].index\n    new_df.loc[ind,\"chosen\"]=1\n    \n    del new_df[\"text\"]\n    del new_df[\"summary\"]\n    del new_df[\"sentence\"]\n    \n    return new_df\n\n\ndef text_proc_labels(data_dir,df_dir=\"/kaggle/input/summarizer-data/\"):\n#     train_df = pd.read_csv(os.path.join(df_dir,\"train_set.csv\"))\n    dev_df = pd.read_csv(os.path.join(df_dir,\"dev_set.csv\"))\n    test_df = pd.read_csv(os.path.join(df_dir,\"test_set.csv\"))\n\n#     train_df1, train_df2, train_df3, train_df4 = np.array_split(train_df, 4)\n    splits=4\n#     train_data_list=[]\n#     for i,train_df in enumerate(np.array_split(train_df, splits)):\n#         train_data = text_processing(train_df,f\"train{i+1}\",data_dir,\"rougeL\")\n#         train_data_list.append(train_data)\n    \n    test_data = text_processing(test_df,\"test\",data_dir,\"rougeL\")\n    dev_data = text_processing(dev_df,\"dev\",data_dir,\"rougeL\")\n    \n    \n    return train_data_list, dev_data, test_data\n\n\ndef df_add_tid(data_dir, df_dir):\n    sc_type=\"rougeL\"\n    data_labels_dir=\"/kaggle/input/summarizer-data\"\n\n#     data_type=\"train\"\n#     train_df = pd.read_csv(os.path.join(df_dir,f\"{data_type}_set.csv\"))\n#     splits=4\n#     for i,df in enumerate(np.array_split(train_df, 4)):\n#         if i==2:\n#             data_type=f\"train{i+1}\"\n#             print(data_type+\"\\n\")\n#             train_ch_tid_df=add_chosen_text_id(df)\n#             train_rougeL=pd.read_csv(os.path.join(data_labels_dir,f\"{data_type}_data_{sc_type}.csv\"))\n#             print(train_rougeL.columns, train_rougeL.shape)\n#             train_rougeL[\"text_id\"]=train_ch_tid_df[\"text_id\"]\n#             train_rougeL[\"chosen\"]=train_ch_tid_df[\"chosen\"]\n#             print(train_rougeL.columns, train_rougeL.shape)\n#             train_rougeL.to_csv(os.path.join(data_dir,f\"{data_type}_data_{sc_type}_tid.csv\"), header=True, index=False)\n\n    data_type=\"dev\" \n    dev_df = pd.read_csv(os.path.join(df_dir,f\"{data_type}_set.csv\"))\n    dev_ch_tid_df=add_chosen_text_id(dev_df)\n    dev_rougeL=pd.read_csv(os.path.join(data_labels_dir,f\"{data_type}_data_{sc_type}.csv\"))\n    print(dev_rougeL.columns, dev_rougeL.shape)\n    dev_rougeL[\"text_id\"]=dev_ch_tid_df[\"text_id\"]\n    dev_rougeL[\"chosen\"]=dev_ch_tid_df[\"chosen\"]\n    print(dev_rougeL.columns, dev_rougeL.shape)\n    dev_rougeL.to_csv(os.path.join(data_dir,f\"{data_type}_data_{sc_type}_tid.csv\"), header=True, index=False)\n\n    data_type=\"test\"\n    test_df = pd.read_csv(os.path.join(df_dir,f\"{data_type}_set.csv\"))\n    test_ch_tid_df=add_chosen_text_id(test_df)\n    test_rougeL=pd.read_csv(os.path.join(data_labels_dir,f\"{data_type}_data_{sc_type}.csv\"))\n    print(test_rougeL.columns, test_rougeL.shape)\n    test_rougeL[\"text_id\"]=test_ch_tid_df[\"text_id\"]\n    test_rougeL[\"chosen\"]=test_ch_tid_df[\"chosen\"]\n    print(test_rougeL.columns, test_rougeL.shape)\n    test_rougeL.to_csv(os.path.join(data_dir,f\"{data_type}_data_{sc_type}_tid.csv\"), header=True, index=False)\n\n    \ndef data_stats(df):\n    groups = df.groupby(\"chosen\")\n    print(f\"DataFrame shape: {df.shape}\\n\\n\")\n    print(groups.describe()[\"rougeL\"])\n\n    \ndef thematic_ratio(them_words, word_list):\n    them_occ= sum( [ word_list.count(w) for w in set(them_words)&set(word_list)])\n    them_ratio=them_occ/len(word_list)\n    return them_ratio\n\n\ndef s_position(t_position,tot_sent):\n    N=tot_sent\n    th=0.2*N\n    min_p= th*N\n    max_p= 2*th*N\n    \n    if t_position==tot_sent or t_position==1:\n        pos=1.0\n    else: \n        pos=math.cos((t_position - min_p)*((1/max_p) - min_p))\n        \n    return pos\n    \n    \ndef prop_nouns(tokens):\n    if type(tokens)!=list:\n        tokens=eval(tokens)\n    pos= nltk.pos_tag(tokens)\n    tags_count=Counter(tag for _, tag in pos if tag==\"NNP\" or tag==\"NNPS\")\n    return tags_count[\"NNP\"]+tags_count[\"NNPS\"] \n\n\ndef feature_df(df, data_dir, data_type):\n    global pbar_cleaning\n    feat_df=pd.DataFrame()\n    \n    # tokenize\n    pbar_cleaning=tqdm(total=train_data.shape[0], leave=True)\n    df[\"tokens\"] = df[\"sentence\"].apply(lambda x: sentence_cleaning(x)[1]) \n    \n\n    # create sentence features\n    # 1. thematic words\n    col = df.groupby(\"text_id\")[\"tokens\"].apply(sum)\n    thematic_cols= pd.DataFrame({\"text_id\": col.index, \"thematic\": [ [ t[0] for t in Counter(x).most_common(10) ] for x in col ]})  \n    df=df.join(thematic_cols[\"thematic\"], on='text_id' )\n    feat_df[\"thematic_ratio\"] = df.apply(lambda row: thematic_ratio(row.thematic, row.tokens) if len(row.tokens)>0 else 0.0, axis=1)\n\n    # 2. sentence position in the text\n    feat_df[\"text_position\"] = df.groupby(\"text_id\").cumcount().add(1)\n    df[\"tot_sent\"] = df.groupby(\"text_id\")[\"sentence\"].transform(len)\n    feat_df[\"s_position\"] = df.apply(lambda row: s_position(row.text_position,row.tot_sent), axis=1)\n\n    # 3. sentence length - threshold=3\n    threshold=3\n    feat_df[\"len\"]= df[\"tokens\"].apply(lambda x: 0 if len(x)<threshold else len(x))\n\n    # 4. sentence position - paragraph relative\n    feat_df['s_pos_par'] = feat_df[\"s_position\"].values\n    feat_df.loc[feat_df.s_pos_par!=1.0, 's_pos_par']=0.0\n\n    # 5. numerals\n    feat_df[\"num_ratio\"]=df[\"tokens\"].apply(lambda x: sum( [ 1 for t in x if t.isnumeric() ] )/len(x) if len(x)>0 else 0 )\n\n\n    # ?. Term Frequency-Inverse Sentence Frequency\n\n\n    # ?. proper nouns - not so useful\n    # train_data_feats[\"NNPs\"]=train_data_feats[\"tokens\"].apply(lambda x: prop_nouns(x) )\n\n\n\n\n    # train_data_feats=train_data[[\"len\",\"text_position\"]]\n    feat_df.to_csv(os.path.join(data_dir,f\"{data_type}_set_feats.csv\"), header=True, index=False)\n\n    return feat_df\n\n\n\n# SVM_scaler =  StandardScaler()\n# LR_scaler =  MinMaxScaler()\n# KNN_scaler =  StandardScaler()\n# # classifier parameters\n# KNN_n_num = 9\n# LR_C = 1.0\n# SVM_C = 0.001\n# algos={\n#   \"ElNet\": make_pipeline(SVM_scaler, SVC(C=SVM_C)),\n#   \"LR\":  make_pipeline(LR_scaler, LogisticRegression(C=LR_C)),\n#   \"KNN\": make_pipeline(KNN_scaler, KNeighborsClassifier(n_neighbors = KNN_n_num)),\n# }\n\n\ndef classifier_training(model,X_train,y_train):\n    model.fit(X_train,y_train)\n    preds=model.predict(X_train)\n    c_rep=classification_report(y_train,preds)\n    c_rep_dict=classification_report(y_train,preds,output_dict=True)\n    return model, c_rep, c_rep_dict\n\ndef classifier_validation(model,X_dev,y_dev):\n    preds=model.predict(X_dev)\n    c_rep = classification_report(y_dev,preds)\n    c_rep_dict=classification_report(y_dev,preds,output_dict=True)\n    return c_rep, c_rep_dict\n\ndef classifier_test(model,X_test,y_test):\n    preds=model.predict(X_test)\n    c_rep = classification_report(y_test,preds)\n    c_rep_dict=classification_report(y_test,preds,output_dict=True)\n    return c_rep, c_rep_dict\n\n\n# Training - Validation - Test pipeline\ndef classifier_T_V_T(X_train, y_train, X_dev, y_dev, X_test, y_test, algo_type=\"LR\"):\n    model=algos[algo_type]\n\n    model,c_rep_train,c_rep_dict_train=classifier_training(model,X_train,y_train)\n    c_rep_dev,c_rep_dict_dev=classifier_validation(model,X_dev,y_dev)\n    c_rep_test,c_rep_dict_test=classifier_validation(model,X_test,y_test)\n\n    return model, c_rep_train, c_rep_dict_train, c_rep_dev, c_rep_dict_dev, c_rep_test, c_rep_dict_test\n\n","metadata":{"execution":{"iopub.status.busy":"2022-08-03T20:46:39.007129Z","iopub.execute_input":"2022-08-03T20:46:39.008935Z","iopub.status.idle":"2022-08-03T20:46:39.151432Z","shell.execute_reply.started":"2022-08-03T20:46:39.008681Z","shell.execute_reply":"2022-08-03T20:46:39.141640Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"# memory clean\n# for v in globals():\n#     print(v)\n#     if str(v) not \"__name__\":\n#         del v\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-08-03T20:27:24.387119Z","iopub.execute_input":"2022-08-03T20:27:24.388196Z","iopub.status.idle":"2022-08-03T20:27:24.556020Z","shell.execute_reply.started":"2022-08-03T20:27:24.388150Z","shell.execute_reply":"2022-08-03T20:27:24.555185Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"df_dir=\"/kaggle/input/summarizer-data\"\n\ndata_dir= \"/kaggle/working/Data/DataFrames\"\nif not os.path.exists(data_dir):\n    os.makedirs(data_dir)\n    \n# test_json_path=\"/kaggle/input/summarizer-data/test.jsonl\"\n# json_to_df(test_json_path,\"test\")\n\n# dev_json_path=\"/kaggle/input/summarizer-data/dev.jsonl\"\n# json_to_df(dev_json_path,\"dev\")\n\n# load saved dataframes and create labels\n# train_data_list, dev_data, test_data = text_proc_labels(data_dir)\n\n# load dataframes with labels\n# df_add_tid(data_dir, df_dir)\n\n\nFileLinks(\".\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-output":false,"scrolled":true,"execution":{"iopub.status.busy":"2022-08-03T20:27:24.557981Z","iopub.execute_input":"2022-08-03T20:27:24.558915Z","iopub.status.idle":"2022-08-03T20:27:24.573702Z","shell.execute_reply.started":"2022-08-03T20:27:24.558875Z","shell.execute_reply":"2022-08-03T20:27:24.572611Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# # load training dataset\n# sc_type=\"rougeL\"\n# data_type=\"train1\"\n# train_data=pd.read_csv(os.path.join(df_dir,f\"{data_type}_data_{sc_type}_tid.csv\"))\n# data_stats(train_data)\n# labels=train_data[\"rougeL\"].values\n# del train_data[\"rougeL\"]\n\n\n# sc_type=\"rougeL\"\n# data_type=\"dev\"\n# dev_data=pd.read_csv(os.path.join(df_dir,f\"{data_type}_data_{sc_type}_tid.csv\"))\n# data_stats(dev_data)\n# labels_dev=dev_data[\"rougeL\"].values\n# del dev_data[\"rougeL\"]\n\n\n# sc_type=\"rougeL\"\n# data_type=\"test\"\n# test_data=pd.read_csv(os.path.join(df_dir,f\"{data_type}_data_{sc_type}_tid.csv\"))\n# data_stats(test_data)\n# labels_test=test_data[\"rougeL\"].values\n# del test_data[\"rougeL\"]","metadata":{"execution":{"iopub.status.busy":"2022-08-03T20:27:24.575396Z","iopub.execute_input":"2022-08-03T20:27:24.576320Z","iopub.status.idle":"2022-08-03T20:27:24.581716Z","shell.execute_reply.started":"2022-08-03T20:27:24.576272Z","shell.execute_reply":"2022-08-03T20:27:24.580800Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"def load_datasets(df_dir, sc_type=\"rougeL\"):\n    # train_data_feats=pd.read_csv(\"/kaggle/input/summarizer-data/train1_set_feats.csv\")\n    data_type=\"train1\"\n    print(f\"\\nLoading {data_type.capitalize()} Data . . \")\n    # train_data=feature_df(train_data, data_dir, data_type)\n    train_data=pd.read_csv(f\"/kaggle/input/summarizer-data/{data_type}_set_feats.csv\") \n    \n    train_data_rougeLTid=pd.read_csv(os.path.join(df_dir,f\"{data_type}_data_{sc_type}_tid.csv\"))\n    train_data_rougeLTid[\"text_position\"] = train_data_rougeLTid.groupby(\"text_id\").cumcount().add(1)\n    \n    data_stats(train_data_rougeLTid)\n    labels_train=train_data_rougeLTid[\"rougeL\"].values\n#     del train_data[\"rougeL\"]\n    \n    train_data_feats=train_data\n    train_data[\"sentence\"]=train_data_rougeLTid[\"sentence\"]\n\n    train_data_feats=train_data_feats[[\"len\", \"s_position\", \"thematic_ratio\", \"s_pos_par\", \"num_ratio\"]]\n\n    # lens= StandardScaler().fit_transform(np.array(train_data_feats[\"len\"]).reshape(-1,1) )\n    # train_data_feats[\"len\"]=lens\n    # train_data_feats\n\n\n    data_type=\"dev\"\n    print(f\"\\n\\nLoading {data_type.capitalize()} Data . . \")\n    # dev_data=feature_df(dev_data, data_dir, data_type)\n    dev_data=pd.read_csv(f\"/kaggle/input/summarizer-data/{data_type}_set_feats.csv\") \n    \n    dev_data_rougeLTid=pd.read_csv(os.path.join(df_dir,f\"{data_type}_data_{sc_type}.csv\"))\n    dev_data_rougeLTid[\"text_position\"] = dev_data_rougeLTid.groupby(\"text_id\").cumcount().add(1)\n    \n    data_stats(dev_data_rougeLTid)\n    labels_dev=dev_data_rougeLTid[\"rougeL\"].values\n    \n    dev_data_feats=dev_data\n    dev_data[\"sentence\"]=dev_data_rougeLTid[\"sentence\"]\n    dev_data_feats=dev_data_feats[[\"len\", \"s_position\", \"thematic_ratio\", \"s_pos_par\", \"num_ratio\"]]\n\n\n    data_type=\"test\"\n    print(f\"\\n\\nLoading {data_type.capitalize()} Data . . \")\n    # test_data=feature_df(test_data, data_dir, data_type)\n    test_data=pd.read_csv(f\"/kaggle/input/summarizer-data/{data_type}_set_feats.csv\") \n    \n    test_data_rougeLTid=pd.read_csv(os.path.join(df_dir,f\"{data_type}_data_{sc_type}.csv\"))\n    test_data_rougeLTid[\"text_position\"] = test_data_rougeLTid.groupby(\"text_id\").cumcount().add(1)\n    \n    data_stats(test_data_rougeLTid)\n    labels_test=test_data_rougeLTid[\"rougeL\"].values\n    \n    test_data_feats=test_data\n    test_data[\"sentence\"]=test_data_rougeLTid[\"sentence\"]\n    test_data_feats=test_data_feats[[\"len\", \"s_position\", \"thematic_ratio\", \"s_pos_par\", \"num_ratio\"]]\n    \n    return train_data_rougeLTid, train_data_feats, labels_train, dev_data_rougeLTid, dev_data_feats, labels_dev, test_data_rougeLTid, test_data_feats, labels_test\n\n\ndef load_dataset_test(df_dir, sc_type=\"rougeL\"):\n    \n    data_type=\"test\"\n    print(f\"\\nLoading {data_type.capitalize()} Data . . \")\n    # test_data=feature_df(test_data, data_dir, data_type)\n    test_data=pd.read_csv(f\"/kaggle/input/summarizer-data/{data_type}_set_feats.csv\") \n    \n\n    test_data_rougeLTid=pd.read_csv(os.path.join(df_dir,f\"{data_type}_data_{sc_type}.csv\"))\n    test_data_rougeLTid[\"text_position\"] = test_data_rougeLTid.groupby(\"text_id\").cumcount().add(1)\n                                                 \n    data_stats(test_data_rougeLTid)\n    labels_test=test_data_rougeLTid[\"rougeL\"].values\n    \n    test_data_feats=test_data\n    test_data[\"sentence\"]=test_data_rougeLTid[\"sentence\"]\n    test_data_feats=test_data_feats[[\"len\", \"s_position\", \"thematic_ratio\", \"s_pos_par\", \"num_ratio\"]]\n    \n    return test_data_rougeLTid, test_data_feats, labels_test\n","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-08-03T21:47:13.371295Z","iopub.execute_input":"2022-08-03T21:47:13.372362Z","iopub.status.idle":"2022-08-03T21:47:13.389179Z","shell.execute_reply.started":"2022-08-03T21:47:13.372306Z","shell.execute_reply":"2022-08-03T21:47:13.388081Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"code","source":"score_type=\"rougeL\"\ntrain_data, train_data_feats, labels, dev_data, dev_data_feats, labels_dev, test_data, test_data_feats, labels_test = load_datasets(df_dir, sc_type=score_type)\n\nFileLinks(\".\")","metadata":{"execution":{"iopub.status.busy":"2022-08-03T21:46:26.963206Z","iopub.execute_input":"2022-08-03T21:46:26.965695Z","iopub.status.idle":"2022-08-03T21:46:57.024339Z","shell.execute_reply.started":"2022-08-03T21:46:26.965607Z","shell.execute_reply":"2022-08-03T21:46:57.023170Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"code","source":"# feature_df(test_data,data_dir,\"test\")\nfeature_df(dev_data,data_dir,\"dev\")\nfeature_df(train_data,data_dir,\"train1\")","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # tokenize\n# global pbar_cleaning\n# pbar_cleaning=tqdm(total=train_data.shape[0], leave=True)\n# train_data[\"tokens\"] = train_data[\"sentence\"].apply(lambda x: sentence_cleaning(x)[1]) \n\n# train_data_feats=pd.DataFrame()\n# # create sentence features\n# # 1. thematic words\n# col = train_data.groupby(\"text_id\").tokens.apply(sum)\n# thematic_cols= pd.DataFrame({\"text_id\": col.index, \"thematic\": [ [ t[0] for t in Counter(x).most_common(10) ] for x in col ]})  \n# train_data=train_data.join(thematic_cols[\"thematic\"], on='text_id' )\n# train_data_feats[\"thematic_ratio\"] = train_data.apply(lambda row: thematic_ratio(row.thematic, row.tokens) if len(row.tokens)>0 else 0.0, axis=1)\n\n# # 2. sentence position in the text\n# train_data[\"text_position\"] = train_data.groupby(\"text_id\").cumcount().add(1)\n# train_data[\"tot_sent\"] = train_data.groupby(\"text_id\")[\"sentence\"].transform(len)\n# train_data_feats[\"s_position\"] = train_data.apply(lambda row: s_position(row.text_position,row.tot_sent), axis=1)\n\n# # 3. sentence length - threshold=3\n# threshold=3\n# train_data_feats[\"len\"]= train_data[\"tokens\"].apply(lambda x: 0 if len(x)<threshold else len(x))\n\n# # 4. sentence position - paragraph relative\n# train_data_feats['s_pos_par'] = train_data_feats[\"s_position\"].values\n# train_data_feats.loc[train_data_feats.s_pos_par!=1.0, 's_pos_par']=0.0\n\n# # 5. numerals\n# train_data_feats[\"num_ratio\"]=train_data_feats[\"tokens\"].apply(lambda x: sum( [ 1 for t in eval(x) if t.isnumeric() ] )/len(x) )\n\n\n# # ?. Term Frequency-Inverse Sentence Frequency\n\n\n# # ?. proper nouns - not so useful\n# # train_data_feats[\"NNPs\"]=train_data_feats[\"tokens\"].apply(lambda x: prop_nouns(x) )\n\n\n\n\n# # train_data_feats=train_data[[\"len\",\"text_position\"]]\n# train_data_feats.to_csv(os.path.join(data_dir,f\"{data_type}_set_feats.csv\"), header=True, index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train_data_feats=train_data[[\"len\",\"text_position\",\"thematic_words\", ]]\n# data_type=\"train1\"\n# train_data.to_csv(os.path.join(data_dir,f\"{data_type}_set_feats.csv\"), header=True, index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# FileLinks(\".\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sc_type=\"rougeL\"\n# data_type=\"train1\"\n# train_data=pd.read_csv(os.path.join(df_dir,f\"{data_type}_data_{sc_type}_tid.csv\"))\n# data_stats(train_data)\n# labels=train_data[\"rougeL\"].values\n\n# sc_type=\"rougeL\"\n# data_type=\"dev\"\n# dev_data=pd.read_csv(os.path.join(df_dir,f\"{data_type}_data_{sc_type}_tid.csv\"))\n# data_stats(dev_data)\n# labels=dev_data[\"rougeL\"].values","metadata":{"execution":{"iopub.status.busy":"2022-08-02T00:11:56.581636Z","iopub.execute_input":"2022-08-02T00:11:56.582151Z","iopub.status.idle":"2022-08-02T00:12:13.593052Z","shell.execute_reply.started":"2022-08-02T00:11:56.582104Z","shell.execute_reply":"2022-08-02T00:12:13.591998Z"},"trusted":true},"execution_count":59,"outputs":[]},{"cell_type":"code","source":"# test_num=1000\n\n# model = make_pipeline(MinMaxScaler(), ElasticNet(alpha=5e-3, warm_start=True, random_state=99, fit_intercept=True))\n# model.fit(train_data_feats, labels)\n\n# cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n# # evaluate model\n# scores_mae = cross_val_score(model, train_data_feats.iloc[:test_num,:] , labels[:test_num], scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1)\n# scores_mse = cross_val_score(model, train_data_feats.iloc[:test_num,:] , labels[:test_num], scoring='neg_mean_squared_error', cv=cv, n_jobs=-1)\n# # scores_r2 = cross_val_score(model, train_data_feats.iloc[:test_num,:] , labels[:test_num], scoring='r2', cv=cv, n_jobs=-1)\n\n# # force scores to be positive\n# scores_mae = np.absolute(scores_mae)\n# scores_mse = np.absolute(scores_mse)\n# # scores_r2 = np.absolute(scores_r2)\n# print('Mean MAE: %.3f (%.3f)' % (np.mean(scores_mae), np.std(scores_mae)))\n# print('Mean MSE: %.3f (%.3f)' % (np.mean(scores_mse), np.std(scores_mse)))\n# # print('Mean R2: %.3f (%.3f)' % (np.mean(scores_r2), np.std(scores_r2)))\n\n# print(labels)\n# print(model.predict(train_data_feats))\n# # svr_results(labels[:test_num],  train_data_feats.iloc[:test_num,:], model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# test_num=10000\n\n# # model = make_pipeline(MinMaxScaler(), ElasticNetCV(random_state=99))\n# model =   make_pipeline(StandardScaler(),ElasticNet(alpha=5e-6, random_state=99, fit_intercept=True))\n# model.fit(train_data_feats, labels)\n\n# cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n# # evaluate model\n# scores_mae = cross_val_score(model, train_data_feats.iloc[:test_num,:] , labels[:test_num], scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1)\n# scores_mse = cross_val_score(model, train_data_feats.iloc[:test_num,:] , labels[:test_num], scoring='neg_mean_squared_error', cv=cv, n_jobs=-1)\n# # scores_r2 = cross_val_score(model, train_data_feats.iloc[:test_num,:] , labels[:test_num], scoring='r2', cv=cv, n_jobs=-1)\n\n# # force scores to be positive\n# scores_mae = np.absolute(scores_mae)\n# scores_mse = np.absolute(scores_mse)\n# # scores_r2 = np.absolute(scores_r2)\n# print('Mean MAE: %.3f (%.3f)' % (np.mean(scores_mae), np.std(scores_mae)))\n# print('Mean MSE: %.3f (%.3f)' % (np.mean(scores_mse), np.std(scores_mse)))\n# # print('Mean R2: %.3f (%.3f)' % (np.mean(scores_r2), np.std(scores_r2)))\n\n# print(labels)\n# print(model.predict(train_data_feats))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# test_num=1000\n\n# # model = make_pipeline(MinMaxScaler(), ElasticNetCV(random_state=99))\n# eps=0\n# model = make_pipeline(StandardScaler(), LinearSVR(epsilon=eps, C=5e-1, fit_intercept=True, intercept_scaling=1.0, loss=\"squared_epsilon_insensitive\"))\n# # model = LinearSVR(epsilon=eps, C=5e-4, fit_intercept=True)\n# model.fit(train_data_feats, labels)\n\n# cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n# # evaluate model\n# scores_mae = cross_val_score(model, train_data_feats.iloc[:test_num,:] , labels[:test_num], scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1)\n# scores_mse = cross_val_score(model, train_data_feats.iloc[:test_num,:] , labels[:test_num], scoring='neg_mean_squared_error', cv=cv, n_jobs=-1)\n# # scores_r2 = cross_val_score(model, train_data_feats.iloc[:test_num,:] , labels[:test_num], scoring='r2', cv=cv, n_jobs=-1)\n\n# # force scores to be positive\n# scores_mae = np.absolute(scores_mae)\n# scores_mse = np.absolute(scores_mse)\n# # scores_r2 = np.absolute(scores_r2)\n# print('Mean MAE: %.3f (%.3f)' % (np.mean(scores_mae), np.std(scores_mae)))\n# print('Mean MSE: %.3f (%.3f)' % (np.mean(scores_mse), np.std(scores_mse)))\n# # print('Mean R2: %.3f (%.3f)' % (np.mean(scores_r2), np.std(scores_r2)))\n\n# print(labels)\n# print(model.predict(train_data_feats))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# grid search - Dev Set Tuning\ndef estimator_tuning(X, y, estimator=SGDRegressor(), scaler=StandardScaler()):\n    model_grid = make_pipeline(scaler, estimator )\n\n    param_grid = {\n        \"sgdregressor__alpha\": [8.192e-10],\n        \"sgdregressor__tol\": [6.4e-5],\n        \"sgdregressor__epsilon\": [3.2e-4],\n        \"sgdregressor__loss\": [\"squared_error\"],\n        \"sgdregressor__penalty\": [\"elasticnet\"],\n        \"sgdregressor__learning_rate\": [\"adaptive\"]\n    }\n    # \"sgdregressor__alpha\": 5.0 ** -np.arange(2, 7)\n    # \"sgdregressor__tol\": 10.0 ** -np.arange(2, 7)\n    # \"sgdregressor__loss\": [\"squared_error\", \"huber\", \"epsilon_insensitive\"]\n    # sgdregressor__penalty\": [\"l2\", \"l1\", \"elasticnet\"]\n    # \"sgdregressor__learning_rate\": [\"constant\", \"optimal\", \"invscaling\", \"adaptive\"]\n\n    g_search = GridSearchCV(model_grid, param_grid, verbose=9, return_train_score=True, cv=2)\n    g_search.fit(X, y)\n    \n    with open(os.path.join(\"/kaggle/working/Data\",f\"{estimator}_grid_search_results.txt\"), \"w\", encoding=\"utf-8\" ) as writer:\n          writer.write(f\"Best ParametersL:\\n{g_search.best_params_}\\n\\n\\n{g_search.cv_results_}\")\n\n    print(f\"Best score: { g_search.best_score_}\\nParams: {g_search.best_params_}\")\n    return g_search.best_estimator_\n\n\nbest_model=estimator_tuning(dev_data_feats,labels_dev)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-08-02T14:41:33.573323Z","iopub.status.idle":"2022-08-02T14:41:33.574259Z","shell.execute_reply.started":"2022-08-02T14:41:33.573972Z","shell.execute_reply":"2022-08-02T14:41:33.574000Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Best sofar","metadata":{}},{"cell_type":"code","source":"def estimator_training(train_data_feats, labels, test_data_feats, labels_test):\n    model = make_pipeline(StandardScaler(), SGDRegressor(alpha=8.192e-10, max_iter=1000, tol=6.4e-5, epsilon=3.2e-4, learning_rate=\"adaptive\", loss=\"squared_error\", penalty=\"elasticnet\"))\n    # model = LinearSVR(epsilon=eps, C=5e-4, fit_intercept=True)\n    model.fit(train_data_feats, labels)\n\n    cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n    \n    # evaluate model\n    test_samples=1000\n    # MAE\n    scores_mae = cross_val_score(model, test_data_feats.iloc[:test_samples,:] , labels_test[:test_samples], scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1)\n    scores_mae = np.absolute(scores_mae)\n    print('Mean MAE: %.3f (%.3f)' % (np.mean(scores_mae), np.std(scores_mae)))\n    # MSE\n    scores_mse = cross_val_score(model, test_data_feats.iloc[:test_samples,:] , labels_test[:test_samples], scoring='neg_mean_squared_error', cv=cv, n_jobs=-1)\n    scores_mse = np.absolute(scores_mse)\n    print('Mean MSE: %.3f (%.3f)' % (np.mean(scores_mse), np.std(scores_mse)))\n    # R2    \n    scores_r2 = cross_val_score(model, test_data_feats.iloc[:test_samples,:] , labels_test[:test_samples], scoring='r2', cv=cv, n_jobs=-1)\n#     scores_r2 = np.absolute(scores_r2)\n    print('Mean R2: %.3f (%.3f)' % (np.mean(scores_r2), np.std(scores_r2)))\n\n#     print(labels_test)\n#     print(model.predict(test_data_feats))\n    \n    return model\n\n\nmodel=estimator_training(train_data_feats, labels, test_data_feats, labels_test)\n\npkl_filepath=os.path.join(\"/kaggle/working/Data\",\"SGD_model.pkl\")\nwith open(pkl_filepath,\"wb\") as model_writer:\n    pickle.dump(model, model_writer)","metadata":{"execution":{"iopub.status.busy":"2022-08-03T20:28:33.338105Z","iopub.execute_input":"2022-08-03T20:28:33.338881Z","iopub.status.idle":"2022-08-03T20:28:58.375316Z","shell.execute_reply.started":"2022-08-03T20:28:33.338839Z","shell.execute_reply":"2022-08-03T20:28:58.374104Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# preds_df=pd.DataFrame({\"text_id\": train_data[\"text_id\"].values, \"text_position\": train_data[\"text_position\"].values, \"sentence\": train_data[\"sentence\"].values, \"pred_rougeL\": model.predict(train_data_feats)})\n\n# n_largest=preds_df.groupby([\"text_id\"])[\"text_position\",\"pred_rougeL\"].apply(lambda x: x.nlargest(3,columns=[\"pred_rougeL\"]).sort_index())","metadata":{"execution":{"iopub.status.busy":"2022-08-01T17:29:07.932684Z","iopub.execute_input":"2022-08-01T17:29:07.933515Z","iopub.status.idle":"2022-08-01T17:29:08.214945Z","shell.execute_reply.started":"2022-08-01T17:29:07.933472Z","shell.execute_reply":"2022-08-01T17:29:08.213571Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"data_dir=\"/kaggle/working/Data\"\ndef scoring(pred_summary, ref_summary,doc_id):\n    pred_summary = re.sub(r'\\n+', '', pred_summary)\n    \n    score_ind={\"precision\":0, \"recall\":1, \"fmeasure\":2}\n    r_scorer=rouge_scorer.RougeScorer([\"rouge1\",\"rouge2\",\"rougeL\"], use_stemmer=True)\n\n    rouge=r_scorer.score(ref_summary,pred_summary)\n    \n    r1=rouge[\"rouge1\"][score_ind[\"fmeasure\"]]\n    r2=rouge[\"rouge2\"][score_ind[\"fmeasure\"]]\n    rL=rouge[\"rougeL\"][score_ind[\"fmeasure\"]]\n    \n    with open(os.path.join(data_dir,\"summaResults.txt\"), \"a\", encoding=\"utf-8\") as writer:\n        file_str=f\"> Document #{doc_id}\\n\\nPredicted Summary\\n{pred_summary}\\n\\nReference Summary\\n{ref_summary}\\n\\nR1: {r1:.9f} | R2: {r2:.9f} | RL: {rL:.9f}\\n\\n\\n\\n\"\n        writer.write(file_str)\n        \n    return r1,r2,rL\n\n\ndef summarization(sentences):\n    sentences = [ str(s) for s in sentences if len(s)>0 ]\n    if len(sentences)==0:\n        return \" \"\n    \n    summary = \".\".join(sentences)\n    if summary[-1] is not \".\":\n        summary+=\".\"\n    \n    return summary\n\n\ndef create_summary(df,s_num,th=0.19):\n    n_largest=df.groupby([\"text_id\"])[\"text_position\",\"sentence\",\"pred_rougeL\"].apply(lambda x: x.nlargest(s_num,columns=[\"pred_rougeL\"]).sort_index())\n    max_rL=max(n_largest[\"pred_rougeL\"])\n    sent = [ n_largest.loc[n_largest[\"pred_rougeL\"].idxmax(),\"sentence\"] ]\n    sent.extend( [ s for s,r in zip(n_largest[\"sentence\"].values, n_largest[\"pred_rougeL\"].values) if r > th and r!=max_rL] )\n    summary = summarization( sent)   \n    return summary\n\n\ndef get_ref_summary(ref_summaries,doc_id):\n    ref_sum=ref_summaries.loc[ref_summaries.text_id==doc_id, \"summary\"].values\n    \n    if type(ref_sum)!=str:\n        ref_sum=ref_sum[0]\n#     if type(ref_sum)!=str:\n#         ref_sum=\" \"\n    return ref_sum\n\n\ndef get_preds(model,docX):\n    return model.predict(docX)\n\n\ndef doc_summary(model,document,ref_summaries,doc_id,th,s_num=4) :\n    global doc_p_bar, tot_r1, tot_r2, tot_rL\n    \n#         if i>10:\n#             break\n    cols=[\"len\", \"s_position\", \"thematic_ratio\", \"s_pos_par\", \"num_ratio\"]\n    feats=document[cols]\n\n\n    preds=get_preds(model,feats)\n    document[\"pred_rougeL\"]=preds.tolist()\n    pred_summary=create_summary(document,s_num,th)\n    ref_summary=get_ref_summary(ref_summaries,doc_id)\n    r1,r2,rL = scoring(pred_summary,ref_summary,doc_id)\n    tot_r1.append(r1)\n    tot_r2.append(r2)\n    tot_rL.append(rL)\n    doc_p_bar.postfix[1] = np.mean(tot_r1)\n    doc_p_bar.postfix[3] = np.mean(tot_r2)\n    doc_p_bar.postfix[5] = np.mean(tot_rL)\n    doc_p_bar.postfix[6][\"value\"] = doc_id\n    doc_p_bar.update(1)\n\n    del document\n    del feats\n    del preds\n    del pred_summary\n    del ref_summary\n    gc.collect()\n    \n    return r1, r2, rL","metadata":{"execution":{"iopub.status.busy":"2022-08-03T20:28:58.377266Z","iopub.execute_input":"2022-08-03T20:28:58.377829Z","iopub.status.idle":"2022-08-03T20:28:58.397663Z","shell.execute_reply.started":"2022-08-03T20:28:58.377791Z","shell.execute_reply":"2022-08-03T20:28:58.396313Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# sc_type=\"rougeL\"\n# data_type=\"train1\"\n# train_data=pd.read_csv(os.path.join(df_dir,f\"{data_type}_data_{sc_type}_tid.csv\"))\n# # train_data[\"text_position\"] = train_data.groupby(\"text_id\").cumcount().add(1)\n# data_stats(train_data)\n# labels=train_data[\"rougeL\"].values\n\n# sc_type=\"rougeL\"\n# data_type=\"test\"\n# test_data=pd.read_csv(os.path.join(df_dir,f\"{data_type}_data_{sc_type}_tid.csv\"))\n# test_data[\"text_position\"] = test_data.groupby(\"text_id\").cumcount().add(1)\n# data_stats(test_data)\n# labels=test_data[\"rougeL\"].values","metadata":{"execution":{"iopub.status.busy":"2022-08-02T23:59:45.163539Z","iopub.execute_input":"2022-08-02T23:59:45.164850Z","iopub.status.idle":"2022-08-02T23:59:45.168883Z","shell.execute_reply.started":"2022-08-02T23:59:45.164804Z","shell.execute_reply":"2022-08-02T23:59:45.167989Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# summaries=pd.read_csv(\"/kaggle/input/summarizer-data/train_summaries.csv\")\n# train_len=train_data.groupby(\"text_id\").size().shape[0]\n\n# summaries=summaries.iloc[:train_len,:]\n# train_data_feats[\"summary\"]=summaries[\"summary\"]\n# train_data_feats[\"sentence\"]=train_data[\"sentence\"]\n# train_data_feats[\"text_id\"]=train_data[\"text_id\"]\n# train_data_feats[\"text_position\"]=train_data[\"text_position\"]\n\n# X=train_data_feats\n# y=pd.DataFrame(labels)\n\n\nscore_type=\"rougeL\"\ntest_data, test_data_feats, labels_test = load_dataset_test(df_dir, sc_type=score_type)\n\nsc_type=\"rougeL\"\ndata_type=\"test\"\n# test_data=pd.read_csv(os.path.join(df_dir,f\"{data_type}_data_{sc_type}.csv\"))\n# test_data[\"text_position\"] = test_data.groupby(\"text_id\").cumcount().add(1)\nlabels=test_data[\"rougeL\"].values\n\nsummaries=pd.read_csv(\"/kaggle/input/summarizer-data/test_data_rougeL_summaries_grouped.csv\")\nsummaries=pd.DataFrame(summaries[\"summary\"])\nsummaries[\"text_id\"]=summaries.index\n\n# summaries.to_csv(\"/kaggle/working/Data/DataFrames/test_summaries.csv\", header=True, index=False)\n# test_len=test_data.groupby(\"text_id\").size().shape[0]\n# summaries[\"text_id\"]=summaries.index\n# if \"summary\" not in summaries.columns:\n#     summaries[\"summary\"]=summaries[\"sum_clean\"]\n#     del summaries[\"sum_clean\"]\n\n# summaries=summaries.iloc[:dev_len,:]\n# dev_data_feats[\"summary\"]=summaries[\"sum_clean\"]\ntest_data_feats[\"sentence\"]=test_data[\"sentence\"]\ntest_data_feats[\"text_id\"]=test_data[\"text_id\"]\ntest_data_feats[\"text_position\"]=test_data[\"text_position\"]\n\nX=test_data_feats\ny=pd.DataFrame(labels_test)","metadata":{"execution":{"iopub.status.busy":"2022-08-03T21:47:19.936273Z","iopub.execute_input":"2022-08-03T21:47:19.937350Z","iopub.status.idle":"2022-08-03T21:47:23.717506Z","shell.execute_reply.started":"2022-08-03T21:47:19.937303Z","shell.execute_reply":"2022-08-03T21:47:23.716545Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"code","source":"# X.loc[X.text_id==10000,\"sentence\"].values, \"\\n\\n\", summaries.loc[10000,\"summary\"]","metadata":{"execution":{"iopub.status.busy":"2022-08-03T18:02:06.635935Z","iopub.execute_input":"2022-08-03T18:02:06.636342Z","iopub.status.idle":"2022-08-03T18:02:06.645844Z","shell.execute_reply.started":"2022-08-03T18:02:06.636310Z","shell.execute_reply":"2022-08-03T18:02:06.644960Z"},"trusted":true},"execution_count":240,"outputs":[]},{"cell_type":"code","source":"open(os.path.join(data_dir,\"summaResults.txt\"),\"w\")\n%cd /kaggle/working\nFileLinks(\".\")","metadata":{"execution":{"iopub.status.busy":"2022-08-03T18:15:06.591814Z","iopub.execute_input":"2022-08-03T18:15:06.592250Z","iopub.status.idle":"2022-08-03T18:15:06.605654Z","shell.execute_reply.started":"2022-08-03T18:15:06.592216Z","shell.execute_reply":"2022-08-03T18:15:06.604426Z"},"trusted":true},"execution_count":252,"outputs":[]},{"cell_type":"code","source":"# %cd /kaggle/working\n# FileLinks(\".\")\n\ntot_r1=[]\ntot_r2=[]\ntot_rL=[]\n\nglobal doc_p_bar\ndoc_p_bar=tqdm(summaries[\"summary\"], desc=f\"{data_type} Set Summary Scoring\", bar_format=\"{postfix[0]}: {postfix[1]:.10f} | {postfix[2]}: {postfix[3]:.10f} | {postfix[4]}: {postfix[5]:.10f} ( {postfix[6][value]}/{postfix[7]} ) {elapsed}<{remaining}\", postfix=[\"Mean Rouge1\", np.mean(tot_r1), \"Mean Rouge2\", np.mean(tot_r2), \"Mean RougeL\", np.mean(tot_rL), dict(value=0) ,summaries.shape[0]], leave=True)\n\nopen(os.path.join(data_dir,\"summaResults.txt\"),\"w\")\n# FileLinks(\".\")\nthreshold=0.3\nFileLinks(\"\")\nscores=X.groupby(\"text_id\").apply(lambda x: doc_summary(model,x,summaries,int(x.text_id.values[0]),threshold) )\n\nfinal_res=f\"\\nMean Rouge1: {np.mean(r1)}\\nMean Rouge2: {np.mean(r2)}\\nMean RougeL: {np.mean(rL)}\\n\\n\"\nprint(final_res)\n\nwith open(os.path.join(data_dir,\"summaResults.txt\"), \"a\", encoding=\"utf-8\") as writer:\n        writer.write(\"\\n\\n---> Final Results\\n\\n\"+final_res)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-08-03T18:30:29.209049Z","iopub.execute_input":"2022-08-03T18:30:29.209775Z","iopub.status.idle":"2022-08-03T18:33:36.101441Z","shell.execute_reply.started":"2022-08-03T18:30:29.209735Z","shell.execute_reply":"2022-08-03T18:33:36.099787Z"},"trusted":true},"execution_count":258,"outputs":[]},{"cell_type":"code","source":"cols=[\"len\", \"s_position\", \"thematic_ratio\", \"s_pos_par\", \"num_ratio\"]\nfeats=test_data_feats[cols]\npreds=model.predict(feats).tolist()","metadata":{"execution":{"iopub.status.busy":"2022-08-03T21:47:39.785163Z","iopub.execute_input":"2022-08-03T21:47:39.785581Z","iopub.status.idle":"2022-08-03T21:47:39.917511Z","shell.execute_reply.started":"2022-08-03T21:47:39.785543Z","shell.execute_reply":"2022-08-03T21:47:39.916261Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"code","source":"preds_df=pd.DataFrame({\"sentence\": test_data_feats[\"sentence\"], \"text_id\": test_data_feats[\"text_id\"], \"text_position\": test_data_feats[\"text_position\"], \"pred_rougeL\": preds})","metadata":{"execution":{"iopub.status.busy":"2022-08-03T21:47:45.461087Z","iopub.execute_input":"2022-08-03T21:47:45.461503Z","iopub.status.idle":"2022-08-03T21:47:45.605692Z","shell.execute_reply.started":"2022-08-03T21:47:45.461467Z","shell.execute_reply":"2022-08-03T21:47:45.604659Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"code","source":"threshold=0.3\ns_num_max=5\npred_sums=preds_df.groupby(\"text_id\").apply(lambda gr: create_summary(gr,s_num_max,threshold))","metadata":{"execution":{"iopub.status.busy":"2022-08-03T21:50:25.196483Z","iopub.execute_input":"2022-08-03T21:50:25.196896Z","iopub.status.idle":"2022-08-03T21:52:51.045343Z","shell.execute_reply.started":"2022-08-03T21:50:25.196862Z","shell.execute_reply":"2022-08-03T21:52:51.044105Z"},"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"code","source":"r_scorer=rouge_scorer.RougeScorer([\"rouge1\",\"rouge2\",\"rougeL\"], use_stemmer=True)\n\nscores=[ r_scorer.score(ref,pred) for ref,pred in zip(summaries[\"summary\"].values,pred_sums) ]","metadata":{"execution":{"iopub.status.busy":"2022-08-03T21:59:55.814809Z","iopub.execute_input":"2022-08-03T21:59:55.815877Z","iopub.status.idle":"2022-08-03T22:01:32.085812Z","shell.execute_reply.started":"2022-08-03T21:59:55.815822Z","shell.execute_reply":"2022-08-03T22:01:32.084649Z"},"trusted":true},"execution_count":76,"outputs":[]},{"cell_type":"code","source":"score_df=pd.DataFrame(scores)","metadata":{"execution":{"iopub.status.busy":"2022-08-03T22:01:32.087818Z","iopub.execute_input":"2022-08-03T22:01:32.088190Z","iopub.status.idle":"2022-08-03T22:01:32.130965Z","shell.execute_reply.started":"2022-08-03T22:01:32.088149Z","shell.execute_reply":"2022-08-03T22:01:32.130075Z"},"trusted":true},"execution_count":77,"outputs":[]},{"cell_type":"code","source":"score_df[\"ref_sum\"]=summaries[\"summary\"].values\nscore_df[\"pred_sum\"]=pred_sums\nscore_df[\"text_id\"]=score_df.index\n\nscore_df[\"rouge1_precision\"]=score_df[\"rouge1\"].apply(lambda x: x[0])\nscore_df[\"rouge1_recall\"]=score_df[\"rouge1\"].apply(lambda x: x[1])\nscore_df[\"rouge1_fmeasure\"]=score_df[\"rouge1\"].apply(lambda x: x[2])\ndel score_df[\"rouge1\"]\n\nscore_df[\"rouge2_precision\"]=score_df[\"rouge2\"].apply(lambda x: x[0])\nscore_df[\"rouge2_recall\"]=score_df[\"rouge2\"].apply(lambda x: x[1])\nscore_df[\"rouge2_fmeasure\"]=score_df[\"rouge2\"].apply(lambda x: x[2])\ndel score_df[\"rouge2\"]\n\nscore_df[\"rougeL_precision\"]=score_df[\"rougeL\"].apply(lambda x: x[0])\nscore_df[\"rougeL_recall\"]=score_df[\"rougeL\"].apply(lambda x: x[1])\nscore_df[\"rougeL_fmeasure\"]=score_df[\"rougeL\"].apply(lambda x: x[2])\ndel score_df[\"rougeL\"]","metadata":{"execution":{"iopub.status.busy":"2022-08-03T22:02:26.800185Z","iopub.execute_input":"2022-08-03T22:02:26.800615Z","iopub.status.idle":"2022-08-03T22:02:26.945635Z","shell.execute_reply.started":"2022-08-03T22:02:26.800580Z","shell.execute_reply":"2022-08-03T22:02:26.944559Z"},"trusted":true},"execution_count":80,"outputs":[]},{"cell_type":"code","source":"def log_to_file(row):\n    pred_summary = re.sub(r'\\n+', '', row['pred_sum'])\n    doc_str=f\"> Document #{row['text_id']}\\n\\n#Predicted Summary\\n{pred_summary}\\n\\n#Reference Summary\\n{row['ref_sum']}\\n\\nR1: {row['rouge1_fmeasure']}  |  R2: {row['rouge2_fmeasure']}  |  RL:  {row['rougeL_fmeasure']}\\n\\n\\n\\n\"\n    return doc_str\n\nfile_str=score_df.apply(lambda row: log_to_file(row), axis=1)\n\nwith open(os.path.join(data_dir,f\"summaResults_{data_type}.txt\"),\"w\",encoding=\"utf-8\") as log_writer:\n    for row_str in file_str:\n        log_writer.write(row_str)\n        \nFileLinks(\".\")","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-08-03T22:20:31.721124Z","iopub.execute_input":"2022-08-03T22:20:31.721548Z","iopub.status.idle":"2022-08-03T22:20:33.335092Z","shell.execute_reply.started":"2022-08-03T22:20:31.721516Z","shell.execute_reply":"2022-08-03T22:20:33.334118Z"},"trusted":true},"execution_count":101,"outputs":[]},{"cell_type":"code","source":"FileLinks(\".\")","metadata":{"execution":{"iopub.status.busy":"2022-08-03T22:20:57.682964Z","iopub.execute_input":"2022-08-03T22:20:57.684113Z","iopub.status.idle":"2022-08-03T22:20:57.692418Z","shell.execute_reply.started":"2022-08-03T22:20:57.684053Z","shell.execute_reply":"2022-08-03T22:20:57.691223Z"},"trusted":true},"execution_count":102,"outputs":[]},{"cell_type":"markdown","source":"___\n___\n# **Notes**\n\n\n1. scoring -> label rougeL -> sentence feats -> train\n2. grid search ?! (or manual fine tuning)\n3. test -> input doc -> predict score -> keep N first sentences or keep those over a threshold -> create summary -> calculate rouge1/2/L\n\n___\n### **References**\n\n1. [Named Entity Recognition (NER) with TensorflowNamed Entity Recognition (NER) with Tensorflow](https://www.kaggle.com/code/naseralqaydeh/named-entity-recognition-ner-with-tensorflow)\n2. [Extractive Summarization using Deep LearningExtractive Summarization using Deep Learning](https://arxiv.org/pdf/1708.04439v1.pdf)\n3. [NLTK](https://www.bogotobogo.com/python/NLTK/Stemming_NLTK.php)\n4. [Text Features Library](https://github.com/pmbaumgartner/text-feat-lib/tree/master/notebooks)\n5. []()\n\n\n\n### **Feats**\n1. [Feature extraction](https://arxiv.org/pdf/1708.04439v1.pdf)\n    1. Number of thematic words\n    2. Sentence position\n    3. Sentence length\n    4. Sentence position relative to paragraph\n    5. Number of proper nouns\n    6. Number of numerals\n    7. Number of named entities\n    8. Term Frequency-Inverse Sentence Frequency\n    9. Sentence to Centroid similarity\n    \n    \n2. [Text Summarization References](https://github.com/Tian312/awesome-text-summarization/blob/master/README.md)\n\n\n\n___\n### **Feature Base**\n\nThe feature base model extracts the features of the sentence, then evaluate its importance. Here is the representative research.\nSentence Extraction Based Single Document Summarization\nFollowing features are used in the above method.\n\n1. Position of the sentence in the input document\n2. Presence of the verb in the sentence\n3. Length of the sentence\n4. Term frequency\n5. Named entity tag NE\n6. Font style\n\n…etc. All the features are accumulated as the score.\nThe No.of coreferences are the number of pronouns to the previous sentence. It is simply calculated by counting the pronouns occurred in the first half of the sentence. So the Score represents the reference to the previous sentence.\nNow we can evaluate each sentence. Next is selecting the sentence to avoid the duplicate of the information. In this paper, the same word between the new and selected sentence is considered. And the refinement to connect the selected sentences are executed.\nLuhn’s Algorithm is also feature base. It evaluates the “significance” of the word that is calculated from the frequency.\nYou can try feature base text summarization by TextTeaser (PyTeaser is available for Python user).","metadata":{}},{"cell_type":"markdown","source":"# Unused","metadata":{}},{"cell_type":"code","source":"# train_set = f\"..{os.sep}Data{os.sep}release{os.sep}train.jsonl\"\n# dev_set = f\"..{os.sep}Data{os.sep}release{os.sep}dev.jsonl\"\n# test_set = f\"..{os.sep}Data{os.sep}release{os.sep}test.jsonl\"\n# load json files and convert them to dataframes to load faster next time\n# train_df = funs.json_to_df(train_set,\"train\")\n# dev_df = funs.json_to_df(dev_set,\"dev\")\n# test_df = funs.json_to_df(test_set,\"test\")\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# colab command to download the dataset\n# !kaggle datasets download -d tkylafi/summarizer-data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for items in os.walk(\"/kaggle/input\"):\n#     print(items)\n    \n\n# test_json=json.loads(test_json_path)","metadata":{"execution":{"iopub.status.busy":"2022-08-03T09:50:32.238730Z","iopub.execute_input":"2022-08-03T09:50:32.239653Z","iopub.status.idle":"2022-08-03T09:50:53.156609Z","shell.execute_reply.started":"2022-08-03T09:50:32.239611Z","shell.execute_reply":"2022-08-03T09:50:53.155512Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}