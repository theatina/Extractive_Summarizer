{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"___\n\n# M915 - Œ£œÖœÉœÑŒÆŒºŒ±œÑŒ± ŒöŒ±œÑŒ±ŒΩœåŒ∑œÉŒ∑œÇ Œ∫Œ±Œπ Œ†Œ±œÅŒ±Œ≥œâŒ≥ŒÆœÇ ŒöŒµŒπŒºŒ≠ŒΩŒøœÖ <br> <span style=\"font-size:6mm;\"> Assignment 1 </span> <br><br> <span style=\"font-size:5mm;\"> Kylafi Christina-Theano </span> <br> <span style=\"font-size:4mm;\"> LT1200012 </span>\n---\n---","metadata":{}},{"cell_type":"markdown","source":"## Imports\n---","metadata":{}},{"cell_type":"code","source":"from heapq import heapify\nimport os\nimport json\nfrom typing import Counter\nimport pandas as pd\nimport re\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pickle\n\nimport math\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer, PorterStemmer, SnowballStemmer, LancasterStemmer\nfrom nltk.tokenize import sent_tokenize, word_tokenize\nfrom nltk import pos_tag\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport sklearn.model_selection\nfrom sklearn.model_selection import RepeatedKFold, cross_val_score, GridSearchCV\nimport sklearn.preprocessing as preproc\nfrom sklearn.feature_extraction import text\nfrom sklearn.svm import LinearSVR\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression, ElasticNet, ElasticNetCV, LassoLars, SGDRegressor\nfrom sklearn.metrics import confusion_matrix, mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, MaxAbsScaler\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.svm import SVR\n\nfrom IPython.display import FileLink, FileLinks\n\nfrom tqdm import tqdm\n\n!pip install rouge-score\nfrom rouge_score import rouge_scorer\n\nimport psutil\nimport gc\nfrom collections import Counter\n\n# Add Data -> \"Summarizer Data\" / \"summarizer-data\" -> \"Add\"\ndf_dir=\"/kaggle/input/summarizer-data\"\ndata_dir= \"/kaggle/working/Data/DataFrames\"\nif not os.path.exists(data_dir):\n    os.makedirs(data_dir)\n    ","metadata":{"execution":{"iopub.status.busy":"2022-09-04T14:57:38.835967Z","iopub.execute_input":"2022-09-04T14:57:38.836877Z","iopub.status.idle":"2022-09-04T14:57:54.981898Z","shell.execute_reply.started":"2022-09-04T14:57:38.836833Z","shell.execute_reply":"2022-09-04T14:57:54.980706Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Collecting rouge-score\n  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: absl-py in /opt/conda/lib/python3.7/site-packages (from rouge-score) (1.1.0)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.7/site-packages (from rouge-score) (3.7)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from rouge-score) (1.21.6)\nRequirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.7/site-packages (from rouge-score) (1.16.0)\nRequirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from nltk->rouge-score) (8.0.4)\nRequirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.7/site-packages (from nltk->rouge-score) (2021.11.10)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from nltk->rouge-score) (4.64.0)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from nltk->rouge-score) (1.0.1)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from click->nltk->rouge-score) (4.12.0)\nRequirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->click->nltk->rouge-score) (4.1.1)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->click->nltk->rouge-score) (3.8.0)\nBuilding wheels for collected packages: rouge-score\n  Building wheel for rouge-score (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24955 sha256=c0374d71be1efd5aa25fcc07f77fef3808b1a2c24bac26242a90ab9975b72397\n  Stored in directory: /root/.cache/pip/wheels/84/ac/6b/38096e3c5bf1dc87911e3585875e21a3ac610348e740409c76\nSuccessfully built rouge-score\nInstalling collected packages: rouge-score\nSuccessfully installed rouge-score-0.1.2\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"markdown","source":"## Functions\n---","metadata":{}},{"cell_type":"code","source":"# create dataframes of the json datasets (train, dev & test)\ndef json_to_df(json_path,data_type):\n    data=[]\n    for ln in open(json_path,\"r\"):\n        obj = json.loads(ln)\n        data.append(obj)\n    df=pd.DataFrame(data)\n    \n    cols=[ \"title\",\t\"date\",\t\"text\",\t\"summary\", \"compression\", \"coverage\", \"density\", \"compression_bin\", \"coverage_bin\"]\n    df=df.loc[df.density_bin==\"extractive\"].reset_index()\n    \n    # save the dataframe and the summaries as they are for future use\n    df.to_csv(f\"/kaggle/working/Data{os.sep}DataFrames{os.sep}{data_type}_set.csv\", header=True, index=False )\n    df[\"summary\"].to_csv(f\"/kaggle/working/Data{os.sep}DataFrames{os.sep}{data_type}_summaries.csv\", header=True, index=False )\n    \n    return df\n\n\n# text processing functions\n# A list of contractions from http://stackoverflow.com/questions/19790188/expanding-english-language-contractions-in-python\ncontractions = { \"ain't\": \"am not\", \"aren't\": \"are not\", \"can't\": \"cannot\", \"can't've\": \"cannot have\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \"couldn't've\": \"could not have\", \"didn't\": \"did not\", \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hadn't've\": \"had not have\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\", \"he'd've\": \"he would have\", \"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'll\": \"how will\", \"how's\": \"how is\", \"i'd\": \"i would\", \"i'll\": \"i will\", \"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'll\": \"it will\", \"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\", \"mightn't\": \"might not\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"needn't\": \"need not\", \"oughtn't\": \"ought not\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"she'd\": \"she would\", \"she'll\": \"she will\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"that'd\": \"that would\", \"that's\": \"that is\", \"there'd\": \"there had\", \"there's\": \"there is\", \"they'd\": \"they would\", \"they'll\": \"they will\", \"they're\": \"they are\", \"they've\": \"they have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'll\": \"we will\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what're\": \"what are\", \"what's\": \"what is\", \"what've\": \"what have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"who'll\": \"who will\", \"who's\": \"who is\", \"won't\": \"will not\", \"wouldn't\": \"would not\", \"you'd\": \"you would\", \"you'll\": \"you will\", \"you're\": \"you are\" }\n\n\n# text cleaning process\ndef sentence_cleaning(text, remove_stopwords = True, sub_contractions=True, stemming=True):\n    global pbar_cleaning\n    pbar_cleaning.update(1)\n    \n    # Convert words to lower case\n    text = text.lower()\n    toks = word_tokenize(text)\n      \n    # Replace contractions with their longer forms \n    if sub_contractions:\n        new_text = []\n        for word in toks:\n            if word in contractions:\n                new_text.append(contractions[word])\n            else:\n                new_text.append(word)\n    \n    text = \" \".join(new_text)\n\n    # Format words and remove unwanted characters\n    text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n    text = re.sub(r'\\<a href', ' ', text)\n    text = re.sub(r'&amp;', '', text) \n    text = re.sub(r'[_\"\\-;%()|+&=*%.,!?:#$@\\[\\]/]', ' ', text)\n    text = re.sub(r'<br />', ' ', text)\n    text = re.sub(r'\\'', ' ', text)\n\n    toks_clean = word_tokenize(text)\n    \n    if remove_stopwords:\n        stops = set(stopwords.words(\"english\"))\n        toks_clean = [w for w in toks_clean if not w in stops]\n\n    \n    if stemming: \n        stemmer=SnowballStemmer(language=\"english\")\n        toks_clean=[ stemmer.stem(w) for w in toks_clean ]\n\n    text = \" \".join(toks_clean)\n    \n    return text, toks_clean\n\n\n# score sentences (RougeL fmeasure) with respect to gold summaries \n# Source: [SummaRuNNer: a recurrent neural network based sequence model for extractive summarization of documents](https://arxiv.org/abs/1611.04230)\ndef rouge_scoring(sentence,summary,sc_type=\"rougeL\",score=\"fmeasure\"):\n    global pbar\n    pbar.update(1)\n    r_scorer=rouge_scorer.RougeScorer([\"rouge1\",\"rouge2\",\"rougeL\"])\n    r_L=r_scorer.score(summary,sentence)\n    score_ind={\"precision\":0, \"recall\":1, \"fmeasure\":2}\n    \n    return r_L[sc_type][score_ind[score]]\n\n\ndef text_processing(df,data_type,df_dir,sc_type=\"rougeL\"):\n    global pbar\n    cols=[\"sentence\", \"summary\", \"text\", \"text_id\"] \n    \n    df[\"summary\"].to_csv(os.path.join(df_dir,f\"{data_type}_data_{sc_type}_summaries_grouped.csv\"), header=True, index=False)\n\n    # sentence split \n    sentences=[ sent_tokenize(t) for t in df[\"text\"].values ]\n\n    summaries=df[\"summary\"].values\n    sent_sum_text=[ [ s,summary,t,i  ] for i, (s_list,summary,t) in enumerate(zip( sentences, summaries, df[\"text\"] )) for s in s_list ]\n    new_df=pd.DataFrame(sent_sum_text, columns=cols)\n    \n    # add a column indicating if sentence was chosen to be in the summary\n    new_df[\"chosen\"]= 0\n    ind = new_df[[ s in t for s,t in zip( new_df[\"sentence\"], new_df[\"summary\"] ) ]].index\n    new_df.loc[ind,\"chosen\"]=1\n    del new_df[\"text\"]\n\n    # labels - RougeL scores\n    pbar = tqdm(total=new_df.shape[0] )\n    new_df[\"rougeL\"]= new_df.apply(lambda row: rouge_scoring(row[\"sentence\"],row[\"summary\"], sc_type=sc_type, score=\"fmeasure\" ), axis=1)\n\n    new_df[\"summary\"].to_csv(os.path.join(df_dir,f\"{data_type}_data_{sc_type}_summaries.csv\"), header=True, index=False)\n    del new_df[\"summary\"]\n    \n    # save dataframe with scoring/labels\n    new_df.to_csv(os.path.join(df_dir,f\"{data_type}_data_{sc_type}.csv\"), header=True, index=False)\n\n    return new_df\n\n\ndef add_chosen_text_id(df):\n    cols=[\"sentence\", \"summary\", \"text\"] \n\n    # sentence split \n    sentences=[ sent_tokenize(t) for t in df[\"text\"].values ]\n    summaries=df[\"summary\"].values\n    \n    sent_sum_text=[ [ s,summary,t  ] for s_list,summary,t in zip( sentences, summaries, df[\"text\"] ) for s in s_list  ]\n    new_df=pd.DataFrame(sent_sum_text, columns=cols)\n    \n    new_df[\"text_id\"]=new_df[\"text\"].factorize()[0]\n    new_df[\"chosen\"]= 0\n    ind = new_df[[ s in t for s,t in zip( new_df[\"sentence\"], new_df[\"summary\"] ) ]].index\n    new_df.loc[ind,\"chosen\"]=1\n    \n    del new_df[\"text\"]\n    del new_df[\"summary\"]\n    del new_df[\"sentence\"]\n    \n    return new_df\n\n\ndef text_proc_labels(data_dir,df_dir=\"/kaggle/input/summarizer-data/\"):\n#     train_df = pd.read_csv(os.path.join(df_dir,\"train_set.csv\"))\n    dev_df = pd.read_csv(os.path.join(df_dir,\"dev_set.csv\"))\n    test_df = pd.read_csv(os.path.join(df_dir,\"test_set.csv\"))\n\n    dev_data=None\n    test_data=None\n    \n#     train_df1, train_df2, train_df3, train_df4 = np.array_split(train_df, 4)\n    splits=4\n    train_data_list=[]\n    for i in range(splits):\n        train_data_type=f\"train{i+1}\"\n        train_df = pd.read_csv(os.path.join(df_dir,f\"{train_data_type}_set.csv\"))\n        train_data = text_processing(train_df,f\"train{i+1}\",data_dir,\"rougeL\")\n        train_data_list.append(train_data)\n    \n    test_data = text_processing(test_df,\"test\",data_dir,\"rougeL\")\n    dev_data = text_processing(dev_df,\"dev\",data_dir,\"rougeL\")\n    \n    \n    return train_data_list, dev_data, test_data\n\n\ndef df_add_tid(data_dir, df_dir):\n    sc_type=\"rougeL\"\n    data_labels_dir=\"/kaggle/input/summarizer-data\"\n\n    data_type=\"train\"\n    train_df = pd.read_csv(os.path.join(df_dir,f\"{data_type}_set.csv\"))\n    splits=4\n    for i,df in enumerate(np.array_split(train_df, 4)):\n        if i==2:\n            data_type=f\"train{i+1}\"\n            print(data_type+\"\\n\")\n            train_ch_tid_df=add_chosen_text_id(df)\n            train_rougeL=pd.read_csv(os.path.join(data_labels_dir,f\"{data_type}_data_{sc_type}.csv\"))\n            print(train_rougeL.columns, train_rougeL.shape)\n            train_rougeL[\"text_id\"]=train_ch_tid_df[\"text_id\"]\n            train_rougeL[\"chosen\"]=train_ch_tid_df[\"chosen\"]\n            print(train_rougeL.columns, train_rougeL.shape)\n            train_rougeL.to_csv(os.path.join(data_dir,f\"{data_type}_data_{sc_type}_tid.csv\"), header=True, index=False)\n\n    data_type=\"dev\" \n    dev_df = pd.read_csv(os.path.join(df_dir,f\"{data_type}_set.csv\"))\n    dev_ch_tid_df=add_chosen_text_id(dev_df)\n    dev_rougeL=pd.read_csv(os.path.join(data_labels_dir,f\"{data_type}_data_{sc_type}.csv\"))\n    print(dev_rougeL.columns, dev_rougeL.shape)\n    dev_rougeL[\"text_id\"]=dev_ch_tid_df[\"text_id\"]\n    dev_rougeL[\"chosen\"]=dev_ch_tid_df[\"chosen\"]\n    print(dev_rougeL.columns, dev_rougeL.shape)\n    dev_rougeL.to_csv(os.path.join(data_dir,f\"{data_type}_data_{sc_type}_tid.csv\"), header=True, index=False)\n\n    data_type=\"test\"\n    test_df = pd.read_csv(os.path.join(df_dir,f\"{data_type}_set.csv\"))\n    test_ch_tid_df=add_chosen_text_id(test_df)\n    test_rougeL=pd.read_csv(os.path.join(data_labels_dir,f\"{data_type}_data_{sc_type}.csv\"))\n    print(test_rougeL.columns, test_rougeL.shape)\n    test_rougeL[\"text_id\"]=test_ch_tid_df[\"text_id\"]\n    test_rougeL[\"chosen\"]=test_ch_tid_df[\"chosen\"]\n    print(test_rougeL.columns, test_rougeL.shape)\n    test_rougeL.to_csv(os.path.join(data_dir,f\"{data_type}_data_{sc_type}_tid.csv\"), header=True, index=False)\n\n\n# sentence statistics \ndef data_stats(df):\n    groups = df.groupby(\"chosen\")\n    print(f\"DataFrame shape: {df.shape}\\n\\n\")\n    print(groups.describe()[\"rougeL\"])\n\n    \n# feature variable: ratio of thematic words in sentence\ndef thematic_ratio(them_words, word_list):\n    them_occ= sum( [ word_list.count(w) for w in set(them_words)&set(word_list)])\n    them_ratio=them_occ/len(word_list)\n    return them_ratio\n\n# feature variable: sentence position - \"By this, we get a high feature value towards the beginning and ending of the document, and a progressively decremented value towards the middle\" as mentioned in the paper\ndef s_position(t_position,tot_sent):\n    N=tot_sent\n    th=0.2*N\n    min_p= th*N\n    max_p= 2*th*N\n    \n    if t_position==tot_sent or t_position==1:\n        pos=1.0\n    else: \n        pos=math.cos((t_position - min_p)*((1/max_p) - min_p))\n        \n    return pos\n    \n# feature variable (failed experiment) - timeout, too much time needed for pos tagging - left for future work     \ndef prop_nouns(tokens):\n    if type(tokens)!=list:\n        tokens=eval(tokens)\n    pos= nltk.pos_tag(tokens)\n    tags_count=Counter(tag for _, tag in pos if tag==\"NNP\" or tag==\"NNPS\")\n    return tags_count[\"NNP\"]+tags_count[\"NNPS\"] \n\n\n# create sentence features\ndef feature_df(df, data_dir, data_type):\n    global pbar_cleaning\n    feat_df=pd.DataFrame()\n    \n    # tokenize sentences\n    pbar_cleaning=tqdm(total=df.shape[0], leave=True)\n    df[\"tokens\"] = df[\"sentence\"].apply(lambda x: sentence_cleaning(x)[1]) \n\n    # create sentence features based on paper \"Extractive Summarization using Deep Learning\"\n    # Source: [Extractive Summarization using Deep Learning](https://arxiv.org/pdf/1708.04439v1.pdf)\n    # 1. thematic words ratio\n    col = df.groupby(\"text_id\")[\"tokens\"].apply(sum)\n    thematic_cols= pd.DataFrame({\"text_id\": col.index, \"thematic\": [ [ t[0] for t in Counter(x).most_common(10) ] for x in col ]})  \n    df=df.join(thematic_cols[\"thematic\"], on='text_id' )\n    feat_df[\"thematic_ratio\"] = df.apply(lambda row: thematic_ratio(row.thematic, row.tokens) if len(row.tokens)>0 else 0.0, axis=1)\n\n    # 2. sentence position in the text\n    feat_df[\"text_position\"] = df.groupby(\"text_id\").cumcount().add(1)\n    df[\"text_position\"]=feat_df[\"text_position\"]\n    df[\"tot_sent\"] = df.groupby(\"text_id\")[\"sentence\"].transform(len)\n    feat_df[\"s_position\"] = df.apply(lambda row: s_position(row.text_position,row.tot_sent), axis=1)\n\n    # 3. sentence length - threshold=3\n    threshold=3\n    feat_df[\"len\"]= df[\"tokens\"].apply(lambda x: 0 if len(x)<threshold else len(x))\n\n    # 4. sentence position - paragraph relative\n    feat_df['s_pos_par'] = feat_df[\"s_position\"].values\n    feat_df.loc[feat_df.s_pos_par!=1.0, 's_pos_par']=0.0\n\n    # 5. numerals ratio\n    feat_df[\"num_ratio\"]=df[\"tokens\"].apply(lambda x: sum( [ 1 for t in x if t.isnumeric() ] )/len(x) if len(x)>0 else 0 )\n\n    # store sentence features\n    feat_df.to_csv(os.path.join(data_dir,f\"{data_type}_set_feats.csv\"), header=True, index=False)\n\n    return feat_df\n\n\ndef load_datasets(df_dir, sc_type=\"rougeL\"):\n    data_type=\"train1\"\n    print(f\"\\nLoading {data_type.capitalize()} Data . . \")\n    # train_data=feature_df(train_data, data_dir, data_type)\n    train_data=pd.read_csv(f\"/kaggle/input/summarizer-data/{data_type}_set_feats.csv\") \n    \n    train_data_rougeLTid=pd.read_csv(os.path.join(df_dir,f\"{data_type}_data_{sc_type}.csv\"))\n    train_data_rougeLTid[\"text_position\"] = train_data_rougeLTid.groupby(\"text_id\").cumcount().add(1)\n    \n    data_stats(train_data_rougeLTid)\n    labels_train=train_data_rougeLTid[\"rougeL\"].values\n    \n    train_data_feats=train_data\n    train_data[\"sentence\"]=train_data_rougeLTid[\"sentence\"]\n\n    train_data_feats=train_data_feats[[\"len\", \"s_position\", \"thematic_ratio\", \"s_pos_par\", \"num_ratio\"]]\n\n\n    data_type=\"dev\"\n    print(f\"\\n\\nLoading {data_type.capitalize()} Data . . \")\n    dev_data=pd.read_csv(f\"/kaggle/input/summarizer-data/{data_type}_set_feats.csv\") \n    \n    dev_data_rougeLTid=pd.read_csv(os.path.join(df_dir,f\"{data_type}_data_{sc_type}.csv\"))\n    dev_data_rougeLTid[\"text_position\"] = dev_data_rougeLTid.groupby(\"text_id\").cumcount().add(1)\n    \n    data_stats(dev_data_rougeLTid)\n    labels_dev=dev_data_rougeLTid[\"rougeL\"].values\n    \n    dev_data_feats=dev_data\n    dev_data[\"sentence\"]=dev_data_rougeLTid[\"sentence\"]\n    dev_data_feats=dev_data_feats[[\"len\", \"s_position\", \"thematic_ratio\", \"s_pos_par\", \"num_ratio\"]]\n\n\n    data_type=\"test\"\n    print(f\"\\n\\nLoading {data_type.capitalize()} Data . . \")\n    test_data=pd.read_csv(f\"/kaggle/input/summarizer-data/{data_type}_set_feats.csv\") \n    \n    test_data_rougeLTid=pd.read_csv(os.path.join(df_dir,f\"{data_type}_data_{sc_type}.csv\"))\n    test_data_rougeLTid[\"text_position\"] = test_data_rougeLTid.groupby(\"text_id\").cumcount().add(1)\n    \n    data_stats(test_data_rougeLTid)\n    labels_test=test_data_rougeLTid[\"rougeL\"].values\n    \n    test_data_feats=test_data\n    test_data[\"sentence\"]=test_data_rougeLTid[\"sentence\"]\n    test_data_feats=test_data_feats[[\"len\", \"s_position\", \"thematic_ratio\", \"s_pos_par\", \"num_ratio\"]]\n    \n    return train_data_rougeLTid, train_data_feats, labels_train, dev_data_rougeLTid, dev_data_feats, labels_dev, test_data_rougeLTid, test_data_feats, labels_test\n\n\ndef load_dataset_train(df_dir, sc_type=\"rougeL\", train_data_type=\"train1\"):\n    data_type=train_data_type\n    print(f\"\\nLoading {data_type.capitalize()} Data . . \")\n    train_data=pd.read_csv(f\"/kaggle/input/summarizer-data/{data_type}_set_feats.csv\") \n    \n    train_data_rougeLTid=pd.read_csv(os.path.join(df_dir,f\"{data_type}_data_{sc_type}.csv\"))\n    train_data_rougeLTid[\"text_position\"] = train_data_rougeLTid.groupby(\"text_id\").cumcount().add(1)\n    \n    data_stats(train_data_rougeLTid)\n    labels_train=train_data_rougeLTid[\"rougeL\"].values\n    \n    train_data_feats=train_data\n    train_data[\"sentence\"]=train_data_rougeLTid[\"sentence\"]\n\n    train_data_feats=train_data_feats[[\"len\", \"s_position\", \"thematic_ratio\", \"s_pos_par\", \"num_ratio\"]]\n    \n    return train_data_rougeLTid, train_data_feats, labels_train\n\n\n    \ndef load_dataset_test(df_dir, sc_type=\"rougeL\"):\n    \n    data_type=\"test\"\n    print(f\"\\nLoading {data_type.capitalize()} Data . . \")\n    test_data=pd.read_csv(f\"/kaggle/input/summarizer-data/{data_type}_set_feats.csv\") \n    \n    test_data_rougeLTid=pd.read_csv(os.path.join(df_dir,f\"{data_type}_data_{sc_type}.csv\"))\n    test_data_rougeLTid[\"text_position\"] = test_data_rougeLTid.groupby(\"text_id\").cumcount().add(1)\n                                                 \n    data_stats(test_data_rougeLTid)\n    labels_test=test_data_rougeLTid[\"rougeL\"].values\n    \n    test_data_feats=test_data\n    test_data[\"sentence\"]=test_data_rougeLTid[\"sentence\"]\n    test_data_feats=test_data_feats[[\"len\", \"s_position\", \"thematic_ratio\", \"s_pos_par\", \"num_ratio\"]]\n    \n    return test_data_rougeLTid, test_data_feats, labels_test\n\n\n# Rouge scores of predicted summaries\ndef scoring(pred_summary, ref_summary,doc_id):\n    pred_summary = re.sub(r'\\n+', '', pred_summary)\n    \n    score_ind={\"precision\":0, \"recall\":1, \"fmeasure\":2}\n    r_scorer=rouge_scorer.RougeScorer([\"rouge1\",\"rouge2\",\"rougeL\"], use_stemmer=True)\n\n    rouge=r_scorer.score(ref_summary,pred_summary)\n    \n    r1=rouge[\"rouge1\"][score_ind[\"fmeasure\"]]\n    r2=rouge[\"rouge2\"][score_ind[\"fmeasure\"]]\n    rL=rouge[\"rougeL\"][score_ind[\"fmeasure\"]]\n    \n    with open(os.path.join(data_dir,\"summaResults.txt\"), \"a\", encoding=\"utf-8\") as writer:\n        file_str=f\"> Document #{doc_id}\\n\\nPredicted Summary\\n{pred_summary}\\n\\nReference Summary\\n{ref_summary}\\n\\nR1: {r1:.9f} | R2: {r2:.9f} | RL: {rL:.9f}\\n\\n\\n\\n\"\n        writer.write(file_str)\n        \n    return r1,r2,rL\n\n\n# functions to create the predicted summary after the sentence scoring\ndef summarization(sentences):\n    sentences = [ str(s) for s in sentences if len(s)>0 ]\n    if len(sentences)==0:\n        return \" \"\n    \n    summary = \".\".join(sentences)\n    if summary[-1] is not \".\":\n        summary+=\".\"\n    \n    return summary\n\n\n# keep s_num sentences with RougeL score > threshold th to create the predicted summary\ndef create_summary(df,s_num,th=0.19):\n    n_largest=df.groupby([\"text_id\"])[\"text_position\",\"sentence\",\"pred_rougeL\"].apply(lambda x: x.nlargest(s_num,columns=[\"pred_rougeL\"]).sort_index())\n    max_rL=max(n_largest[\"pred_rougeL\"])\n    sent = [ n_largest.loc[n_largest[\"pred_rougeL\"].idxmax(),\"sentence\"] ]\n    sent.extend( [ s for s,r in zip(n_largest[\"sentence\"].values, n_largest[\"pred_rougeL\"].values) if r > th and r!=max_rL] )\n    summary = summarization( sent)   \n    return summary\n\n\ndef get_ref_summary(ref_summaries,doc_id):\n    ref_sum=ref_summaries.loc[ref_summaries.text_id==doc_id, \"summary\"].values\n    \n    if type(ref_sum)!=str:\n        ref_sum=ref_sum[0]\n#     if type(ref_sum)!=str:\n#         ref_sum=\" \"\n    return ref_sum\n\n\ndef get_preds(model,docX):\n    return model.predict(docX)\n\n\ndef doc_summary(model,document,ref_summaries,doc_id,th,s_num=4) :\n    global doc_p_bar, tot_r1, tot_r2, tot_rL\n\n    cols=[\"len\", \"s_position\", \"thematic_ratio\", \"s_pos_par\", \"num_ratio\"]\n    feats=document[cols]\n\n\n    preds=get_preds(model,feats)\n    document[\"pred_rougeL\"]=preds.tolist()\n    pred_summary=create_summary(document,s_num,th)\n    ref_summary=get_ref_summary(ref_summaries,doc_id)\n    r1,r2,rL = scoring(pred_summary,ref_summary,doc_id)\n    tot_r1.append(r1)\n    tot_r2.append(r2)\n    tot_rL.append(rL)\n    doc_p_bar.postfix[1] = np.mean(tot_r1)\n    doc_p_bar.postfix[3] = np.mean(tot_r2)\n    doc_p_bar.postfix[5] = np.mean(tot_rL)\n    doc_p_bar.postfix[6][\"value\"] = doc_id\n    doc_p_bar.update(1)\n\n    del document\n    del feats\n    del preds\n    del pred_summary\n    del ref_summary\n    gc.collect()\n    \n    return r1, r2, rL\n\n\n# memory clean\n# for v in globals():\n#     print(v)\n#     if str(v) not \"__name__\":\n#         del v\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-09-04T14:57:54.985242Z","iopub.execute_input":"2022-09-04T14:57:54.986001Z","iopub.status.idle":"2022-09-04T14:57:55.261709Z","shell.execute_reply.started":"2022-09-04T14:57:54.985951Z","shell.execute_reply":"2022-09-04T14:57:55.260565Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"0"},"metadata":{}}]},{"cell_type":"markdown","source":"## Essential DataFrames Creation\n___","metadata":{}},{"cell_type":"code","source":"# test_json_path=\"/kaggle/input/summarizer-data/test.jsonl\"\n# json_to_df(test_json_path,\"test\")\n\n# dev_json_path=\"/kaggle/input/summarizer-data/dev.jsonl\"\n# json_to_df(dev_json_path,\"dev\")\n\n# load saved dataframes and create labels\n# train_data_list, dev_data, test_data = text_proc_labels(data_dir)\n# train_data_list, dev_data = text_proc_labels(data_dir)\n\n# load dataframes with labels\n# df_add_tid(data_dir, df_dir)\n\n\nFileLinks(\".\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-output":false,"scrolled":true,"execution":{"iopub.status.busy":"2022-08-07T09:34:09.838118Z","iopub.execute_input":"2022-08-07T09:34:09.839240Z","iopub.status.idle":"2022-08-07T09:34:09.848778Z","shell.execute_reply.started":"2022-08-07T09:34:09.839206Z","shell.execute_reply":"2022-08-07T09:34:09.847786Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train, Dev & Test features\n___","metadata":{}},{"cell_type":"code","source":"# train_data1, train_data2 =  pd.read_csv(f\"/kaggle/input/summarizer-data/train1_data_rougeL.csv\"), pd.read_csv(f\"/kaggle/input/summarizer-data/train2_data_rougeL.csv\")\n# train_data3, train_data4 = pd.read_csv(f\"/kaggle/input/summarizer-data/train3_data_rougeL.csv\"), pd.read_csv(f\"/kaggle/input/summarizer-data/train4_data_rougeL.csv\")\n# dev_data = pd.read_csv(f\"/kaggle/input/summarizer-data/dev_set.csv\")\n# test_data = pd.read_csv(f\"/kaggle/input/summarizer-data/test_set.csv\")\n\n# _____________________________________________________________________________\n# feature_df(test_data,data_dir,\"test\")\n# feature_df(dev_data,data_dir,\"dev\")\n# FileLinks(\".\")\n\n# data_type=\"train1\"\n# feature_df(train_data1,data_dir,data_type)\n# FileLinks(\".\")\n\n# data_type=\"train2\"\n# feature_df(train_data2,data_dir,data_type)\n# FileLinks(\".\")\n\n# data_type=\"train3\"\n# feature_df(train_data3,data_dir,data_type)\n# FileLinks(\".\")\n\n# data_type=\"train4\"\n# feature_df(train_data4,data_dir,data_type)\n# FileLinks(\".\")\n","metadata":{"execution":{"iopub.status.busy":"2022-09-01T13:52:07.646787Z","iopub.execute_input":"2022-09-01T13:52:07.647216Z","iopub.status.idle":"2022-09-01T13:52:07.663607Z","shell.execute_reply.started":"2022-09-01T13:52:07.647182Z","shell.execute_reply":"2022-09-01T13:52:07.658951Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load Train, Dev & Test feature and label datasets\n___","metadata":{}},{"cell_type":"code","source":"score_type=\"rougeL\"\ntrain_data, train_data_feats, labels, dev_data, dev_data_feats, labels_dev, test_data, test_data_feats, labels_test = load_datasets(df_dir, sc_type=score_type)\ntrain_data1, train_data_feats1, labels1 = load_dataset_train(df_dir, sc_type=score_type, train_data_type=\"train1\")\ntrain_data2, train_data_feats2, labels2 = load_dataset_train(df_dir, sc_type=score_type, train_data_type=\"train2\")\n# train_data3, train_data_feats3, labels3 = load_dataset_train(df_dir, sc_type=score_type, train_data_type=\"train3\")\n# train_data4, train_data_feats4, labels4 = load_dataset_train(df_dir, sc_type=score_type, train_data_type=\"train4\")\n\nFileLinks(\".\")","metadata":{"execution":{"iopub.status.busy":"2022-09-04T14:57:55.263218Z","iopub.execute_input":"2022-09-04T14:57:55.263523Z","iopub.status.idle":"2022-09-04T14:58:47.938614Z","shell.execute_reply.started":"2022-09-04T14:57:55.263494Z","shell.execute_reply":"2022-09-04T14:58:47.937096Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"\nLoading Train1 Data . . \nDataFrame shape: (2444760, 5)\n\n\n            count      mean       std  min      25%       50%       75%  max\nchosen                                                                      \n0       2318103.0  0.105294  0.120241  0.0  0.05000  0.083333  0.122449  1.0\n1        126657.0  0.349503  0.234453  0.0  0.17734  0.306667  0.462428  1.0\n\n\nLoading Dev Data . . \nDataFrame shape: (1050564, 5)\n\n\n            count      mean       std  min       25%       50%       75%  max\nchosen                                                                       \n0       1021110.0  0.109909  0.127365  0.0  0.051948  0.088889  0.128205  1.0\n1         29454.0  0.457510  0.297183  0.0  0.216561  0.392666  0.666667  1.0\n\n\nLoading Test Data . . \nDataFrame shape: (1037722, 5)\n\n\n            count      mean       std  min       25%       50%       75%  max\nchosen                                                                       \n0       1008495.0  0.109895  0.127409  0.0  0.051948  0.088889  0.128205  1.0\n1         29227.0  0.456524  0.300030  0.0  0.210927  0.389610  0.666667  1.0\n\nLoading Train1 Data . . \nDataFrame shape: (2444760, 5)\n\n\n            count      mean       std  min      25%       50%       75%  max\nchosen                                                                      \n0       2318103.0  0.105294  0.120241  0.0  0.05000  0.083333  0.122449  1.0\n1        126657.0  0.349503  0.234453  0.0  0.17734  0.306667  0.462428  1.0\n\nLoading Train2 Data . . \nDataFrame shape: (2565751, 5)\n\n\n            count      mean       std  min       25%       50%       75%  max\nchosen                                                                       \n0       2522457.0  0.111558  0.129375  0.0  0.054054  0.090909  0.130435  1.0\n1         43294.0  0.578162  0.288707  0.0  0.343750  0.571429  0.823529  1.0\n","output_type":"stream"},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"./\n  __notebook_source__.ipynb","text/html":"./<br>\n&nbsp;&nbsp;<a href='./__notebook_source__.ipynb' target='_blank'>__notebook_source__.ipynb</a><br>"},"metadata":{}}]},{"cell_type":"markdown","source":"## Scorer Fine Tuning\n___","metadata":{}},{"cell_type":"code","source":"# grid search - Dev Set Tuning\ndef estimator_tuning(X, y, estimator=SGDRegressor(), scaler=StandardScaler()):\n    model_grid = make_pipeline(scaler, estimator )\n\n    param_grid = {\n        \"sgdregressor__alpha\": [8.192e-10],\n        \"sgdregressor__tol\": [6.4e-5],\n        \"sgdregressor__epsilon\": [3.2e-4],\n        \"sgdregressor__loss\": [\"squared_error\"],\n        \"sgdregressor__penalty\": [\"elasticnet\"],\n        \"sgdregressor__learning_rate\": [\"adaptive\"]\n    }\n    # \"sgdregressor__alpha\": 5.0 ** -np.arange(2, 7)\n    # \"sgdregressor__tol\": 10.0 ** -np.arange(2, 7)\n    # \"sgdregressor__loss\": [\"squared_error\", \"huber\", \"epsilon_insensitive\"]\n    # sgdregressor__penalty\": [\"l2\", \"l1\", \"elasticnet\"]\n    # \"sgdregressor__learning_rate\": [\"constant\", \"optimal\", \"invscaling\", \"adaptive\"]\n\n    g_search = GridSearchCV(model_grid, param_grid, verbose=9, return_train_score=True, cv=2)\n    g_search.fit(X, y)\n    \n    with open(os.path.join(\"/kaggle/working/Data\",f\"{estimator[:-2]}_grid_search_results.txt\"), \"w\", encoding=\"utf-8\" ) as writer:\n          writer.write(f\"Best ParametersL:\\n{g_search.best_params_}\\n\\n\\n{g_search.cv_results_}\")\n\n    print(f\"Best score: { g_search.best_score_}\\nParams: {g_search.best_params_}\")\n    return g_search.best_estimator_\n\n\nbest_model=estimator_tuning(dev_data_feats, labels_dev)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-09-04T14:58:47.941957Z","iopub.execute_input":"2022-09-04T14:58:47.942425Z","iopub.status.idle":"2022-09-04T14:59:03.072595Z","shell.execute_reply.started":"2022-09-04T14:58:47.942391Z","shell.execute_reply":"2022-09-04T14:59:03.071450Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Fitting 2 folds for each of 1 candidates, totalling 2 fits\n[CV 1/2] END sgdregressor__alpha=8.192e-10, sgdregressor__epsilon=0.00032, sgdregressor__learning_rate=adaptive, sgdregressor__loss=squared_error, sgdregressor__penalty=elasticnet, sgdregressor__tol=6.4e-05;, score=(train=0.195, test=0.189) total time=   3.7s\n[CV 2/2] END sgdregressor__alpha=8.192e-10, sgdregressor__epsilon=0.00032, sgdregressor__learning_rate=adaptive, sgdregressor__loss=squared_error, sgdregressor__penalty=elasticnet, sgdregressor__tol=6.4e-05;, score=(train=0.190, test=0.194) total time=   3.7s\nBest score: 0.19161433549061424\nParams: {'sgdregressor__alpha': 8.192e-10, 'sgdregressor__epsilon': 0.00032, 'sgdregressor__learning_rate': 'adaptive', 'sgdregressor__loss': 'squared_error', 'sgdregressor__penalty': 'elasticnet', 'sgdregressor__tol': 6.4e-05}\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Best Scorer\n___","metadata":{}},{"cell_type":"code","source":"def estimator_training(train_data_feats, labels, test_data_feats, labels_test):\n    model = make_pipeline(StandardScaler(), SGDRegressor(alpha=8.192e-10, max_iter=1000, tol=6.4e-5, epsilon=3.2e-4, learning_rate=\"adaptive\", loss=\"squared_error\", penalty=\"elasticnet\"))\n    model.fit(train_data_feats, labels)\n\n    cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n    \n    print('\\n\\n> Scores')\n    # evaluate model\n    test_samples=1000\n    # MAE\n    scores_mae = cross_val_score(model, test_data_feats.iloc[:test_samples,:] , labels_test[:test_samples], scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1)\n    scores_mae = np.absolute(scores_mae)\n    print('Mean MAE: %.3f (%.3f)' % (np.mean(scores_mae), np.std(scores_mae)))\n    # MSE\n    scores_mse = cross_val_score(model, test_data_feats.iloc[:test_samples,:] , labels_test[:test_samples], scoring='neg_mean_squared_error', cv=cv, n_jobs=-1)\n    scores_mse = np.absolute(scores_mse)\n    print('Mean MSE: %.3f (%.3f)' % (np.mean(scores_mse), np.std(scores_mse)))\n    # R2    \n    scores_r2 = cross_val_score(model, test_data_feats.iloc[:test_samples,:] , labels_test[:test_samples], scoring='r2', cv=cv, n_jobs=-1)\n    # scores_r2 = np.absolute(scores_r2)\n    print('Mean R2: %.3f (%.3f)' % (np.mean(scores_r2), np.std(scores_r2)))\n    \n    return model\n\n\ntrain_data=pd.DataFrame(np.vstack((train_data1,train_data2)))\ntrain_data_feats=pd.DataFrame(np.vstack((train_data_feats1,train_data_feats2)))\nlabels=np.concatenate((labels1,labels2),axis=0)\nscore_type=\"rougeL\"\ntest_data, test_data_feats, labels_test = load_dataset_test(df_dir, sc_type=score_type)\nmodel=estimator_training(train_data_feats, labels, test_data_feats, labels_test)\n\npkl_filepath=os.path.join(\"/kaggle/working/Data\",\"SGD_model.pkl\")\nwith open(pkl_filepath,\"wb\") as model_writer:\n    pickle.dump(model, model_writer)","metadata":{"execution":{"iopub.status.busy":"2022-09-04T14:59:03.073899Z","iopub.execute_input":"2022-09-04T14:59:03.074752Z","iopub.status.idle":"2022-09-04T15:00:04.821785Z","shell.execute_reply.started":"2022-09-04T14:59:03.074716Z","shell.execute_reply":"2022-09-04T15:00:04.820224Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"\nLoading Test Data . . \nDataFrame shape: (1037722, 5)\n\n\n            count      mean       std  min       25%       50%       75%  max\nchosen                                                                       \n0       1008495.0  0.109895  0.127409  0.0  0.051948  0.088889  0.128205  1.0\n1         29227.0  0.456524  0.300030  0.0  0.210927  0.389610  0.666667  1.0\n\n\n> Scores\nMean MAE: 0.069 (0.007)\nMean MSE: 0.016 (0.005)\nMean R2: 0.156 (0.158)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Evaluation\n___","metadata":{}},{"cell_type":"code","source":"score_type=\"rougeL\"\ntest_data, test_data_feats, labels_test = load_dataset_test(df_dir, sc_type=score_type)\n\nsc_type=\"rougeL\"\ndata_type=\"test\"\nlabels=test_data[\"rougeL\"].values\n\nsummaries=pd.read_csv(\"/kaggle/input/summarizer-data/test_data_rougeL_summaries_grouped.csv\")\nsummaries=pd.DataFrame(summaries[\"summary\"])\nsummaries[\"text_id\"]=summaries.index\n\ntest_data_feats[\"sentence\"]=test_data[\"sentence\"]\ntest_data_feats[\"text_id\"]=test_data[\"text_id\"]\ntest_data_feats[\"text_position\"]=test_data[\"text_position\"]\n\nX=test_data_feats\ny=pd.DataFrame(labels_test)","metadata":{"execution":{"iopub.status.busy":"2022-09-04T15:00:04.824082Z","iopub.execute_input":"2022-09-04T15:00:04.824596Z","iopub.status.idle":"2022-09-04T15:00:08.764818Z","shell.execute_reply.started":"2022-09-04T15:00:04.824536Z","shell.execute_reply":"2022-09-04T15:00:08.763837Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"\nLoading Test Data . . \nDataFrame shape: (1037722, 5)\n\n\n            count      mean       std  min       25%       50%       75%  max\nchosen                                                                       \n0       1008495.0  0.109895  0.127409  0.0  0.051948  0.088889  0.128205  1.0\n1         29227.0  0.456524  0.300030  0.0  0.210927  0.389610  0.666667  1.0\n","output_type":"stream"}]},{"cell_type":"code","source":"data_dir=\"/kaggle/working/Data\"\nopen(os.path.join(data_dir,\"summaResults.txt\"),\"w\")\n%cd /kaggle/working\nFileLinks(\".\")","metadata":{"execution":{"iopub.status.busy":"2022-09-04T15:00:08.766189Z","iopub.execute_input":"2022-09-04T15:00:08.766511Z","iopub.status.idle":"2022-09-04T15:00:08.780179Z","shell.execute_reply.started":"2022-09-04T15:00:08.766480Z","shell.execute_reply":"2022-09-04T15:00:08.779198Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"/kaggle/working\n","output_type":"stream"},{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"./\n  __notebook_source__.ipynb\n./Data/\n  SGD_model.pkl\n  SGDRegressor()_grid_search_results.txt\n  summaResults.txt","text/html":"./<br>\n&nbsp;&nbsp;<a href='./__notebook_source__.ipynb' target='_blank'>__notebook_source__.ipynb</a><br>\n./Data/<br>\n&nbsp;&nbsp;<a href='./Data/SGD_model.pkl' target='_blank'>SGD_model.pkl</a><br>\n&nbsp;&nbsp;<a href='./Data/SGDRegressor()_grid_search_results.txt' target='_blank'>SGDRegressor()_grid_search_results.txt</a><br>\n&nbsp;&nbsp;<a href='./Data/summaResults.txt' target='_blank'>summaResults.txt</a><br>"},"metadata":{}}]},{"cell_type":"code","source":"cols=[\"len\", \"s_position\", \"thematic_ratio\", \"s_pos_par\", \"num_ratio\"]\nfeats=test_data_feats[cols]\npreds=model.predict(feats).tolist()\npreds_df=pd.DataFrame({\"sentence\": test_data_feats[\"sentence\"], \"text_id\": test_data_feats[\"text_id\"], \"text_position\": test_data_feats[\"text_position\"], \"pred_rougeL\": preds})\n\n# max sentences to keep -> s_num_max & threshold for sentence rougeL score\nthreshold=0.5\ns_num_max=5\n# create predicted summaries\npred_sums=preds_df.groupby(\"text_id\").apply(lambda doc: create_summary(doc,s_num_max,threshold))\n\n# scores of predicted summaries\nr_scorer=rouge_scorer.RougeScorer([\"rouge1\",\"rouge2\",\"rougeL\"], use_stemmer=True)\nscores=[ r_scorer.score(ref,pred) for ref,pred in zip(summaries[\"summary\"].values,pred_sums) ]\n\n# score dataframes for visualization\nscore_df=pd.DataFrame(scores)\nscore_df[\"ref_sum\"]=summaries[\"summary\"].values\nscore_df[\"pred_sum\"]=pred_sums\nscore_df[\"text_id\"]=score_df.index\n\nscore_df[\"rouge1_precision\"]=score_df[\"rouge1\"].apply(lambda x: x[0])\nscore_df[\"rouge1_recall\"]=score_df[\"rouge1\"].apply(lambda x: x[1])\nscore_df[\"rouge1_fmeasure\"]=score_df[\"rouge1\"].apply(lambda x: x[2])\ndel score_df[\"rouge1\"]\n\nscore_df[\"rouge2_precision\"]=score_df[\"rouge2\"].apply(lambda x: x[0])\nscore_df[\"rouge2_recall\"]=score_df[\"rouge2\"].apply(lambda x: x[1])\nscore_df[\"rouge2_fmeasure\"]=score_df[\"rouge2\"].apply(lambda x: x[2])\ndel score_df[\"rouge2\"]\n\nscore_df[\"rougeL_precision\"]=score_df[\"rougeL\"].apply(lambda x: x[0])\nscore_df[\"rougeL_recall\"]=score_df[\"rougeL\"].apply(lambda x: x[1])\nscore_df[\"rougeL_fmeasure\"]=score_df[\"rougeL\"].apply(lambda x: x[2])\ndel score_df[\"rougeL\"]\n\n\n# store predictions & their respective scores\ndef log_to_file(row):\n    pred_summary = re.sub(r'\\n+', '', row['pred_sum'])\n    doc_str=f\"> Document #{row['text_id']}\\n\\n#Predicted Summary\\n{pred_summary}\\n\\n#Reference Summary\\n{row['ref_sum']}\\n\\nR1: {row['rouge1_fmeasure']}  |  R2: {row['rouge2_fmeasure']}  |  RL:  {row['rougeL_fmeasure']}\\n\\n\\n\\n\"\n    return doc_str\n\nfile_str=score_df.apply(lambda row: log_to_file(row), axis=1)\n\nwith open(os.path.join(data_dir,f\"summaResults_{data_type}_N{s_num_max}_th{threshold}.txt\"),\"w\",encoding=\"utf-8\") as log_writer:\n    for row_str in file_str:\n        log_writer.write(row_str)\n        \nFileLinks(\".\")","metadata":{"execution":{"iopub.status.busy":"2022-09-04T15:28:07.349900Z","iopub.execute_input":"2022-09-04T15:28:07.350493Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"score_stats=score_df.describe()\nscore_stats.to_csv(os.path.join(data_dir,f\"results_stats_{data_type}_N{s_num_max}_th{threshold}.csv\"), header=True, index=True)\nscore_stats_plt=score_df.describe()\nscores_to_plot=score_stats_plt.iloc[1:,1:]\nscores_to_plot","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Score Visualization\n___","metadata":{}},{"cell_type":"code","source":"score_stats_plt=score_df.describe()\nscores_to_plot=score_stats_plt.iloc[1:,1:]\ndata_dir=\"/kaggle/working/Data\"\n\nplot_types=[\"area\", \"bar\", \"box\", \"kde\", \"line\"]\nfor kind in plot_types:\n    scores_to_plot.plot(kind=kind,figsize=(24,12), grid=True, rot=0)\n    plt.savefig(os.path.join(data_dir,f\"score_stats_{data_type}_{kind}.png\"),dpi=300)\n\nFileLinks(\".\")","metadata":{"execution":{"iopub.status.busy":"2022-08-07T10:25:45.931527Z","iopub.execute_input":"2022-08-07T10:25:45.932962Z","iopub.status.idle":"2022-08-07T10:26:00.702392Z","shell.execute_reply.started":"2022-08-07T10:25:45.932883Z","shell.execute_reply":"2022-08-07T10:26:00.700731Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## üìö References\n\n1. [Named Entity Recognition (NER) with TensorflowNamed Entity Recognition (NER) with Tensorflow](https://www.kaggle.com/code/naseralqaydeh/named-entity-recognition-ner-with-tensorflow)\n2. [Extractive Summarization using Deep LearningExtractive Summarization using Deep Learning](https://arxiv.org/pdf/1708.04439v1.pdf)\n3. [NLTK](https://www.bogotobogo.com/python/NLTK/Stemming_NLTK.php)\n4. [Text Features Library](https://github.com/pmbaumgartner/text-feat-lib/tree/master/notebooks)\n5. [SummaRuNNer: a recurrent neural network based sequence model for extractive summarization of documents](https://arxiv.org/abs/1611.04230)\n6. [Applying Text Classification Using Logistic Regression](https://medium.com/analytics-vidhya/applying-text-classification-using-logistic-regression-a-comparison-between-bow-and-tf-idf-1f1ed1b83640)\n7. [Text Summarization in Python using Extractive method (including end-to-end implementation)](https://medium.com/analytics-vidhya/text-summarization-in-python-using-extractive-method-including-end-to-end-implementation-2688b3fd1c8c)\n8. [An intro to ROUGE, and how to use it to evaluate summaries](https://www.freecodecamp.org/news/what-is-rouge-and-how-it-works-for-evaluation-of-summaries-e059fb8ac840/)\n9. [A Comparative Study of Classifiers for Extractive Text Summarization](https://www.researchgate.net/profile/Anshuman-Pattanaik/publication/340111386_A_Comparative_Study_of_Classifiers_for_Extractive_Text_Summarization/links/5f81d18792851c14bcbc59e1/A-Comparative-Study-of-Classifiers-for-Extractive-Text-Summarization.pdf)\n\n","metadata":{}}]}