{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from heapq import heapify\nimport os\nimport json\nfrom typing import Counter\nimport pandas as pd\nimport re\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pickle\n\nimport math\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer, PorterStemmer, SnowballStemmer, LancasterStemmer\nfrom nltk.tokenize import sent_tokenize, word_tokenize\nfrom nltk import pos_tag\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport sklearn.model_selection\nfrom sklearn.model_selection import RepeatedKFold, cross_val_score\nimport sklearn.preprocessing as preproc\nfrom sklearn.feature_extraction import text\nfrom sklearn.svm import LinearSVR\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression, ElasticNet, ElasticNetCV, LassoLars, SGDRegressor\nfrom sklearn.metrics import confusion_matrix, mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, MaxAbsScaler\nfrom sklearn.pipeline import make_pipeline\nfrom IPython.display import FileLink, FileLinks\n\nfrom tqdm import tqdm\n\n!pip install rouge-score\nfrom rouge_score import rouge_scorer\n\nimport psutil\nimport gc\nfrom collections import Counter","metadata":{"execution":{"iopub.status.busy":"2022-08-01T21:07:30.763291Z","iopub.execute_input":"2022-08-01T21:07:30.763794Z","iopub.status.idle":"2022-08-01T21:07:47.019961Z","shell.execute_reply.started":"2022-08-01T21:07:30.763688Z","shell.execute_reply":"2022-08-01T21:07:47.018801Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting rouge-score\n  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: absl-py in /opt/conda/lib/python3.7/site-packages (from rouge-score) (1.1.0)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.7/site-packages (from rouge-score) (3.7)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from rouge-score) (1.21.6)\nRequirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.7/site-packages (from rouge-score) (1.16.0)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from nltk->rouge-score) (1.0.1)\nRequirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from nltk->rouge-score) (8.0.4)\nRequirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.7/site-packages (from nltk->rouge-score) (2021.11.10)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from nltk->rouge-score) (4.64.0)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from click->nltk->rouge-score) (4.12.0)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->click->nltk->rouge-score) (3.8.0)\nRequirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->click->nltk->rouge-score) (4.1.1)\nBuilding wheels for collected packages: rouge-score\n  Building wheel for rouge-score (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24955 sha256=0e9265f834a02b5d183596e38e332f2ba321b261d96e1033b5d8a85643a937fa\n  Stored in directory: /root/.cache/pip/wheels/84/ac/6b/38096e3c5bf1dc87911e3585875e21a3ac610348e740409c76\nSuccessfully built rouge-score\nInstalling collected packages: rouge-score\nSuccessfully installed rouge-score-0.1.2\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"\ndef json_to_df(json_path,type):\n    with open(json_path, \"r\", encoding=\"utf-8\") as f: \n        lines = [eval(l) for l in f.readlines()]\n\n    # exclude lines with surrogates in their text/summary\n    surr = [ i for i,l in enumerate(lines) for k in l.keys() if k in [\"text\",\"summary\"] and re.search(r'[\\uD800-\\uDFFF]', l[k])!=None ]\n\n    lines = [ l for i,l in zip( range(len(lines)),lines ) if i not in surr ]\n\n    cols=[ \"title\",\t\"date\",\t\"text\",\t\"summary\", \"compression\", \"coverage\", \"density\", \"compression_bin\", \"coverage_bin\"]\n\n    # we need only the extractive summaries as we are building an extractive summarizer\n    data=[ [ l[k] for k in l.keys() if k in cols ] for l in lines if l[\"density_bin\"]==\"extractive\" ]\n    df = pd.DataFrame(data,columns=cols)\n\n    df.to_csv(f\"..{os.sep}Data{os.sep}DataFrames{os.sep}{type}_set.csv\", header=True, index=False )\n\n    return df\n\n\n# text processing functions\n\n# A list of contractions from http://stackoverflow.com/questions/19790188/expanding-english-language-contractions-in-python\ncontractions = { \"ain't\": \"am not\", \"aren't\": \"are not\", \"can't\": \"cannot\", \"can't've\": \"cannot have\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \"couldn't've\": \"could not have\", \"didn't\": \"did not\", \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hadn't've\": \"had not have\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\", \"he'd've\": \"he would have\", \"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'll\": \"how will\", \"how's\": \"how is\", \"i'd\": \"i would\", \"i'll\": \"i will\", \"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'll\": \"it will\", \"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\", \"mightn't\": \"might not\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"needn't\": \"need not\", \"oughtn't\": \"ought not\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"she'd\": \"she would\", \"she'll\": \"she will\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"that'd\": \"that would\", \"that's\": \"that is\", \"there'd\": \"there had\", \"there's\": \"there is\", \"they'd\": \"they would\", \"they'll\": \"they will\", \"they're\": \"they are\", \"they've\": \"they have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'll\": \"we will\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what're\": \"what are\", \"what's\": \"what is\", \"what've\": \"what have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"who'll\": \"who will\", \"who's\": \"who is\", \"won't\": \"will not\", \"wouldn't\": \"would not\", \"you'd\": \"you would\", \"you'll\": \"you will\", \"you're\": \"you are\" }\n\n\ndef sentence_cleaning(text, remove_stopwords = True, sub_contractions=True, stemming=True):\n    global pbar_cleaning\n    pbar_cleaning.update(1)\n    \n    # Convert words to lower case\n    text = text.lower()\n    toks = word_tokenize(text)\n      \n    # Replace contractions with their longer forms \n    if sub_contractions:\n        new_text = []\n        for word in toks:\n            if word in contractions:\n                new_text.append(contractions[word])\n            else:\n                new_text.append(word)\n    \n    text = \" \".join(new_text)\n\n    # Format words and remove unwanted characters\n    text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n    text = re.sub(r'\\<a href', ' ', text)\n    text = re.sub(r'&amp;', '', text) \n    text = re.sub(r'[_\"\\-;%()|+&=*%.,!?:#$@\\[\\]/]', ' ', text)\n    text = re.sub(r'<br />', ' ', text)\n    text = re.sub(r'\\'', ' ', text)\n\n    toks_clean = word_tokenize(text)\n    \n    if remove_stopwords:\n        stops = set(stopwords.words(\"english\"))\n        toks_clean = [w for w in toks_clean if not w in stops]\n\n    \n    if stemming: \n        stemmer=SnowballStemmer(language=\"english\")\n        toks_clean=[ stemmer.stem(w) for w in toks_clean ]\n\n    text = \" \".join(toks_clean)\n    \n    return text, toks_clean\n\n\ndef rouge_scoring(sentence,summary,sc_type=\"rougeL\",score=\"fmeasure\"):\n    global pbar\n    pbar.update(1)\n    r_scorer=rouge_scorer.RougeScorer([\"rouge1\",\"rouge2\",\"rougeL\"])\n    r_L=r_scorer.score(summary,sentence)\n    score_ind={\"precision\":0, \"recall\":1, \"fmeasure\":2}\n    \n    return r_L[sc_type][score_ind[score]]\n\n\ndef text_processing(df,data_type,df_dir,sc_type=\"rougeL\"):\n    global pbar\n    cols=[\"sentence\", \"summary\", \"text\"] \n    # new_df=pd.DataFrame()\n\n    # sentence split \n    sentences=[ sent_tokenize(t) for t in df[\"text\"].values ]\n\n    summaries=df[\"summary\"].values\n    sent_sum_text=[ [ s,summary,t  ] for s_list,summary,t in zip( sentences, summaries, df[\"text\"] ) for s in s_list ]\n    new_df=pd.DataFrame(sent_sum_text, columns=cols)\n    new_df[\"text_id\"]=new_df[\"text\"].factorize()[0]\n    new_df[\"chosen\"]= 0\n    ind = new_df[[ s in t for s,t in zip( new_df[\"sentence\"], new_df[\"summary\"] ) ]].index\n    new_df.loc[ind,\"chosen\"]=1\n    del new_df[\"text\"]\n    # for c in new_df.columns:\n    #   new_df[c]=new_df[c].astype(str)\n\n    # labels\n    # columns -> sentence: 0, summary: 1, text: 2\n    pbar = tqdm(total=new_df.shape[0] )\n    new_df[\"rougeL\"]= new_df.apply(lambda row: rouge_scoring(row[\"sentence\"],row[\"summary\"], sc_type=sc_type, score=\"fmeasure\" ), axis=1)\n    print(new_df[\"rougeL\"])\n\n    new_df[\"summary\"].to_csv(os.path.join(df_dir,f\"{data_type}_data_{sc_type}_summaries.csv\"), header=True, index=False)\n    del new_df[\"summary\"]\n    \n    new_df.to_csv(os.path.join(df_dir,f\"{data_type}_data_{sc_type}.csv\"), header=True, index=False)\n\n    return new_df\n\n\ndef add_chosen_text_id(df):\n    cols=[\"sentence\", \"summary\", \"text\"] \n    # new_df=pd.DataFrame()\n\n    # sentence split \n    sentences=[ sent_tokenize(t) for t in df[\"text\"].values ]\n    summaries=df[\"summary\"].values\n    \n    sent_sum_text=[ [ s,summary,t  ] for s_list,summary,t in zip( sentences, summaries, df[\"text\"] ) for s in s_list ]\n    new_df=pd.DataFrame(sent_sum_text, columns=cols)\n    \n    new_df[\"text_id\"]=new_df[\"text\"].factorize()[0]\n    new_df[\"chosen\"]= 0\n    ind = new_df[[ s in t for s,t in zip( new_df[\"sentence\"], new_df[\"summary\"] ) ]].index\n    new_df.loc[ind,\"chosen\"]=1\n    \n    del new_df[\"text\"]\n    del new_df[\"summary\"]\n    del new_df[\"sentence\"]\n    \n    return new_df\n\n\ndef create_labels(data_dir,df_dir=\"/kaggle/input/summarizer-data/\"):\n#     train_df = pd.read_csv(os.path.join(df_dir,\"train_set.csv\"))\n    dev_df = pd.read_csv(os.path.join(df_dir,\"dev_set.csv\"))\n    test_df = pd.read_csv(os.path.join(df_dir,\"test_set.csv\"))\n\n#     train_df1, train_df2, train_df3, train_df4 = np.array_split(train_df, 4)\n    splits=4\n#     train_data_list=[]\n#     for i,train_df in enumerate(np.array_split(train_df, splits)):\n#         train_data = text_processing(train_df,f\"train{i+1}\",data_dir,\"rougeL\")\n#         train_data_list.append(train_data)\n\n    dev_data = text_processing(dev_df,\"dev\",data_dir,\"rougeL\")\n    test_data = text_processing(test_df,\"test\",data_dir,\"rougeL\")\n    \n    return train_data_list, dev_data, test_data\n\n\ndef df_add_tid(data_dir, df_dir):\n    sc_type=\"rougeL\"\n    data_labels_dir=\"/kaggle/input/summarizer-data\"\n\n#     data_type=\"train\"\n#     train_df = pd.read_csv(os.path.join(df_dir,f\"{data_type}_set.csv\"))\n#     splits=4\n#     for i,df in enumerate(np.array_split(train_df, 4)):\n#         if i==2:\n#             data_type=f\"train{i+1}\"\n#             print(data_type+\"\\n\")\n#             train_ch_tid_df=add_chosen_text_id(df)\n#             train_rougeL=pd.read_csv(os.path.join(data_labels_dir,f\"{data_type}_data_{sc_type}.csv\"))\n#             print(train_rougeL.columns, train_rougeL.shape)\n#             train_rougeL[\"text_id\"]=train_ch_tid_df[\"text_id\"]\n#             train_rougeL[\"chosen\"]=train_ch_tid_df[\"chosen\"]\n#             print(train_rougeL.columns, train_rougeL.shape)\n#             train_rougeL.to_csv(os.path.join(data_dir,f\"{data_type}_data_{sc_type}_tid.csv\"), header=True, index=False)\n\n    data_type=\"dev\" \n    dev_df = pd.read_csv(os.path.join(df_dir,f\"{data_type}_set.csv\"))\n    dev_ch_tid_df=add_chosen_text_id(dev_df)\n    dev_rougeL=pd.read_csv(os.path.join(data_labels_dir,f\"{data_type}_data_{sc_type}.csv\"))\n    print(dev_rougeL.columns, dev_rougeL.shape)\n    dev_rougeL[\"text_id\"]=dev_ch_tid_df[\"text_id\"]\n    dev_rougeL[\"chosen\"]=dev_ch_tid_df[\"chosen\"]\n    print(dev_rougeL.columns, dev_rougeL.shape)\n    dev_rougeL.to_csv(os.path.join(data_dir,f\"{data_type}_data_{sc_type}_tid.csv\"), header=True, index=False)\n\n    data_type=\"test\"\n    test_df = pd.read_csv(os.path.join(df_dir,f\"{data_type}_set.csv\"))\n    test_ch_tid_df=add_chosen_text_id(test_df)\n    test_rougeL=pd.read_csv(os.path.join(data_labels_dir,f\"{data_type}_data_{sc_type}.csv\"))\n    print(test_rougeL.columns, test_rougeL.shape)\n    test_rougeL[\"text_id\"]=test_ch_tid_df[\"text_id\"]\n    test_rougeL[\"chosen\"]=test_ch_tid_df[\"chosen\"]\n    print(test_rougeL.columns, test_rougeL.shape)\n    test_rougeL.to_csv(os.path.join(data_dir,f\"{data_type}_data_{sc_type}_tid.csv\"), header=True, index=False)\n\n    \ndef data_stats(df):\n    groups = df.groupby(\"chosen\")\n    print(groups.describe()[\"rougeL\"])\n\n    \ndef thematic_ratio(them_words, word_list):\n    them_occ= sum( [ word_list.count(w) for w in set(them_words)&set(word_list)])\n    them_ratio=them_occ/len(word_list)\n    return them_ratio\n\n\ndef s_position(t_position,tot_sent):\n    N=tot_sent\n    th=0.2*N\n    min_p= th*N\n    max_p= 2*th*N\n    \n    if t_position==tot_sent or t_position==1:\n        pos=1.0\n    else: \n        pos=math.cos((t_position - min_p)*((1/max_p) - min_p))\n        \n    return pos\n    \n    \ndef prop_nouns(tokens):\n    if type(tokens)!=type(list):\n        tokens=eval(tokens)\n    pos= nltk.pos_tag(tokens)\n    tags_count=Counter(tag for _, tag in pos if tag==\"NNP\" or tag==\"NNPS\")\n    return tags_count[\"NNP\"]+tags_count[\"NNPS\"] \n\n\ndef feature_df(df, data_dir, data_type):\n    global pbar_cleaning\n    feat_df=pd.DataFrame()\n    \n    # tokenize\n    pbar_cleaning=tqdm(total=train_data.shape[0], leave=True)\n    df[\"tokens\"] = df[\"sentence\"].apply(lambda x: sentence_cleaning(x)[1]) \n    \n\n    # create sentence features\n    # 1. thematic words\n    col = df.groupby(\"text_id\")[\"tokens\"].apply(sum)\n    thematic_cols= pd.DataFrame({\"text_id\": col.index, \"thematic\": [ [ t[0] for t in Counter(x).most_common(10) ] for x in col ]})  \n    df=df.join(thematic_cols[\"thematic\"], on='text_id' )\n    feat_df[\"thematic_ratio\"] = df.apply(lambda row: thematic_ratio(row.thematic, row.tokens) if len(row.tokens)>0 else 0.0, axis=1)\n\n    # 2. sentence position in the text\n    df[\"text_position\"] = df.groupby(\"text_id\").cumcount().add(1)\n    df[\"tot_sent\"] = df.groupby(\"text_id\")[\"sentence\"].transform(len)\n    feat_df[\"s_position\"] = df.apply(lambda row: s_position(row.text_position,row.tot_sent), axis=1)\n\n    # 3. sentence length - threshold=3\n    threshold=3\n    feat_df[\"len\"]= df[\"tokens\"].apply(lambda x: 0 if len(x)<threshold else len(x))\n\n    # 4. sentence position - paragraph relative\n    feat_df['s_pos_par'] = feat_df[\"s_position\"].values\n    feat_df.loc[feat_df.s_pos_par!=1.0, 's_pos_par']=0.0\n\n    # 5. numerals\n    feat_df[\"num_ratio\"]=df[\"tokens\"].apply(lambda x: sum( [ 1 for t in x if t.isnumeric() ] )/len(x) if len(x)>0 else 0 )\n\n\n    # ?. Term Frequency-Inverse Sentence Frequency\n\n\n    # ?. proper nouns - not so useful\n    # train_data_feats[\"NNPs\"]=train_data_feats[\"tokens\"].apply(lambda x: prop_nouns(x) )\n\n\n\n\n    # train_data_feats=train_data[[\"len\",\"text_position\"]]\n    feat_df.to_csv(os.path.join(data_dir,f\"{data_type}_set_feats.csv\"), header=True, index=False)\n\n    return feat_df\n\n\n\n# SVM_scaler =  StandardScaler()\n# LR_scaler =  MinMaxScaler()\n# KNN_scaler =  StandardScaler()\n# # classifier parameters\n# KNN_n_num = 9\n# LR_C = 1.0\n# SVM_C = 0.001\n# algos={\n#   \"ElNet\": make_pipeline(SVM_scaler, SVC(C=SVM_C)),\n#   \"LR\":  make_pipeline(LR_scaler, LogisticRegression(C=LR_C)),\n#   \"KNN\": make_pipeline(KNN_scaler, KNeighborsClassifier(n_neighbors = KNN_n_num)),\n# }\n\n\ndef classifier_training(model,X_train,y_train):\n    model.fit(X_train,y_train)\n    preds=model.predict(X_train)\n    c_rep=classification_report(y_train,preds)\n    c_rep_dict=classification_report(y_train,preds,output_dict=True)\n    return model, c_rep, c_rep_dict\n\ndef classifier_validation(model,X_dev,y_dev):\n    preds=model.predict(X_dev)\n    c_rep = classification_report(y_dev,preds)\n    c_rep_dict=classification_report(y_dev,preds,output_dict=True)\n    return c_rep, c_rep_dict\n\ndef classifier_test(model,X_test,y_test):\n    preds=model.predict(X_test)\n    c_rep = classification_report(y_test,preds)\n    c_rep_dict=classification_report(y_test,preds,output_dict=True)\n    return c_rep, c_rep_dict\n\n\n# Training - Validation - Test pipeline\ndef classifier_T_V_T(X_train, y_train, X_dev, y_dev, X_test, y_test, algo_type=\"LR\"):\n    model=algos[algo_type]\n\n    model,c_rep_train,c_rep_dict_train=classifier_training(model,X_train,y_train)\n    c_rep_dev,c_rep_dict_dev=classifier_validation(model,X_dev,y_dev)\n    c_rep_test,c_rep_dict_test=classifier_validation(model,X_test,y_test)\n\n    return model, c_rep_train, c_rep_dict_train, c_rep_dev, c_rep_dict_dev, c_rep_test, c_rep_dict_test\n\n","metadata":{"execution":{"iopub.status.busy":"2022-08-01T22:57:00.092964Z","iopub.execute_input":"2022-08-01T22:57:00.093365Z","iopub.status.idle":"2022-08-01T22:57:00.155552Z","shell.execute_reply.started":"2022-08-01T22:57:00.093331Z","shell.execute_reply":"2022-08-01T22:57:00.154009Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"# memory clean\n# for v in globals():\n#     print(v)\n#     if str(v) not \"__name__\":\n#         del v\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-08-01T22:37:33.428163Z","iopub.execute_input":"2022-08-01T22:37:33.428885Z","iopub.status.idle":"2022-08-01T22:37:34.637370Z","shell.execute_reply.started":"2022-08-01T22:37:33.428841Z","shell.execute_reply":"2022-08-01T22:37:34.636347Z"},"trusted":true},"execution_count":40,"outputs":[{"execution_count":40,"output_type":"execute_result","data":{"text/plain":"89"},"metadata":{}}]},{"cell_type":"code","source":"df_dir=\"/kaggle/input/summarizer-data\"\n\ndata_dir= \"/kaggle/working/Data/DataFrames\"\nif not os.path.exists(data_dir):\n    os.makedirs(data_dir)\n\n# load saved dataframes and create labels\n# train_data_list, dev_data, test_data = create_labels(data_dir)\n\n# load dataframes with labels\ndf_add_tid(data_dir, df_dir)\n\nFileLinks(\".\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-output":false,"execution":{"iopub.status.busy":"2022-08-01T21:18:08.129524Z","iopub.execute_input":"2022-08-01T21:18:08.129967Z","iopub.status.idle":"2022-08-01T21:19:09.366782Z","shell.execute_reply.started":"2022-08-01T21:18:08.129905Z","shell.execute_reply":"2022-08-01T21:19:09.365628Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Index(['sentence', 'text_id', 'chosen', 'rougeL'], dtype='object') (1048562, 4)\nIndex(['sentence', 'text_id', 'chosen', 'rougeL'], dtype='object') (1048562, 4)\n","output_type":"stream"},{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"./\n  __notebook_source__.ipynb\n./Data/DataFrames/\n  dev_data_rougeL_tid.csv\n  test_data_rougeL_tid.csv","text/html":"./<br>\n&nbsp;&nbsp;<a href='./__notebook_source__.ipynb' target='_blank'>__notebook_source__.ipynb</a><br>\n./Data/DataFrames/<br>\n&nbsp;&nbsp;<a href='./Data/DataFrames/dev_data_rougeL_tid.csv' target='_blank'>dev_data_rougeL_tid.csv</a><br>\n&nbsp;&nbsp;<a href='./Data/DataFrames/test_data_rougeL_tid.csv' target='_blank'>test_data_rougeL_tid.csv</a><br>"},"metadata":{}}]},{"cell_type":"code","source":"# load training dataset\nsc_type=\"rougeL\"\ndata_type=\"train1\"\ntrain_data=pd.read_csv(os.path.join(df_dir,f\"{data_type}_data_{sc_type}_tid.csv\"))\ndata_stats(train_data)\nlabels=train_data[\"rougeL\"].values\ndel train_data[\"rougeL\"]\n\n\nsc_type=\"rougeL\"\ndata_type=\"dev\"\ndev_data=pd.read_csv(os.path.join(df_dir,f\"{data_type}_data_{sc_type}_tid.csv\"))\ndata_stats(dev_data)\nlabels_dev=dev_data[\"rougeL\"].values\ndel dev_data[\"rougeL\"]\n\n\nsc_type=\"rougeL\"\ndata_type=\"test\"\ntest_data=pd.read_csv(os.path.join(df_dir,f\"{data_type}_data_{sc_type}_tid.csv\"))\ndata_stats(test_data)\nlabels_test=test_data[\"rougeL\"].values\ndel test_data[\"rougeL\"]","metadata":{"execution":{"iopub.status.busy":"2022-08-01T21:29:46.502519Z","iopub.execute_input":"2022-08-01T21:29:46.503966Z","iopub.status.idle":"2022-08-01T21:30:01.363604Z","shell.execute_reply.started":"2022-08-01T21:29:46.503905Z","shell.execute_reply":"2022-08-01T21:30:01.362372Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"            count      mean       std  min       25%       50%       75%  max\nchosen                                                                       \n0       2310412.0  0.105404  0.120294  0.0  0.050000  0.083333  0.122449  1.0\n1        126615.0  0.349349  0.234293  0.0  0.177295  0.306569  0.461538  1.0\n            count      mean       std  min       25%       50%       75%  max\nchosen                                                                       \n0       1019122.0  0.109952  0.127384  0.0  0.051948  0.088889  0.128205  1.0\n1         29440.0  0.457427  0.297148  0.0  0.216498  0.392638  0.666667  1.0\n","output_type":"stream"}]},{"cell_type":"code","source":"# train_data_feats=pd.read_csv(\"/kaggle/input/summarizer-data/train1_set_feats.csv\")\ndata_type=\"train1\"\n# train_data=feature_df(train_data, data_dir, data_type)\ntrain_data=pd.read_csv(\"/kaggle/input/summarizer-data/train1_set_feats.csv\") \n\ntrain_data_sent=pd.read_csv(os.path.join(df_dir,f\"{data_type}_data_{sc_type}_tid.csv\"))\n\ntrain_data_feats=train_data\ntrain_data[\"sentence\"]=train_data_sent[\"sentence\"]\n\ntrain_data_feats=train_data_feats[[\"len\", \"s_position\", \"thematic_ratio\", \"s_pos_par\", \"num_ratio\"]]\n# lens= StandardScaler().fit_transform(np.array(train_data_feats[\"len\"]).reshape(-1,1) )\n# train_data_feats[\"len\"]=lens\n# train_data_feats\n\n\ndata_type=\"dev\"\ndev_data=feature_df(dev_data, data_dir, data_type)\n# dev_data=pd.read_csv(\"/kaggle/input/summarizer-data/train1_set_feats.csv\") \n\ndev_data_sent=pd.read_csv(os.path.join(df_dir,f\"{data_type}_data_{sc_type}_tid.csv\"))\ndev_data_feats=dev_data\ndev_data[\"sentence\"]=dev_data_sent[\"sentence\"]\ndev_data_feats=dev_data_feats[[\"len\", \"s_position\", \"thematic_ratio\", \"s_pos_par\", \"num_ratio\"]]\n\n\ndata_type=\"test\"\ntest_data=feature_df(test_data, data_dir, data_type)\n# dev_data=pd.read_csv(\"/kaggle/input/summarizer-data/train1_set_feats.csv\") \n\ntest_data_sent=pd.read_csv(os.path.join(df_dir,f\"{data_type}_data_{sc_type}_tid.csv\"))\ntest_data_feats=test_data\ntest_data[\"sentence\"]=test_data_feats[\"sentence\"]\ntest_data_feats=test_data_feats[[\"len\", \"s_position\", \"thematic_ratio\", \"s_pos_par\", \"num_ratio\"]]","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-08-01T22:57:14.356473Z","iopub.execute_input":"2022-08-01T22:57:14.357448Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"\n 43%|████▎     | 1048562/2437027 [18:47<24:53, 929.93it/s] \n\n  0%|          | 92/2437027 [00:00<44:11, 918.94it/s]\u001b[A\n  0%|          | 208/2437027 [00:00<38:18, 1059.96it/s]\u001b[A\n  0%|          | 323/2437027 [00:00<36:55, 1099.96it/s]\u001b[A\n  0%|          | 440/2437027 [00:00<36:02, 1126.55it/s]\u001b[A\n  0%|          | 559/2437027 [00:00<35:22, 1147.95it/s]\u001b[A\n  0%|          | 674/2437027 [00:00<36:10, 1122.38it/s]\u001b[A\n  0%|          | 795/2437027 [00:00<35:20, 1148.99it/s]\u001b[A\n  0%|          | 911/2437027 [00:00<35:46, 1134.91it/s]\u001b[A\n  0%|          | 1025/2437027 [00:00<35:44, 1135.70it/s]\u001b[A\n  0%|          | 1139/2437027 [00:01<36:10, 1122.43it/s]\u001b[A\n  0%|          | 1257/2437027 [00:01<35:40, 1138.07it/s]\u001b[A\n  0%|          | 1375/2437027 [00:01<35:22, 1147.75it/s]\u001b[A\n  0%|          | 1490/2437027 [00:01<36:07, 1123.49it/s]\u001b[A\n  0%|          | 1603/2437027 [00:01<36:22, 1116.04it/s]\u001b[A\n  0%|          | 1721/2437027 [00:01<35:48, 1133.57it/s]\u001b[A\n  0%|          | 1835/2437027 [00:01<35:45, 1134.83it/s]\u001b[A\n  0%|          | 1966/2437027 [00:01<34:13, 1185.89it/s]\u001b[A\n  0%|          | 2093/2437027 [00:01<33:32, 1210.01it/s]\u001b[A\n  0%|          | 2215/2437027 [00:01<33:44, 1202.49it/s]\u001b[A\n  0%|          | 2336/2437027 [00:02<34:18, 1183.01it/s]\u001b[A\n  0%|          | 2455/2437027 [00:02<34:17, 1183.07it/s]\u001b[A\n  0%|          | 2575/2437027 [00:02<34:10, 1187.47it/s]\u001b[A\n  0%|          | 2694/2437027 [00:02<35:22, 1146.68it/s]\u001b[A\n  0%|          | 2809/2437027 [00:02<35:40, 1137.40it/s]\u001b[A\n  0%|          | 2928/2437027 [00:02<35:15, 1150.50it/s]\u001b[A\n  0%|          | 3044/2437027 [00:02<35:31, 1141.76it/s]\u001b[A\n  0%|          | 3159/2437027 [00:02<37:36, 1078.40it/s]\u001b[A\n  0%|          | 3274/2437027 [00:02<36:55, 1098.45it/s]\u001b[A\n  0%|          | 3393/2437027 [00:02<36:07, 1122.57it/s]\u001b[A\n  0%|          | 3509/2437027 [00:03<35:49, 1132.23it/s]\u001b[A\n  0%|          | 3634/2437027 [00:03<34:46, 1166.18it/s]\u001b[A\n  0%|          | 3774/2437027 [00:03<32:52, 1233.46it/s]\u001b[A\n  0%|          | 3902/2437027 [00:03<32:32, 1246.27it/s]\u001b[A\n  0%|          | 4027/2437027 [00:03<33:57, 1194.29it/s]\u001b[A\n  0%|          | 4151/2437027 [00:03<33:39, 1204.54it/s]\u001b[A\n  0%|          | 4273/2437027 [00:03<33:34, 1207.40it/s]\u001b[A\n  0%|          | 4395/2437027 [00:03<34:57, 1159.73it/s]\u001b[A\n  0%|          | 4516/2437027 [00:03<34:35, 1172.05it/s]\u001b[A\n  0%|          | 4634/2437027 [00:04<35:29, 1142.49it/s]\u001b[A\n  0%|          | 4757/2437027 [00:04<34:43, 1167.57it/s]\u001b[A\n  0%|          | 4885/2437027 [00:04<33:47, 1199.80it/s]\u001b[A\n  0%|          | 5012/2437027 [00:04<33:13, 1219.83it/s]\u001b[A\n  0%|          | 5135/2437027 [00:04<34:01, 1191.31it/s]\u001b[A\n  0%|          | 5255/2437027 [00:04<34:01, 1191.37it/s]\u001b[A\n  0%|          | 5375/2437027 [00:04<34:48, 1164.40it/s]\u001b[A\n  0%|          | 5492/2437027 [00:04<35:24, 1144.66it/s]\u001b[A\n  0%|          | 5611/2437027 [00:04<35:00, 1157.37it/s]\u001b[A\n  0%|          | 5727/2437027 [00:04<39:16, 1031.74it/s]\u001b[A\n  0%|          | 5833/2437027 [00:05<40:25, 1002.22it/s]\u001b[A\n  0%|          | 5939/2437027 [00:05<39:50, 1017.04it/s]\u001b[A\n  0%|          | 6049/2437027 [00:05<38:58, 1039.57it/s]\u001b[A\n  0%|          | 6157/2437027 [00:05<38:38, 1048.62it/s]\u001b[A\n  0%|          | 6274/2437027 [00:05<37:24, 1082.84it/s]\u001b[A\n  0%|          | 6383/2437027 [00:05<37:32, 1079.27it/s]\u001b[A\n  0%|          | 6493/2437027 [00:05<37:20, 1084.82it/s]\u001b[A\n  0%|          | 6613/2437027 [00:05<36:12, 1118.55it/s]\u001b[A\n  0%|          | 6726/2437027 [00:05<36:52, 1098.33it/s]\u001b[A\n  0%|          | 6841/2437027 [00:06<36:28, 1110.35it/s]\u001b[A\n  0%|          | 6953/2437027 [00:06<36:33, 1107.68it/s]\u001b[A\n  0%|          | 7065/2437027 [00:06<36:33, 1107.99it/s]\u001b[A\n  0%|          | 7179/2437027 [00:06<36:17, 1116.02it/s]\u001b[A\n  0%|          | 7291/2437027 [00:06<36:34, 1107.15it/s]\u001b[A\n  0%|          | 7418/2437027 [00:06<35:06, 1153.17it/s]\u001b[A\n  0%|          | 7534/2437027 [00:06<35:06, 1153.53it/s]\u001b[A\n  0%|          | 7650/2437027 [00:06<35:53, 1127.98it/s]\u001b[A\n  0%|          | 7763/2437027 [00:06<36:10, 1119.15it/s]\u001b[A\n  0%|          | 7884/2437027 [00:06<35:22, 1144.50it/s]\u001b[A\n  0%|          | 7999/2437027 [00:07<36:27, 1110.32it/s]\u001b[A\n  0%|          | 8111/2437027 [00:07<37:23, 1082.88it/s]\u001b[A\n  0%|          | 8220/2437027 [00:07<38:06, 1062.36it/s]\u001b[A\n  0%|          | 8332/2437027 [00:07<37:31, 1078.87it/s]\u001b[A\n  0%|          | 8445/2437027 [00:07<37:02, 1092.81it/s]\u001b[A\n  0%|          | 8555/2437027 [00:07<37:26, 1081.09it/s]\u001b[A\n  0%|          | 8670/2437027 [00:07<36:48, 1099.66it/s]\u001b[A\n  0%|          | 8786/2437027 [00:07<36:14, 1116.62it/s]\u001b[A\n  0%|          | 8905/2437027 [00:07<35:34, 1137.61it/s]\u001b[A\n  0%|          | 9019/2437027 [00:07<35:55, 1126.57it/s]\u001b[A\n  0%|          | 9147/2437027 [00:08<34:35, 1169.84it/s]\u001b[A\n  0%|          | 9265/2437027 [00:08<34:58, 1157.04it/s]\u001b[A\n  0%|          | 9381/2437027 [00:08<34:58, 1156.85it/s]\u001b[A\n  0%|          | 9507/2437027 [00:08<34:08, 1185.06it/s]\u001b[A\n  0%|          | 9626/2437027 [00:08<35:46, 1131.07it/s]\u001b[A\n  0%|          | 9764/2437027 [00:08<33:38, 1202.63it/s]\u001b[A\n  0%|          | 9885/2437027 [00:08<35:29, 1139.74it/s]\u001b[A\n  0%|          | 10001/2437027 [00:08<38:15, 1057.16it/s]\u001b[A\n  0%|          | 10109/2437027 [00:08<38:20, 1055.02it/s]\u001b[A\n  0%|          | 10216/2437027 [00:09<38:25, 1052.56it/s]\u001b[A\n  0%|          | 10323/2437027 [00:09<38:42, 1044.99it/s]\u001b[A\n  0%|          | 10437/2437027 [00:09<37:46, 1070.69it/s]\u001b[A\n  0%|          | 10545/2437027 [00:09<37:59, 1064.25it/s]\u001b[A\n  0%|          | 10652/2437027 [00:09<40:58, 986.96it/s] \u001b[A\n  0%|          | 10755/2437027 [00:09<40:32, 997.30it/s]\u001b[A\n  0%|          | 10865/2437027 [00:09<39:29, 1024.11it/s]\u001b[A\n  0%|          | 10974/2437027 [00:09<38:53, 1039.87it/s]\u001b[A\n  0%|          | 11092/2437027 [00:09<37:29, 1078.57it/s]\u001b[A\n  0%|          | 11222/2437027 [00:09<35:24, 1142.08it/s]\u001b[A\n  0%|          | 11337/2437027 [00:10<35:42, 1132.11it/s]\u001b[A\n  0%|          | 11459/2437027 [00:10<34:58, 1156.12it/s]\u001b[A\n  0%|          | 11575/2437027 [00:10<36:51, 1096.61it/s]\u001b[A\n  0%|          | 11686/2437027 [00:10<37:03, 1090.72it/s]\u001b[A\n  0%|          | 11796/2437027 [00:10<37:56, 1065.47it/s]\u001b[A\n  0%|          | 11904/2437027 [00:10<37:48, 1069.20it/s]\u001b[A\n  0%|          | 12016/2437027 [00:10<37:21, 1081.77it/s]\u001b[A\n  0%|          | 12130/2437027 [00:10<36:48, 1098.05it/s]\u001b[A\n  1%|          | 12250/2437027 [00:10<35:51, 1126.95it/s]\u001b[A\n  1%|          | 12368/2437027 [00:11<35:26, 1140.36it/s]\u001b[A\n  1%|          | 12485/2437027 [00:11<35:10, 1148.61it/s]\u001b[A\n  1%|          | 12600/2437027 [00:11<36:34, 1104.81it/s]\u001b[A\n  1%|          | 12719/2437027 [00:11<35:50, 1127.09it/s]\u001b[A\n  1%|          | 12833/2437027 [00:11<36:50, 1096.75it/s]\u001b[A\n  1%|          | 12944/2437027 [00:11<36:44, 1099.41it/s]\u001b[A\n  1%|          | 13062/2437027 [00:11<36:03, 1120.23it/s]\u001b[A\n  1%|          | 13177/2437027 [00:11<35:48, 1128.29it/s]\u001b[A\n  1%|          | 13291/2437027 [00:11<36:19, 1111.96it/s]\u001b[A\n  1%|          | 13407/2437027 [00:11<35:55, 1124.46it/s]\u001b[A\n  1%|          | 13520/2437027 [00:12<36:18, 1112.60it/s]\u001b[A\n  1%|          | 13642/2437027 [00:12<35:22, 1141.82it/s]\u001b[A\n  1%|          | 13757/2437027 [00:12<35:23, 1141.41it/s]\u001b[A\n  1%|          | 13877/2437027 [00:12<34:52, 1158.17it/s]\u001b[A\n  1%|          | 13999/2437027 [00:12<34:23, 1174.34it/s]\u001b[A\n  1%|          | 14117/2437027 [00:12<34:51, 1158.50it/s]\u001b[A\n  1%|          | 14251/2437027 [00:12<33:20, 1210.80it/s]\u001b[A\n  1%|          | 14373/2437027 [00:12<34:16, 1177.84it/s]\u001b[A\n  1%|          | 14492/2437027 [00:12<35:05, 1150.33it/s]\u001b[A\n  1%|          | 14608/2437027 [00:12<35:43, 1129.97it/s]\u001b[A\n  1%|          | 14726/2437027 [00:13<35:17, 1143.71it/s]\u001b[A\n  1%|          | 14845/2437027 [00:13<34:53, 1156.91it/s]\u001b[A\n  1%|          | 14961/2437027 [00:13<35:21, 1141.69it/s]\u001b[A\n  1%|          | 15079/2437027 [00:13<35:06, 1149.89it/s]\u001b[A\n  1%|          | 15195/2437027 [00:13<35:24, 1139.81it/s]\u001b[A\n  1%|          | 15320/2437027 [00:13<34:27, 1171.44it/s]\u001b[A\n  1%|          | 15438/2437027 [00:13<34:34, 1167.51it/s]\u001b[A\n  1%|          | 15559/2437027 [00:13<34:14, 1178.38it/s]\u001b[A\n  1%|          | 15680/2437027 [00:13<33:59, 1187.39it/s]\u001b[A\n  1%|          | 15803/2437027 [00:14<33:42, 1197.30it/s]\u001b[A\n  1%|          | 15923/2437027 [00:14<34:06, 1183.24it/s]\u001b[A\n  1%|          | 16045/2437027 [00:14<33:47, 1193.96it/s]\u001b[A\n  1%|          | 16170/2437027 [00:14<33:19, 1210.58it/s]\u001b[A\n  1%|          | 16293/2437027 [00:14<33:11, 1215.62it/s]\u001b[A\n  1%|          | 16415/2437027 [00:14<34:01, 1185.62it/s]\u001b[A\n  1%|          | 16543/2437027 [00:14<33:17, 1212.05it/s]\u001b[A\n  1%|          | 16665/2437027 [00:14<33:24, 1207.32it/s]\u001b[A\n  1%|          | 16786/2437027 [00:14<33:49, 1192.69it/s]\u001b[A\n  1%|          | 16910/2437027 [00:14<33:27, 1205.59it/s]\u001b[A\n  1%|          | 17034/2437027 [00:15<33:12, 1214.66it/s]\u001b[A\n  1%|          | 17156/2437027 [00:15<33:13, 1213.96it/s]\u001b[A\n  1%|          | 17280/2437027 [00:15<33:07, 1217.45it/s]\u001b[A\n  1%|          | 17402/2437027 [00:15<34:34, 1166.60it/s]\u001b[A\n  1%|          | 17520/2437027 [00:15<34:30, 1168.79it/s]\u001b[A\n  1%|          | 17642/2437027 [00:15<34:06, 1182.23it/s]\u001b[A\n  1%|          | 17761/2437027 [00:15<34:40, 1163.02it/s]\u001b[A\n  1%|          | 17879/2437027 [00:15<34:35, 1165.74it/s]\u001b[A\n  1%|          | 17996/2437027 [00:15<35:32, 1134.34it/s]\u001b[A\n  1%|          | 18110/2437027 [00:15<35:52, 1123.80it/s]\u001b[A\n  1%|          | 18225/2437027 [00:16<35:41, 1129.59it/s]\u001b[A\n  1%|          | 18381/2437027 [00:16<32:08, 1254.39it/s]\u001b[A\n  1%|          | 18507/2437027 [00:16<32:32, 1238.81it/s]\u001b[A\n  1%|          | 18632/2437027 [00:16<32:29, 1240.71it/s]\u001b[A\n  1%|          | 18757/2437027 [00:16<34:21, 1172.80it/s]\u001b[A\n  1%|          | 18876/2437027 [00:16<34:17, 1175.06it/s]\u001b[A\n  1%|          | 18995/2437027 [00:16<34:51, 1156.32it/s]\u001b[A\n  1%|          | 19112/2437027 [00:16<35:07, 1147.45it/s]\u001b[A\n  1%|          | 19228/2437027 [00:16<35:02, 1149.70it/s]\u001b[A\n  1%|          | 19344/2437027 [00:17<34:58, 1152.06it/s]\u001b[A\n  1%|          | 19460/2437027 [00:17<36:22, 1107.94it/s]\u001b[A\n  1%|          | 19572/2437027 [00:17<36:17, 1110.30it/s]\u001b[A\n  1%|          | 19699/2437027 [00:17<34:52, 1155.24it/s]\u001b[A\n  1%|          | 19815/2437027 [00:17<35:40, 1129.12it/s]\u001b[A\n  1%|          | 19938/2437027 [00:17<34:50, 1156.14it/s]\u001b[A\n  1%|          | 20054/2437027 [00:17<35:01, 1149.90it/s]\u001b[A\n  1%|          | 20177/2437027 [00:17<34:21, 1172.34it/s]\u001b[A\n  1%|          | 20302/2437027 [00:17<33:46, 1192.78it/s]\u001b[A\n  1%|          | 20422/2437027 [00:17<33:52, 1188.88it/s]\u001b[A\n  1%|          | 20541/2437027 [00:18<34:10, 1178.70it/s]\u001b[A\n  1%|          | 20659/2437027 [00:18<34:42, 1160.24it/s]\u001b[A\n  1%|          | 20795/2437027 [00:18<33:06, 1216.43it/s]\u001b[A\n  1%|          | 20917/2437027 [00:18<33:40, 1195.87it/s]\u001b[A\n  1%|          | 21037/2437027 [00:18<33:45, 1192.55it/s]\u001b[A\n  1%|          | 21157/2437027 [00:18<35:55, 1121.04it/s]\u001b[A\n  1%|          | 21270/2437027 [00:18<36:10, 1112.85it/s]\u001b[A\n  1%|          | 21408/2437027 [00:18<33:51, 1188.97it/s]\u001b[A\n  1%|          | 21529/2437027 [00:18<33:43, 1193.45it/s]\u001b[A\n  1%|          | 21649/2437027 [00:18<34:08, 1178.91it/s]\u001b[A\n  1%|          | 21768/2437027 [00:19<34:03, 1181.73it/s]\u001b[A\n  1%|          | 21887/2437027 [00:19<35:00, 1149.57it/s]\u001b[A\n  1%|          | 22003/2437027 [00:19<35:48, 1124.04it/s]\u001b[A\n  1%|          | 22120/2437027 [00:19<35:29, 1133.98it/s]\u001b[A\n  1%|          | 22237/2437027 [00:19<35:12, 1143.00it/s]\u001b[A\n  1%|          | 22362/2437027 [00:19<34:19, 1172.66it/s]\u001b[A\n  1%|          | 22480/2437027 [00:19<36:16, 1109.53it/s]\u001b[A\n  1%|          | 22592/2437027 [00:19<37:02, 1086.37it/s]\u001b[A\n  1%|          | 22702/2437027 [00:19<37:30, 1072.78it/s]\u001b[A\n  1%|          | 22810/2437027 [00:20<37:53, 1062.05it/s]\u001b[A\n  1%|          | 22925/2437027 [00:20<37:01, 1086.75it/s]\u001b[A\n  1%|          | 23054/2437027 [00:20<35:08, 1144.92it/s]\u001b[A\n  1%|          | 23175/2437027 [00:20<34:35, 1162.91it/s]\u001b[A\n  1%|          | 23292/2437027 [00:20<34:44, 1158.16it/s]\u001b[A\n  1%|          | 23413/2437027 [00:20<34:21, 1170.80it/s]\u001b[A\n  1%|          | 23554/2437027 [00:20<32:24, 1241.04it/s]\u001b[A\n  1%|          | 23681/2437027 [00:20<32:13, 1248.34it/s]\u001b[A\n  1%|          | 23806/2437027 [00:20<32:35, 1234.33it/s]\u001b[A\n  1%|          | 23930/2437027 [00:20<33:07, 1214.30it/s]\u001b[A\n  1%|          | 24052/2437027 [00:21<33:32, 1198.87it/s]\u001b[A\n  1%|          | 24173/2437027 [00:21<34:29, 1166.10it/s]\u001b[A\n  1%|          | 24290/2437027 [00:21<35:16, 1139.77it/s]\u001b[A\n  1%|          | 24407/2437027 [00:21<35:05, 1145.81it/s]\u001b[A\n  1%|          | 24528/2437027 [00:21<34:34, 1162.71it/s]\u001b[A\n  1%|          | 24645/2437027 [00:21<34:54, 1151.72it/s]\u001b[A\n  1%|          | 24761/2437027 [00:21<36:23, 1104.74it/s]\u001b[A\n  1%|          | 24872/2437027 [00:21<37:03, 1084.83it/s]\u001b[A\n  1%|          | 24991/2437027 [00:21<36:06, 1113.14it/s]\u001b[A\n  1%|          | 25115/2437027 [00:22<35:00, 1148.24it/s]\u001b[A\n  1%|          | 25260/2437027 [00:22<32:34, 1234.26it/s]\u001b[A\n  1%|          | 25384/2437027 [00:22<32:58, 1218.84it/s]\u001b[A\n  1%|          | 25507/2437027 [00:22<34:11, 1175.29it/s]\u001b[A\n  1%|          | 25626/2437027 [00:22<34:28, 1165.98it/s]\u001b[A\n  1%|          | 25761/2437027 [00:22<32:58, 1218.81it/s]\u001b[A\n  1%|          | 25884/2437027 [00:22<34:00, 1181.61it/s]\u001b[A\n  1%|          | 26003/2437027 [00:22<35:49, 1121.68it/s]\u001b[A\n  1%|          | 26116/2437027 [00:22<37:22, 1075.27it/s]\u001b[A\n  1%|          | 26247/2437027 [00:22<35:18, 1137.84it/s]\u001b[A\n  1%|          | 26369/2437027 [00:23<34:42, 1157.62it/s]\u001b[A\n  1%|          | 26486/2437027 [00:23<34:53, 1151.24it/s]\u001b[A\n  1%|          | 26602/2437027 [00:23<34:55, 1150.51it/s]\u001b[A\n  1%|          | 26718/2437027 [00:23<36:11, 1110.13it/s]\u001b[A\n  1%|          | 26834/2437027 [00:23<35:46, 1122.75it/s]\u001b[A\n  1%|          | 26965/2437027 [00:23<34:11, 1174.97it/s]\u001b[A\n  1%|          | 27101/2437027 [00:23<32:43, 1227.35it/s]\u001b[A\n  1%|          | 27225/2437027 [00:23<32:54, 1220.67it/s]\u001b[A\n  1%|          | 27348/2437027 [00:23<33:38, 1193.53it/s]\u001b[A\n  1%|          | 27479/2437027 [00:24<32:43, 1227.04it/s]\u001b[A\n  1%|          | 27608/2437027 [00:24<32:16, 1244.11it/s]\u001b[A\n  1%|          | 27733/2437027 [00:24<32:48, 1223.90it/s]\u001b[A\n  1%|          | 27859/2437027 [00:24<32:33, 1232.95it/s]\u001b[A\n  1%|          | 27983/2437027 [00:24<32:56, 1219.01it/s]\u001b[A\n  1%|          | 28106/2437027 [00:24<33:45, 1189.33it/s]\u001b[A\n  1%|          | 28226/2437027 [00:24<34:55, 1149.73it/s]\u001b[A\n  1%|          | 28342/2437027 [00:24<35:32, 1129.63it/s]\u001b[A\n  1%|          | 28456/2437027 [00:24<36:34, 1097.52it/s]\u001b[A\n  1%|          | 28566/2437027 [00:24<37:51, 1060.45it/s]\u001b[A","output_type":"stream"}]},{"cell_type":"code","source":"FileLinks(\".\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# tokenize\nglobal pbar_cleaning\npbar_cleaning=tqdm(total=train_data.shape[0], leave=True)\ntrain_data[\"tokens\"] = train_data[\"sentence\"].apply(lambda x: sentence_cleaning(x)[1]) \n\ntrain_data_feats=pd.DataFrame()\n# create sentence features\n# 1. thematic words\ncol = train_data.groupby(\"text_id\").tokens.apply(sum)\nthematic_cols= pd.DataFrame({\"text_id\": col.index, \"thematic\": [ [ t[0] for t in Counter(x).most_common(10) ] for x in col ]})  \ntrain_data=train_data.join(thematic_cols[\"thematic\"], on='text_id' )\ntrain_data_feats[\"thematic_ratio\"] = train_data.apply(lambda row: thematic_ratio(row.thematic, row.tokens) if len(row.tokens)>0 else 0.0, axis=1)\n\n# 2. sentence position in the text\ntrain_data[\"text_position\"] = train_data.groupby(\"text_id\").cumcount().add(1)\ntrain_data[\"tot_sent\"] = train_data.groupby(\"text_id\")[\"sentence\"].transform(len)\ntrain_data_feats[\"s_position\"] = train_data.apply(lambda row: s_position(row.text_position,row.tot_sent), axis=1)\n\n# 3. sentence length - threshold=3\nthreshold=3\ntrain_data_feats[\"len\"]= train_data[\"tokens\"].apply(lambda x: 0 if len(x)<threshold else len(x))\n\n# 4. sentence position - paragraph relative\ntrain_data_feats['s_pos_par'] = train_data_feats[\"s_position\"].values\ntrain_data_feats.loc[train_data_feats.s_pos_par!=1.0, 's_pos_par']=0.0\n\n# 5. numerals\ntrain_data_feats[\"num_ratio\"]=train_data_feats[\"tokens\"].apply(lambda x: sum( [ 1 for t in eval(x) if t.isnumeric() ] )/len(x) )\n\n\n# ?. Term Frequency-Inverse Sentence Frequency\n\n\n# ?. proper nouns - not so useful\n# train_data_feats[\"NNPs\"]=train_data_feats[\"tokens\"].apply(lambda x: prop_nouns(x) )\n\n\n\n\n# train_data_feats=train_data[[\"len\",\"text_position\"]]\ntrain_data_feats.to_csv(os.path.join(data_dir,f\"{data_type}_set_feats.csv\"), header=True, index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train_data_feats=train_data[[\"len\",\"text_position\",\"thematic_words\", ]]\ndata_type=\"train1\"\ntrain_data.to_csv(os.path.join(data_dir,f\"{data_type}_set_feats.csv\"), header=True, index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"FileLinks(\".\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sc_type=\"rougeL\"\ndata_type=\"train1\"\ntrain_data=pd.read_csv(os.path.join(df_dir,f\"{data_type}_data_{sc_type}_tid.csv\"))\ndata_stats(train_data)\nlabels=train_data[\"rougeL\"].values","metadata":{"execution":{"iopub.status.busy":"2022-08-01T17:27:05.244281Z","iopub.execute_input":"2022-08-01T17:27:05.244839Z","iopub.status.idle":"2022-08-01T17:27:14.956076Z","shell.execute_reply.started":"2022-08-01T17:27:05.244795Z","shell.execute_reply":"2022-08-01T17:27:14.954939Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"            count      mean       std  min       25%       50%       75%  max\nchosen                                                                       \n0       2310412.0  0.105404  0.120294  0.0  0.050000  0.083333  0.122449  1.0\n1        126615.0  0.349349  0.234293  0.0  0.177295  0.306569  0.461538  1.0\n","output_type":"stream"}]},{"cell_type":"code","source":"test_num=1000\n\nmodel = make_pipeline(MinMaxScaler(), ElasticNet(alpha=5e-3, warm_start=True, random_state=99, fit_intercept=True))\nmodel.fit(train_data_feats, labels)\n\ncv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n# evaluate model\nscores_mae = cross_val_score(model, train_data_feats.iloc[:test_num,:] , labels[:test_num], scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1)\nscores_mse = cross_val_score(model, train_data_feats.iloc[:test_num,:] , labels[:test_num], scoring='neg_mean_squared_error', cv=cv, n_jobs=-1)\n# scores_r2 = cross_val_score(model, train_data_feats.iloc[:test_num,:] , labels[:test_num], scoring='r2', cv=cv, n_jobs=-1)\n\n# force scores to be positive\nscores_mae = np.absolute(scores_mae)\nscores_mse = np.absolute(scores_mse)\n# scores_r2 = np.absolute(scores_r2)\nprint('Mean MAE: %.3f (%.3f)' % (np.mean(scores_mae), np.std(scores_mae)))\nprint('Mean MSE: %.3f (%.3f)' % (np.mean(scores_mse), np.std(scores_mse)))\n# print('Mean R2: %.3f (%.3f)' % (np.mean(scores_r2), np.std(scores_r2)))\n\nprint(labels)\nprint(model.predict(train_data_feats))\n# svr_results(labels[:test_num],  train_data_feats.iloc[:test_num,:], model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_num=10000\n\n# model = make_pipeline(MinMaxScaler(), ElasticNetCV(random_state=99))\nmodel =   make_pipeline(StandardScaler(),ElasticNet(alpha=5e-6, random_state=99, fit_intercept=True))\nmodel.fit(train_data_feats, labels)\n\ncv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n# evaluate model\nscores_mae = cross_val_score(model, train_data_feats.iloc[:test_num,:] , labels[:test_num], scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1)\nscores_mse = cross_val_score(model, train_data_feats.iloc[:test_num,:] , labels[:test_num], scoring='neg_mean_squared_error', cv=cv, n_jobs=-1)\n# scores_r2 = cross_val_score(model, train_data_feats.iloc[:test_num,:] , labels[:test_num], scoring='r2', cv=cv, n_jobs=-1)\n\n# force scores to be positive\nscores_mae = np.absolute(scores_mae)\nscores_mse = np.absolute(scores_mse)\n# scores_r2 = np.absolute(scores_r2)\nprint('Mean MAE: %.3f (%.3f)' % (np.mean(scores_mae), np.std(scores_mae)))\nprint('Mean MSE: %.3f (%.3f)' % (np.mean(scores_mse), np.std(scores_mse)))\n# print('Mean R2: %.3f (%.3f)' % (np.mean(scores_r2), np.std(scores_r2)))\n\nprint(labels)\nprint(model.predict(train_data_feats))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_num=1000\n\n# model = make_pipeline(MinMaxScaler(), ElasticNetCV(random_state=99))\neps=0\nmodel = make_pipeline(StandardScaler(), LinearSVR(epsilon=eps, C=5e-1, fit_intercept=True, intercept_scaling=1.0, loss=\"squared_epsilon_insensitive\"))\n# model = LinearSVR(epsilon=eps, C=5e-4, fit_intercept=True)\nmodel.fit(train_data_feats, labels)\n\ncv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n# evaluate model\nscores_mae = cross_val_score(model, train_data_feats.iloc[:test_num,:] , labels[:test_num], scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1)\nscores_mse = cross_val_score(model, train_data_feats.iloc[:test_num,:] , labels[:test_num], scoring='neg_mean_squared_error', cv=cv, n_jobs=-1)\n# scores_r2 = cross_val_score(model, train_data_feats.iloc[:test_num,:] , labels[:test_num], scoring='r2', cv=cv, n_jobs=-1)\n\n# force scores to be positive\nscores_mae = np.absolute(scores_mae)\nscores_mse = np.absolute(scores_mse)\n# scores_r2 = np.absolute(scores_r2)\nprint('Mean MAE: %.3f (%.3f)' % (np.mean(scores_mae), np.std(scores_mae)))\nprint('Mean MSE: %.3f (%.3f)' % (np.mean(scores_mse), np.std(scores_mse)))\n# print('Mean R2: %.3f (%.3f)' % (np.mean(scores_r2), np.std(scores_r2)))\n\nprint(labels)\nprint(model.predict(train_data_feats))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Best sofar","metadata":{}},{"cell_type":"code","source":"test_num=1000\n\n# model = make_pipeline(MinMaxScaler(), ElasticNetCV(random_state=99))\neps=0\nmodel = make_pipeline(StandardScaler(), SGDRegressor(max_iter=1000, tol=1e-3, alpha=5e-3, learning_rate=\"adaptive\"))\n# model = LinearSVR(epsilon=eps, C=5e-4, fit_intercept=True)\nmodel.fit(train_data_feats, labels)\n\ncv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n# evaluate model\nscores_mae = cross_val_score(model, train_data_feats.iloc[:test_num,:] , labels[:test_num], scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1)\nscores_mse = cross_val_score(model, train_data_feats.iloc[:test_num,:] , labels[:test_num], scoring='neg_mean_squared_error', cv=cv, n_jobs=-1)\n# scores_r2 = cross_val_score(model, train_data_feats.iloc[:test_num,:] , labels[:test_num], scoring='r2', cv=cv, n_jobs=-1)\n\n# force scores to be positive\nscores_mae = np.absolute(scores_mae)\nscores_mse = np.absolute(scores_mse)\n# scores_r2 = np.absolute(scores_r2)\nprint('Mean MAE: %.3f (%.3f)' % (np.mean(scores_mae), np.std(scores_mae)))\nprint('Mean MSE: %.3f (%.3f)' % (np.mean(scores_mse), np.std(scores_mse)))\n# print('Mean R2: %.3f (%.3f)' % (np.mean(scores_r2), np.std(scores_r2)))\n\nprint(labels)\nprint(model.predict(train_data_feats))","metadata":{"execution":{"iopub.status.busy":"2022-08-01T17:28:29.117092Z","iopub.execute_input":"2022-08-01T17:28:29.117563Z","iopub.status.idle":"2022-08-01T17:28:50.562322Z","shell.execute_reply.started":"2022-08-01T17:28:29.117523Z","shell.execute_reply":"2022-08-01T17:28:50.558212Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"Mean MAE: 0.051 (0.007)\nMean MSE: 0.009 (0.003)\n[0.33898305 0.2962963  0.1875     ... 0.12       0.13333333 0.06557377]\n[0.30463234 0.11435392 0.12255804 ... 0.11228816 0.10431626 0.31472633]\n","output_type":"stream"}]},{"cell_type":"code","source":"# grid search","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data","metadata":{"execution":{"iopub.status.busy":"2022-08-01T17:44:31.148388Z","iopub.execute_input":"2022-08-01T17:44:31.149449Z","iopub.status.idle":"2022-08-01T17:44:31.181332Z","shell.execute_reply.started":"2022-08-01T17:44:31.149376Z","shell.execute_reply":"2022-08-01T17:44:31.179914Z"},"trusted":true},"execution_count":64,"outputs":[{"execution_count":64,"output_type":"execute_result","data":{"text/plain":"         len  text_position  tot_sent  s_position  \\\n0         19              1        32    1.000000   \n1         16              2        32    0.462483   \n2         25              3        32   -0.879509   \n3         13              4        32    0.995195   \n4         10              5        32   -0.769904   \n...      ...            ...       ...         ...   \n2437022    7             42        46   -0.860405   \n2437023   18             43        46    0.928123   \n2437024   16             44        46   -0.269765   \n2437025   18             45        46   -0.599631   \n2437026   24             46        46    1.000000   \n\n                                                  thematic  \\\n0        ['pfizer', 'sharehold', 'board', 'vote', 'dire...   \n1        ['pfizer', 'sharehold', 'board', 'vote', 'dire...   \n2        ['pfizer', 'sharehold', 'board', 'vote', 'dire...   \n3        ['pfizer', 'sharehold', 'board', 'vote', 'dire...   \n4        ['pfizer', 'sharehold', 'board', 'vote', 'dire...   \n...                                                    ...   \n2437022  ['said', 'villag', '``', 'rape', 'girl', 'poli...   \n2437023  ['said', 'villag', '``', 'rape', 'girl', 'poli...   \n2437024  ['said', 'villag', '``', 'rape', 'girl', 'poli...   \n2437025  ['said', 'villag', '``', 'rape', 'girl', 'poli...   \n2437026  ['said', 'villag', '``', 'rape', 'girl', 'poli...   \n\n                                                    tokens  text_id  \\\n0        ['outsiz', 'execut', 'pay', 'inde', 'becom', '...        0   \n1        ['battl', 'line', 'drawn', 'pfizer', 'owner', ...        0   \n2        ['one', 'side', 'stand', 'hank', 'mckinnel', '...        0   \n3        ['pfizer', 'sharehold', 'angri', '46', 'percen...        0   \n4        ['sharehold', 'threaten', 'withhold', 'vote', ...        0   \n...                                                    ...      ...   \n2437022  ['shock', 'instanc', 'sexual', 'violenc', 'con...    82913   \n2437023  ['``', 'law', 'much', 'end', 'someth', 'endem'...    82913   \n2437024  ['countri', 'new', 'prime', 'minist', 'narendr...    82913   \n2437025  ['women', 'right', 'group', 'critic', 'say', '...    82913   \n2437026  ['read', 'grief', 'fear', 'blame', 'rape', 'in...    82913   \n\n         thematic_ratio  s_pos_par  num_ratio  \\\n0              0.210526        1.0   0.000000   \n1              0.187500        0.0   0.000000   \n2              0.080000        0.0   0.012987   \n3              0.153846        0.0   0.008403   \n4              0.400000        0.0   0.000000   \n...                 ...        ...        ...   \n2437022        0.000000        0.0   0.000000   \n2437023        0.222222        0.0   0.000000   \n2437024        0.187500        0.0   0.000000   \n2437025        0.055556        0.0   0.000000   \n2437026        0.166667        1.0   0.000000   \n\n                                                  sentence  \n0        IF outsized executive pay has indeed become a ...  \n1        The battle lines have been drawn between Pfize...  \n2        On one side stands Hank McKinnell, Pfizer's ch...  \n3        On the other are Pfizer shareholders, angry ov...  \n4        Some shareholders are threatening to withhold ...  \n...                                                    ...  \n2437022  But shocking instances of sexual violence cont...  \n2437023  \"Laws can only do so much when you have to end...  \n2437024  The country's new Prime Minister, Narendra Mod...  \n2437025  But women's rights groups have criticized what...  \n2437026  READ MORE: Grief, fear, blame over rapes in In...  \n\n[2437027 rows x 11 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>len</th>\n      <th>text_position</th>\n      <th>tot_sent</th>\n      <th>s_position</th>\n      <th>thematic</th>\n      <th>tokens</th>\n      <th>text_id</th>\n      <th>thematic_ratio</th>\n      <th>s_pos_par</th>\n      <th>num_ratio</th>\n      <th>sentence</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>19</td>\n      <td>1</td>\n      <td>32</td>\n      <td>1.000000</td>\n      <td>['pfizer', 'sharehold', 'board', 'vote', 'dire...</td>\n      <td>['outsiz', 'execut', 'pay', 'inde', 'becom', '...</td>\n      <td>0</td>\n      <td>0.210526</td>\n      <td>1.0</td>\n      <td>0.000000</td>\n      <td>IF outsized executive pay has indeed become a ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>16</td>\n      <td>2</td>\n      <td>32</td>\n      <td>0.462483</td>\n      <td>['pfizer', 'sharehold', 'board', 'vote', 'dire...</td>\n      <td>['battl', 'line', 'drawn', 'pfizer', 'owner', ...</td>\n      <td>0</td>\n      <td>0.187500</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>The battle lines have been drawn between Pfize...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>25</td>\n      <td>3</td>\n      <td>32</td>\n      <td>-0.879509</td>\n      <td>['pfizer', 'sharehold', 'board', 'vote', 'dire...</td>\n      <td>['one', 'side', 'stand', 'hank', 'mckinnel', '...</td>\n      <td>0</td>\n      <td>0.080000</td>\n      <td>0.0</td>\n      <td>0.012987</td>\n      <td>On one side stands Hank McKinnell, Pfizer's ch...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>13</td>\n      <td>4</td>\n      <td>32</td>\n      <td>0.995195</td>\n      <td>['pfizer', 'sharehold', 'board', 'vote', 'dire...</td>\n      <td>['pfizer', 'sharehold', 'angri', '46', 'percen...</td>\n      <td>0</td>\n      <td>0.153846</td>\n      <td>0.0</td>\n      <td>0.008403</td>\n      <td>On the other are Pfizer shareholders, angry ov...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>10</td>\n      <td>5</td>\n      <td>32</td>\n      <td>-0.769904</td>\n      <td>['pfizer', 'sharehold', 'board', 'vote', 'dire...</td>\n      <td>['sharehold', 'threaten', 'withhold', 'vote', ...</td>\n      <td>0</td>\n      <td>0.400000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>Some shareholders are threatening to withhold ...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2437022</th>\n      <td>7</td>\n      <td>42</td>\n      <td>46</td>\n      <td>-0.860405</td>\n      <td>['said', 'villag', '``', 'rape', 'girl', 'poli...</td>\n      <td>['shock', 'instanc', 'sexual', 'violenc', 'con...</td>\n      <td>82913</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>But shocking instances of sexual violence cont...</td>\n    </tr>\n    <tr>\n      <th>2437023</th>\n      <td>18</td>\n      <td>43</td>\n      <td>46</td>\n      <td>0.928123</td>\n      <td>['said', 'villag', '``', 'rape', 'girl', 'poli...</td>\n      <td>['``', 'law', 'much', 'end', 'someth', 'endem'...</td>\n      <td>82913</td>\n      <td>0.222222</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>\"Laws can only do so much when you have to end...</td>\n    </tr>\n    <tr>\n      <th>2437024</th>\n      <td>16</td>\n      <td>44</td>\n      <td>46</td>\n      <td>-0.269765</td>\n      <td>['said', 'villag', '``', 'rape', 'girl', 'poli...</td>\n      <td>['countri', 'new', 'prime', 'minist', 'narendr...</td>\n      <td>82913</td>\n      <td>0.187500</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>The country's new Prime Minister, Narendra Mod...</td>\n    </tr>\n    <tr>\n      <th>2437025</th>\n      <td>18</td>\n      <td>45</td>\n      <td>46</td>\n      <td>-0.599631</td>\n      <td>['said', 'villag', '``', 'rape', 'girl', 'poli...</td>\n      <td>['women', 'right', 'group', 'critic', 'say', '...</td>\n      <td>82913</td>\n      <td>0.055556</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>But women's rights groups have criticized what...</td>\n    </tr>\n    <tr>\n      <th>2437026</th>\n      <td>24</td>\n      <td>46</td>\n      <td>46</td>\n      <td>1.000000</td>\n      <td>['said', 'villag', '``', 'rape', 'girl', 'poli...</td>\n      <td>['read', 'grief', 'fear', 'blame', 'rape', 'in...</td>\n      <td>82913</td>\n      <td>0.166667</td>\n      <td>1.0</td>\n      <td>0.000000</td>\n      <td>READ MORE: Grief, fear, blame over rapes in In...</td>\n    </tr>\n  </tbody>\n</table>\n<p>2437027 rows × 11 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"preds_df=pd.DataFrame({\"text_id\": train_data[\"text_id\"].values, \"text_position\": train_data[\"text_position\"].values, \"sentence\": train_data[\"sentence\"].values, \"pred_rougeL\": model.predict(train_data_feats)})\n\n# n_largest=preds_df.groupby([\"text_id\"])[\"text_position\",\"pred_rougeL\"].apply(lambda x: x.nlargest(3,columns=[\"pred_rougeL\"]).sort_index())","metadata":{"execution":{"iopub.status.busy":"2022-08-01T17:29:07.932684Z","iopub.execute_input":"2022-08-01T17:29:07.933515Z","iopub.status.idle":"2022-08-01T17:29:08.214945Z","shell.execute_reply.started":"2022-08-01T17:29:07.933472Z","shell.execute_reply":"2022-08-01T17:29:08.213571Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"def scoring(pred_summary, ref_summary):\n    rougeL=0.0\n    r_scorer=rouge_scorer.RougeScorer([\"rouge1\",\"rouge2\",\"rougeL\"])\n    rougeL=r_scorer.score(ref_summary,pred_summary)\n    score_ind={\"precision\":0, \"recall\":1, \"fmeasure\":2}\n    return rougeL[\"rouge1\"][score_ind[\"fmeasure\"]], rougeL[\"rouge2\"][score_ind[\"fmeasure\"]], rougeL[\"rougeL\"][score_ind[\"fmeasure\"]]\n\ndef summarization(sentences):\n    return \".\".join(sentences)\n\ndef create_summary(df,s_num):\n    n_largest=df.groupby([\"text_id\"])[\"text_position\",\"sentence\",\"pred_rougeL\"].apply(lambda x: x.nlargest(s_num,columns=[\"pred_rougeL\"]).sort_index())\n    summary = summarization(n_largest[\"sentence\"].values)   \n    return summary\n\ndef get_ref_summary(df,doc_id):\n    print(df.loc[df.text_id==doc_id, \"summary\"].values)\n    return df.loc[df.text_id==doc_id, \"summary\"].values[0]\n\ndef get_preds(model,docX):\n    return model.predict(docX)\n\ndef single_doc_summarizer(model,X,y):\n    docs=X.groupby(\"text_id\")\n    for i,d in enumerate(docs):\n        if i>10:\n            break\n        document=d[1]\n        cols=[\"len\", \"s_position\", \"thematic_ratio\", \"s_pos_par\", \"num_ratio\"]\n        feats=document[cols]\n\n        preds=get_preds(model,feats)\n        document[\"pred_rougeL\"]=preds.tolist()\n        pred_summary=create_summary(document,3)\n        ref_summary=get_ref_summary(document,i)\n        print(scoring(pred_summary,ref_summary))\n        \n        \n        del document\n        del feats\n        del preds\n        del pred_summary\n        del ref_summary\n        gc.collect()\n#         pred\n\n","metadata":{"execution":{"iopub.status.busy":"2022-08-01T18:10:05.982061Z","iopub.execute_input":"2022-08-01T18:10:05.982816Z","iopub.status.idle":"2022-08-01T18:10:05.999597Z","shell.execute_reply.started":"2022-08-01T18:10:05.982776Z","shell.execute_reply":"2022-08-01T18:10:05.998290Z"},"trusted":true},"execution_count":97,"outputs":[]},{"cell_type":"code","source":"summaries=pd.read_csv(\"/kaggle/input/summarizer-data/train_summaries.csv\")\ntrain_len=train_data.groupby(\"text_id\").size().shape[0]\n\nsummaries=summaries.iloc[:train_len,:]\ntrain_data_feats[\"summary\"]=summaries[\"summary\"]\ntrain_data_feats[\"sentence\"]=train_data[\"sentence\"]\ntrain_data_feats[\"text_id\"]=train_data[\"text_id\"]\ntrain_data_feats[\"text_position\"]=train_data[\"text_position\"]\n\nX=train_data_feats\ny=pd.DataFrame(labels)\n\n# single_doc_summarizer(model,X,y)","metadata":{"execution":{"iopub.status.busy":"2022-08-01T18:09:05.537045Z","iopub.execute_input":"2022-08-01T18:09:05.538075Z","iopub.status.idle":"2022-08-01T18:09:20.359577Z","shell.execute_reply.started":"2022-08-01T18:09:05.538024Z","shell.execute_reply":"2022-08-01T18:09:20.358075Z"},"trusted":true},"execution_count":93,"outputs":[]},{"cell_type":"code","source":"single_doc_summarizer(model,X,y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"___\n___\n# **Notes**\n\n\n1. scoring -> label rougeL -> sentence feats -> train\n2. grid search ?! (or manual fine tuning)\n3. test -> input doc -> predict score -> keep N first sentences or keep those over a threshold -> create summary -> calculate rouge1/2/L\n\n___\n### **References**\n\n1. [Named Entity Recognition (NER) with TensorflowNamed Entity Recognition (NER) with Tensorflow](https://www.kaggle.com/code/naseralqaydeh/named-entity-recognition-ner-with-tensorflow)\n2. [Extractive Summarization using Deep LearningExtractive Summarization using Deep Learning](https://arxiv.org/pdf/1708.04439v1.pdf)\n3. [NLTK](https://www.bogotobogo.com/python/NLTK/Stemming_NLTK.php)\n4. [Text Features Library](https://github.com/pmbaumgartner/text-feat-lib/tree/master/notebooks)\n5. []()\n\n\n\n### **Feats**\n1. [Feature extraction](https://arxiv.org/pdf/1708.04439v1.pdf)\n    1. Number of thematic words\n    2. Sentence position\n    3. Sentence length\n    4. Sentence position relative to paragraph\n    5. Number of proper nouns\n    6. Number of numerals\n    7. Number of named entities\n    8. Term Frequency-Inverse Sentence Frequency\n    9. Sentence to Centroid similarity\n    \n    \n2. [Text Summarization References](https://github.com/Tian312/awesome-text-summarization/blob/master/README.md)\n\n\n\n___\n### **Feature Base**\n\nThe feature base model extracts the features of the sentence, then evaluate its importance. Here is the representative research.\nSentence Extraction Based Single Document Summarization\nFollowing features are used in the above method.\n\n1. Position of the sentence in the input document\n2. Presence of the verb in the sentence\n3. Length of the sentence\n4. Term frequency\n5. Named entity tag NE\n6. Font style\n\n…etc. All the features are accumulated as the score.\nThe No.of coreferences are the number of pronouns to the previous sentence. It is simply calculated by counting the pronouns occurred in the first half of the sentence. So the Score represents the reference to the previous sentence.\nNow we can evaluate each sentence. Next is selecting the sentence to avoid the duplicate of the information. In this paper, the same word between the new and selected sentence is considered. And the refinement to connect the selected sentences are executed.\nLuhn’s Algorithm is also feature base. It evaluates the “significance” of the word that is calculated from the frequency.\nYou can try feature base text summarization by TextTeaser (PyTeaser is available for Python user).","metadata":{}},{"cell_type":"markdown","source":"# Unused","metadata":{}},{"cell_type":"code","source":"# train_set = f\"..{os.sep}Data{os.sep}release{os.sep}train.jsonl\"\n# dev_set = f\"..{os.sep}Data{os.sep}release{os.sep}dev.jsonl\"\n# test_set = f\"..{os.sep}Data{os.sep}release{os.sep}test.jsonl\"\n# load json files and convert them to dataframes to load faster next time\n# train_df = funs.json_to_df(train_set,\"train\")\n# dev_df = funs.json_to_df(dev_set,\"dev\")\n# test_df = funs.json_to_df(test_set,\"test\")\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# colab command to download the dataset\n!kaggle datasets download -d tkylafi/summarizer-data","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}