{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from heapq import heapify\nimport os\nimport json\nfrom typing import Counter\nimport pandas as pd\nimport re\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pickle\n\nimport math\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer, PorterStemmer, SnowballStemmer, LancasterStemmer\nfrom nltk.tokenize import sent_tokenize, word_tokenize\nfrom nltk import pos_tag\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport sklearn.model_selection\nfrom sklearn.model_selection import RepeatedKFold, cross_val_score, GridSearchCV\nimport sklearn.preprocessing as preproc\nfrom sklearn.feature_extraction import text\nfrom sklearn.svm import LinearSVR\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression, ElasticNet, ElasticNetCV, LassoLars, SGDRegressor\nfrom sklearn.metrics import confusion_matrix, mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, MaxAbsScaler\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.svm import SVR\n\nfrom IPython.display import FileLink, FileLinks\n\nfrom tqdm import tqdm\n\n!pip install rouge-score\nfrom rouge_score import rouge_scorer\n\nimport psutil\nimport gc\nfrom collections import Counter","metadata":{"execution":{"iopub.status.busy":"2022-08-02T13:06:03.164507Z","iopub.execute_input":"2022-08-02T13:06:03.165346Z","iopub.status.idle":"2022-08-02T13:06:14.406205Z","shell.execute_reply.started":"2022-08-02T13:06:03.165306Z","shell.execute_reply":"2022-08-02T13:06:14.404683Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Requirement already satisfied: rouge-score in /opt/conda/lib/python3.7/site-packages (0.1.2)\nRequirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.7/site-packages (from rouge-score) (1.16.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from rouge-score) (1.21.6)\nRequirement already satisfied: absl-py in /opt/conda/lib/python3.7/site-packages (from rouge-score) (1.1.0)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.7/site-packages (from rouge-score) (3.7)\nRequirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from nltk->rouge-score) (8.0.4)\nRequirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.7/site-packages (from nltk->rouge-score) (2021.11.10)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from nltk->rouge-score) (1.0.1)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from nltk->rouge-score) (4.64.0)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from click->nltk->rouge-score) (4.12.0)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->click->nltk->rouge-score) (3.8.0)\nRequirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->click->nltk->rouge-score) (4.1.1)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"\ndef json_to_df(json_path,type):\n    with open(json_path, \"r\", encoding=\"utf-8\") as f: \n        lines = [eval(l) for l in f.readlines()]\n\n    # exclude lines with surrogates in their text/summary\n    surr = [ i for i,l in enumerate(lines) for k in l.keys() if k in [\"text\",\"summary\"] and re.search(r'[\\uD800-\\uDFFF]', l[k])!=None ]\n\n    lines = [ l for i,l in zip( range(len(lines)),lines ) if i not in surr ]\n\n    cols=[ \"title\",\t\"date\",\t\"text\",\t\"summary\", \"compression\", \"coverage\", \"density\", \"compression_bin\", \"coverage_bin\"]\n\n    # we need only the extractive summaries as we are building an extractive summarizer\n    data=[ [ l[k] for k in l.keys() if k in cols ] for l in lines if l[\"density_bin\"]==\"extractive\" ]\n    df = pd.DataFrame(data,columns=cols)\n\n    df.to_csv(f\"..{os.sep}Data{os.sep}DataFrames{os.sep}{type}_set.csv\", header=True, index=False )\n\n    return df\n\n\n# text processing functions\n\n# A list of contractions from http://stackoverflow.com/questions/19790188/expanding-english-language-contractions-in-python\ncontractions = { \"ain't\": \"am not\", \"aren't\": \"are not\", \"can't\": \"cannot\", \"can't've\": \"cannot have\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \"couldn't've\": \"could not have\", \"didn't\": \"did not\", \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hadn't've\": \"had not have\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\", \"he'd've\": \"he would have\", \"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'll\": \"how will\", \"how's\": \"how is\", \"i'd\": \"i would\", \"i'll\": \"i will\", \"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'll\": \"it will\", \"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\", \"mightn't\": \"might not\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"needn't\": \"need not\", \"oughtn't\": \"ought not\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"she'd\": \"she would\", \"she'll\": \"she will\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"that'd\": \"that would\", \"that's\": \"that is\", \"there'd\": \"there had\", \"there's\": \"there is\", \"they'd\": \"they would\", \"they'll\": \"they will\", \"they're\": \"they are\", \"they've\": \"they have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'll\": \"we will\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what're\": \"what are\", \"what's\": \"what is\", \"what've\": \"what have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"who'll\": \"who will\", \"who's\": \"who is\", \"won't\": \"will not\", \"wouldn't\": \"would not\", \"you'd\": \"you would\", \"you'll\": \"you will\", \"you're\": \"you are\" }\n\n\ndef sentence_cleaning(text, remove_stopwords = True, sub_contractions=True, stemming=True):\n    global pbar_cleaning\n    pbar_cleaning.update(1)\n    \n    # Convert words to lower case\n    text = text.lower()\n    toks = word_tokenize(text)\n      \n    # Replace contractions with their longer forms \n    if sub_contractions:\n        new_text = []\n        for word in toks:\n            if word in contractions:\n                new_text.append(contractions[word])\n            else:\n                new_text.append(word)\n    \n    text = \" \".join(new_text)\n\n    # Format words and remove unwanted characters\n    text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n    text = re.sub(r'\\<a href', ' ', text)\n    text = re.sub(r'&amp;', '', text) \n    text = re.sub(r'[_\"\\-;%()|+&=*%.,!?:#$@\\[\\]/]', ' ', text)\n    text = re.sub(r'<br />', ' ', text)\n    text = re.sub(r'\\'', ' ', text)\n\n    toks_clean = word_tokenize(text)\n    \n    if remove_stopwords:\n        stops = set(stopwords.words(\"english\"))\n        toks_clean = [w for w in toks_clean if not w in stops]\n\n    \n    if stemming: \n        stemmer=SnowballStemmer(language=\"english\")\n        toks_clean=[ stemmer.stem(w) for w in toks_clean ]\n\n    text = \" \".join(toks_clean)\n    \n    return text, toks_clean\n\n\ndef rouge_scoring(sentence,summary,sc_type=\"rougeL\",score=\"fmeasure\"):\n    global pbar\n    pbar.update(1)\n    r_scorer=rouge_scorer.RougeScorer([\"rouge1\",\"rouge2\",\"rougeL\"])\n    r_L=r_scorer.score(summary,sentence)\n    score_ind={\"precision\":0, \"recall\":1, \"fmeasure\":2}\n    \n    return r_L[sc_type][score_ind[score]]\n\n\ndef text_processing(df,data_type,df_dir,sc_type=\"rougeL\"):\n    global pbar\n    cols=[\"sentence\", \"summary\", \"text\"] \n    # new_df=pd.DataFrame()\n\n    # sentence split \n    sentences=[ sent_tokenize(t) for t in df[\"text\"].values ]\n\n    summaries=df[\"summary\"].values\n    sent_sum_text=[ [ s,summary,t  ] for s_list,summary,t in zip( sentences, summaries, df[\"text\"] ) for s in s_list ]\n    new_df=pd.DataFrame(sent_sum_text, columns=cols)\n    new_df[\"text_id\"]=new_df[\"text\"].factorize()[0]\n    new_df[\"chosen\"]= 0\n    ind = new_df[[ s in t for s,t in zip( new_df[\"sentence\"], new_df[\"summary\"] ) ]].index\n    new_df.loc[ind,\"chosen\"]=1\n    del new_df[\"text\"]\n    # for c in new_df.columns:\n    #   new_df[c]=new_df[c].astype(str)\n\n    # labels\n    # columns -> sentence: 0, summary: 1, text: 2\n    pbar = tqdm(total=new_df.shape[0] )\n    new_df[\"rougeL\"]= new_df.apply(lambda row: rouge_scoring(row[\"sentence\"],row[\"summary\"], sc_type=sc_type, score=\"fmeasure\" ), axis=1)\n    print(new_df[\"rougeL\"])\n\n    new_df[\"summary\"].to_csv(os.path.join(df_dir,f\"{data_type}_data_{sc_type}_summaries.csv\"), header=True, index=False)\n    del new_df[\"summary\"]\n    \n    new_df.to_csv(os.path.join(df_dir,f\"{data_type}_data_{sc_type}.csv\"), header=True, index=False)\n\n    return new_df\n\n\ndef add_chosen_text_id(df):\n    cols=[\"sentence\", \"summary\", \"text\"] \n    # new_df=pd.DataFrame()\n\n    # sentence split \n    sentences=[ sent_tokenize(t) for t in df[\"text\"].values ]\n    summaries=df[\"summary\"].values\n    \n    sent_sum_text=[ [ s,summary,t  ] for s_list,summary,t in zip( sentences, summaries, df[\"text\"] ) for s in s_list ]\n    new_df=pd.DataFrame(sent_sum_text, columns=cols)\n    \n    new_df[\"text_id\"]=new_df[\"text\"].factorize()[0]\n    new_df[\"chosen\"]= 0\n    ind = new_df[[ s in t for s,t in zip( new_df[\"sentence\"], new_df[\"summary\"] ) ]].index\n    new_df.loc[ind,\"chosen\"]=1\n    \n    del new_df[\"text\"]\n    del new_df[\"summary\"]\n    del new_df[\"sentence\"]\n    \n    return new_df\n\n\ndef create_labels(data_dir,df_dir=\"/kaggle/input/summarizer-data/\"):\n#     train_df = pd.read_csv(os.path.join(df_dir,\"train_set.csv\"))\n    dev_df = pd.read_csv(os.path.join(df_dir,\"dev_set.csv\"))\n    test_df = pd.read_csv(os.path.join(df_dir,\"test_set.csv\"))\n\n#     train_df1, train_df2, train_df3, train_df4 = np.array_split(train_df, 4)\n    splits=4\n#     train_data_list=[]\n#     for i,train_df in enumerate(np.array_split(train_df, splits)):\n#         train_data = text_processing(train_df,f\"train{i+1}\",data_dir,\"rougeL\")\n#         train_data_list.append(train_data)\n\n    dev_data = text_processing(dev_df,\"dev\",data_dir,\"rougeL\")\n    test_data = text_processing(test_df,\"test\",data_dir,\"rougeL\")\n    \n    return train_data_list, dev_data, test_data\n\n\ndef df_add_tid(data_dir, df_dir):\n    sc_type=\"rougeL\"\n    data_labels_dir=\"/kaggle/input/summarizer-data\"\n\n#     data_type=\"train\"\n#     train_df = pd.read_csv(os.path.join(df_dir,f\"{data_type}_set.csv\"))\n#     splits=4\n#     for i,df in enumerate(np.array_split(train_df, 4)):\n#         if i==2:\n#             data_type=f\"train{i+1}\"\n#             print(data_type+\"\\n\")\n#             train_ch_tid_df=add_chosen_text_id(df)\n#             train_rougeL=pd.read_csv(os.path.join(data_labels_dir,f\"{data_type}_data_{sc_type}.csv\"))\n#             print(train_rougeL.columns, train_rougeL.shape)\n#             train_rougeL[\"text_id\"]=train_ch_tid_df[\"text_id\"]\n#             train_rougeL[\"chosen\"]=train_ch_tid_df[\"chosen\"]\n#             print(train_rougeL.columns, train_rougeL.shape)\n#             train_rougeL.to_csv(os.path.join(data_dir,f\"{data_type}_data_{sc_type}_tid.csv\"), header=True, index=False)\n\n    data_type=\"dev\" \n    dev_df = pd.read_csv(os.path.join(df_dir,f\"{data_type}_set.csv\"))\n    dev_ch_tid_df=add_chosen_text_id(dev_df)\n    dev_rougeL=pd.read_csv(os.path.join(data_labels_dir,f\"{data_type}_data_{sc_type}.csv\"))\n    print(dev_rougeL.columns, dev_rougeL.shape)\n    dev_rougeL[\"text_id\"]=dev_ch_tid_df[\"text_id\"]\n    dev_rougeL[\"chosen\"]=dev_ch_tid_df[\"chosen\"]\n    print(dev_rougeL.columns, dev_rougeL.shape)\n    dev_rougeL.to_csv(os.path.join(data_dir,f\"{data_type}_data_{sc_type}_tid.csv\"), header=True, index=False)\n\n    data_type=\"test\"\n    test_df = pd.read_csv(os.path.join(df_dir,f\"{data_type}_set.csv\"))\n    test_ch_tid_df=add_chosen_text_id(test_df)\n    test_rougeL=pd.read_csv(os.path.join(data_labels_dir,f\"{data_type}_data_{sc_type}.csv\"))\n    print(test_rougeL.columns, test_rougeL.shape)\n    test_rougeL[\"text_id\"]=test_ch_tid_df[\"text_id\"]\n    test_rougeL[\"chosen\"]=test_ch_tid_df[\"chosen\"]\n    print(test_rougeL.columns, test_rougeL.shape)\n    test_rougeL.to_csv(os.path.join(data_dir,f\"{data_type}_data_{sc_type}_tid.csv\"), header=True, index=False)\n\n    \ndef data_stats(df):\n    groups = df.groupby(\"chosen\")\n    print(groups.describe()[\"rougeL\"])\n\n    \ndef thematic_ratio(them_words, word_list):\n    them_occ= sum( [ word_list.count(w) for w in set(them_words)&set(word_list)])\n    them_ratio=them_occ/len(word_list)\n    return them_ratio\n\n\ndef s_position(t_position,tot_sent):\n    N=tot_sent\n    th=0.2*N\n    min_p= th*N\n    max_p= 2*th*N\n    \n    if t_position==tot_sent or t_position==1:\n        pos=1.0\n    else: \n        pos=math.cos((t_position - min_p)*((1/max_p) - min_p))\n        \n    return pos\n    \n    \ndef prop_nouns(tokens):\n    if type(tokens)!=type(list):\n        tokens=eval(tokens)\n    pos= nltk.pos_tag(tokens)\n    tags_count=Counter(tag for _, tag in pos if tag==\"NNP\" or tag==\"NNPS\")\n    return tags_count[\"NNP\"]+tags_count[\"NNPS\"] \n\n\ndef feature_df(df, data_dir, data_type):\n    global pbar_cleaning\n    feat_df=pd.DataFrame()\n    \n    # tokenize\n    pbar_cleaning=tqdm(total=train_data.shape[0], leave=True)\n    df[\"tokens\"] = df[\"sentence\"].apply(lambda x: sentence_cleaning(x)[1]) \n    \n\n    # create sentence features\n    # 1. thematic words\n    col = df.groupby(\"text_id\")[\"tokens\"].apply(sum)\n    thematic_cols= pd.DataFrame({\"text_id\": col.index, \"thematic\": [ [ t[0] for t in Counter(x).most_common(10) ] for x in col ]})  \n    df=df.join(thematic_cols[\"thematic\"], on='text_id' )\n    feat_df[\"thematic_ratio\"] = df.apply(lambda row: thematic_ratio(row.thematic, row.tokens) if len(row.tokens)>0 else 0.0, axis=1)\n\n    # 2. sentence position in the text\n    feat_df[\"text_position\"] = df.groupby(\"text_id\").cumcount().add(1)\n    df[\"tot_sent\"] = df.groupby(\"text_id\")[\"sentence\"].transform(len)\n    feat_df[\"s_position\"] = df.apply(lambda row: s_position(row.text_position,row.tot_sent), axis=1)\n\n    # 3. sentence length - threshold=3\n    threshold=3\n    feat_df[\"len\"]= df[\"tokens\"].apply(lambda x: 0 if len(x)<threshold else len(x))\n\n    # 4. sentence position - paragraph relative\n    feat_df['s_pos_par'] = feat_df[\"s_position\"].values\n    feat_df.loc[feat_df.s_pos_par!=1.0, 's_pos_par']=0.0\n\n    # 5. numerals\n    feat_df[\"num_ratio\"]=df[\"tokens\"].apply(lambda x: sum( [ 1 for t in x if t.isnumeric() ] )/len(x) if len(x)>0 else 0 )\n\n\n    # ?. Term Frequency-Inverse Sentence Frequency\n\n\n    # ?. proper nouns - not so useful\n    # train_data_feats[\"NNPs\"]=train_data_feats[\"tokens\"].apply(lambda x: prop_nouns(x) )\n\n\n\n\n    # train_data_feats=train_data[[\"len\",\"text_position\"]]\n    feat_df.to_csv(os.path.join(data_dir,f\"{data_type}_set_feats.csv\"), header=True, index=False)\n\n    return feat_df\n\n\n\n# SVM_scaler =  StandardScaler()\n# LR_scaler =  MinMaxScaler()\n# KNN_scaler =  StandardScaler()\n# # classifier parameters\n# KNN_n_num = 9\n# LR_C = 1.0\n# SVM_C = 0.001\n# algos={\n#   \"ElNet\": make_pipeline(SVM_scaler, SVC(C=SVM_C)),\n#   \"LR\":  make_pipeline(LR_scaler, LogisticRegression(C=LR_C)),\n#   \"KNN\": make_pipeline(KNN_scaler, KNeighborsClassifier(n_neighbors = KNN_n_num)),\n# }\n\n\ndef classifier_training(model,X_train,y_train):\n    model.fit(X_train,y_train)\n    preds=model.predict(X_train)\n    c_rep=classification_report(y_train,preds)\n    c_rep_dict=classification_report(y_train,preds,output_dict=True)\n    return model, c_rep, c_rep_dict\n\ndef classifier_validation(model,X_dev,y_dev):\n    preds=model.predict(X_dev)\n    c_rep = classification_report(y_dev,preds)\n    c_rep_dict=classification_report(y_dev,preds,output_dict=True)\n    return c_rep, c_rep_dict\n\ndef classifier_test(model,X_test,y_test):\n    preds=model.predict(X_test)\n    c_rep = classification_report(y_test,preds)\n    c_rep_dict=classification_report(y_test,preds,output_dict=True)\n    return c_rep, c_rep_dict\n\n\n# Training - Validation - Test pipeline\ndef classifier_T_V_T(X_train, y_train, X_dev, y_dev, X_test, y_test, algo_type=\"LR\"):\n    model=algos[algo_type]\n\n    model,c_rep_train,c_rep_dict_train=classifier_training(model,X_train,y_train)\n    c_rep_dev,c_rep_dict_dev=classifier_validation(model,X_dev,y_dev)\n    c_rep_test,c_rep_dict_test=classifier_validation(model,X_test,y_test)\n\n    return model, c_rep_train, c_rep_dict_train, c_rep_dev, c_rep_dict_dev, c_rep_test, c_rep_dict_test\n\n","metadata":{"execution":{"iopub.status.busy":"2022-08-02T13:06:14.409322Z","iopub.execute_input":"2022-08-02T13:06:14.409750Z","iopub.status.idle":"2022-08-02T13:06:14.477027Z","shell.execute_reply.started":"2022-08-02T13:06:14.409709Z","shell.execute_reply":"2022-08-02T13:06:14.476016Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# memory clean\n# for v in globals():\n#     print(v)\n#     if str(v) not \"__name__\":\n#         del v\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-08-02T13:06:43.204517Z","iopub.execute_input":"2022-08-02T13:06:43.204954Z","iopub.status.idle":"2022-08-02T13:06:43.382279Z","shell.execute_reply.started":"2022-08-02T13:06:43.204919Z","shell.execute_reply":"2022-08-02T13:06:43.381152Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"23"},"metadata":{}}]},{"cell_type":"code","source":"df_dir=\"/kaggle/input/summarizer-data\"\n\ndata_dir= \"/kaggle/working/Data/DataFrames\"\nif not os.path.exists(data_dir):\n    os.makedirs(data_dir)\n\n# load saved dataframes and create labels\n# train_data_list, dev_data, test_data = create_labels(data_dir)\n\n# load dataframes with labels\n# df_add_tid(data_dir, df_dir)\n\nFileLinks(\".\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-output":false,"execution":{"iopub.status.busy":"2022-08-02T13:06:46.156930Z","iopub.execute_input":"2022-08-02T13:06:46.157511Z","iopub.status.idle":"2022-08-02T13:06:46.165467Z","shell.execute_reply.started":"2022-08-02T13:06:46.157477Z","shell.execute_reply":"2022-08-02T13:06:46.164358Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"./\n  __notebook_source__.ipynb","text/html":"./<br>\n&nbsp;&nbsp;<a href='./__notebook_source__.ipynb' target='_blank'>__notebook_source__.ipynb</a><br>"},"metadata":{}}]},{"cell_type":"code","source":"# load training dataset\nsc_type=\"rougeL\"\ndata_type=\"train1\"\ntrain_data=pd.read_csv(os.path.join(df_dir,f\"{data_type}_data_{sc_type}_tid.csv\"))\ndata_stats(train_data)\nlabels=train_data[\"rougeL\"].values\ndel train_data[\"rougeL\"]\n\n\nsc_type=\"rougeL\"\ndata_type=\"dev\"\ndev_data=pd.read_csv(os.path.join(df_dir,f\"{data_type}_data_{sc_type}_tid.csv\"))\ndata_stats(dev_data)\nlabels_dev=dev_data[\"rougeL\"].values\ndel dev_data[\"rougeL\"]\n\n\nsc_type=\"rougeL\"\ndata_type=\"test\"\ntest_data=pd.read_csv(os.path.join(df_dir,f\"{data_type}_data_{sc_type}_tid.csv\"))\ndata_stats(test_data)\nlabels_test=test_data[\"rougeL\"].values\ndel test_data[\"rougeL\"]","metadata":{"execution":{"iopub.status.busy":"2022-08-02T13:06:47.699485Z","iopub.execute_input":"2022-08-02T13:06:47.700319Z","iopub.status.idle":"2022-08-02T13:07:00.597116Z","shell.execute_reply.started":"2022-08-02T13:06:47.700277Z","shell.execute_reply":"2022-08-02T13:07:00.595745Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"            count      mean       std  min       25%       50%       75%  max\nchosen                                                                       \n0       2310412.0  0.105404  0.120294  0.0  0.050000  0.083333  0.122449  1.0\n1        126615.0  0.349349  0.234293  0.0  0.177295  0.306569  0.461538  1.0\n            count      mean       std  min       25%       50%       75%  max\nchosen                                                                       \n0       1019122.0  0.109952  0.127384  0.0  0.051948  0.088889  0.128205  1.0\n1         29440.0  0.457427  0.297148  0.0  0.216498  0.392638  0.666667  1.0\n            count      mean       std  min       25%       50%       75%  max\nchosen                                                                       \n0       1006718.0  0.109932  0.127422  0.0  0.051948  0.088889  0.128205  1.0\n1         29216.0  0.456340  0.299931  0.0  0.210740  0.389262  0.666667  1.0\n","output_type":"stream"}]},{"cell_type":"code","source":"def load_datasets(df_dir, sc_type=\"rougeL\"):\n    # train_data_feats=pd.read_csv(\"/kaggle/input/summarizer-data/train1_set_feats.csv\")\n    data_type=\"train1\"\n    print(f\"Loading {data_type.capitalize()} Data . . \")\n    # train_data=feature_df(train_data, data_dir, data_type)\n    train_data=pd.read_csv(f\"/kaggle/input/summarizer-data/{data_type}_set_feats.csv\") \n\n    train_data_sent=pd.read_csv(os.path.join(df_dir,f\"{data_type}_data_{sc_type}_tid.csv\"))\n\n    train_data_feats=train_data\n    train_data[\"sentence\"]=train_data_sent[\"sentence\"]\n\n    train_data_feats=train_data_feats[[\"len\", \"s_position\", \"thematic_ratio\", \"s_pos_par\", \"num_ratio\"]]\n    # lens= StandardScaler().fit_transform(np.array(train_data_feats[\"len\"]).reshape(-1,1) )\n    # train_data_feats[\"len\"]=lens\n    # train_data_feats\n\n\n    data_type=\"dev\"\n    print(f\"Loading {data_type.capitalize()} Data . . \")\n    # dev_data=feature_df(dev_data, data_dir, data_type)\n    dev_data=pd.read_csv(f\"/kaggle/input/summarizer-data/{data_type}_set_feats.csv\") \n\n    dev_data_sent=pd.read_csv(os.path.join(df_dir,f\"{data_type}_data_{sc_type}_tid.csv\"))\n    dev_data_feats=dev_data\n    dev_data[\"sentence\"]=dev_data_sent[\"sentence\"]\n    dev_data_feats=dev_data_feats[[\"len\", \"s_position\", \"thematic_ratio\", \"s_pos_par\", \"num_ratio\"]]\n\n\n    data_type=\"test\"\n    print(f\"Loading {data_type.capitalize()} Data . . \")\n    # test_data=feature_df(test_data, data_dir, data_type)\n    test_data=pd.read_csv(f\"/kaggle/input/summarizer-data/{data_type}_set_feats.csv\") \n\n    test_data_sent=pd.read_csv(os.path.join(df_dir,f\"{data_type}_data_{sc_type}_tid.csv\"))\n    test_data_feats=test_data\n    test_data[\"sentence\"]=test_data_sent[\"sentence\"]\n    test_data_feats=test_data_feats[[\"len\", \"s_position\", \"thematic_ratio\", \"s_pos_par\", \"num_ratio\"]]\n    \n    return train_data, train_data_feats, dev_data, dev_data_feats, test_data, test_data_feats\n\n\n","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-08-02T13:07:00.599230Z","iopub.execute_input":"2022-08-02T13:07:00.599609Z","iopub.status.idle":"2022-08-02T13:07:00.610789Z","shell.execute_reply.started":"2022-08-02T13:07:00.599550Z","shell.execute_reply":"2022-08-02T13:07:00.609826Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"score_type=\"rougeL\"\ntrain_data, train_data_feats, dev_data, dev_data_feats, test_data, test_data_feats = load_datasets(df_dir, sc_type=score_type)\n\nFileLinks(\".\")","metadata":{"execution":{"iopub.status.busy":"2022-08-02T13:40:07.645638Z","iopub.execute_input":"2022-08-02T13:40:07.646035Z","iopub.status.idle":"2022-08-02T13:40:31.313040Z","shell.execute_reply.started":"2022-08-02T13:40:07.646005Z","shell.execute_reply":"2022-08-02T13:40:31.311814Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stdout","text":"Loading Train1 Data . . \nLoading Dev Data . . \nLoading Test Data . . \n","output_type":"stream"},{"execution_count":34,"output_type":"execute_result","data":{"text/plain":"./\n  __notebook_source__.ipynb\n./Data/\n  LinearSVR()_grid_search_results.txt\n  SGD_model.pkl","text/html":"./<br>\n&nbsp;&nbsp;<a href='./__notebook_source__.ipynb' target='_blank'>__notebook_source__.ipynb</a><br>\n./Data/<br>\n&nbsp;&nbsp;<a href='./Data/LinearSVR()_grid_search_results.txt' target='_blank'>LinearSVR()_grid_search_results.txt</a><br>\n&nbsp;&nbsp;<a href='./Data/SGD_model.pkl' target='_blank'>SGD_model.pkl</a><br>"},"metadata":{}}]},{"cell_type":"code","source":"# # tokenize\n# global pbar_cleaning\n# pbar_cleaning=tqdm(total=train_data.shape[0], leave=True)\n# train_data[\"tokens\"] = train_data[\"sentence\"].apply(lambda x: sentence_cleaning(x)[1]) \n\n# train_data_feats=pd.DataFrame()\n# # create sentence features\n# # 1. thematic words\n# col = train_data.groupby(\"text_id\").tokens.apply(sum)\n# thematic_cols= pd.DataFrame({\"text_id\": col.index, \"thematic\": [ [ t[0] for t in Counter(x).most_common(10) ] for x in col ]})  \n# train_data=train_data.join(thematic_cols[\"thematic\"], on='text_id' )\n# train_data_feats[\"thematic_ratio\"] = train_data.apply(lambda row: thematic_ratio(row.thematic, row.tokens) if len(row.tokens)>0 else 0.0, axis=1)\n\n# # 2. sentence position in the text\n# train_data[\"text_position\"] = train_data.groupby(\"text_id\").cumcount().add(1)\n# train_data[\"tot_sent\"] = train_data.groupby(\"text_id\")[\"sentence\"].transform(len)\n# train_data_feats[\"s_position\"] = train_data.apply(lambda row: s_position(row.text_position,row.tot_sent), axis=1)\n\n# # 3. sentence length - threshold=3\n# threshold=3\n# train_data_feats[\"len\"]= train_data[\"tokens\"].apply(lambda x: 0 if len(x)<threshold else len(x))\n\n# # 4. sentence position - paragraph relative\n# train_data_feats['s_pos_par'] = train_data_feats[\"s_position\"].values\n# train_data_feats.loc[train_data_feats.s_pos_par!=1.0, 's_pos_par']=0.0\n\n# # 5. numerals\n# train_data_feats[\"num_ratio\"]=train_data_feats[\"tokens\"].apply(lambda x: sum( [ 1 for t in eval(x) if t.isnumeric() ] )/len(x) )\n\n\n# # ?. Term Frequency-Inverse Sentence Frequency\n\n\n# # ?. proper nouns - not so useful\n# # train_data_feats[\"NNPs\"]=train_data_feats[\"tokens\"].apply(lambda x: prop_nouns(x) )\n\n\n\n\n# # train_data_feats=train_data[[\"len\",\"text_position\"]]\n# train_data_feats.to_csv(os.path.join(data_dir,f\"{data_type}_set_feats.csv\"), header=True, index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train_data_feats=train_data[[\"len\",\"text_position\",\"thematic_words\", ]]\n# data_type=\"train1\"\n# train_data.to_csv(os.path.join(data_dir,f\"{data_type}_set_feats.csv\"), header=True, index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# FileLinks(\".\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sc_type=\"rougeL\"\n# data_type=\"train1\"\n# train_data=pd.read_csv(os.path.join(df_dir,f\"{data_type}_data_{sc_type}_tid.csv\"))\n# data_stats(train_data)\n# labels=train_data[\"rougeL\"].values\n\n# sc_type=\"rougeL\"\n# data_type=\"dev\"\n# dev_data=pd.read_csv(os.path.join(df_dir,f\"{data_type}_data_{sc_type}_tid.csv\"))\n# data_stats(dev_data)\n# labels=dev_data[\"rougeL\"].values","metadata":{"execution":{"iopub.status.busy":"2022-08-02T00:11:56.581636Z","iopub.execute_input":"2022-08-02T00:11:56.582151Z","iopub.status.idle":"2022-08-02T00:12:13.593052Z","shell.execute_reply.started":"2022-08-02T00:11:56.582104Z","shell.execute_reply":"2022-08-02T00:12:13.591998Z"},"trusted":true},"execution_count":59,"outputs":[{"name":"stdout","text":"            count      mean       std  min       25%       50%       75%  max\nchosen                                                                       \n0       2310412.0  0.105404  0.120294  0.0  0.050000  0.083333  0.122449  1.0\n1        126615.0  0.349349  0.234293  0.0  0.177295  0.306569  0.461538  1.0\n            count      mean       std  min       25%       50%       75%  max\nchosen                                                                       \n0       1019122.0  0.109952  0.127384  0.0  0.051948  0.088889  0.128205  1.0\n1         29440.0  0.457427  0.297148  0.0  0.216498  0.392638  0.666667  1.0\n","output_type":"stream"}]},{"cell_type":"code","source":"# test_num=1000\n\n# model = make_pipeline(MinMaxScaler(), ElasticNet(alpha=5e-3, warm_start=True, random_state=99, fit_intercept=True))\n# model.fit(train_data_feats, labels)\n\n# cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n# # evaluate model\n# scores_mae = cross_val_score(model, train_data_feats.iloc[:test_num,:] , labels[:test_num], scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1)\n# scores_mse = cross_val_score(model, train_data_feats.iloc[:test_num,:] , labels[:test_num], scoring='neg_mean_squared_error', cv=cv, n_jobs=-1)\n# # scores_r2 = cross_val_score(model, train_data_feats.iloc[:test_num,:] , labels[:test_num], scoring='r2', cv=cv, n_jobs=-1)\n\n# # force scores to be positive\n# scores_mae = np.absolute(scores_mae)\n# scores_mse = np.absolute(scores_mse)\n# # scores_r2 = np.absolute(scores_r2)\n# print('Mean MAE: %.3f (%.3f)' % (np.mean(scores_mae), np.std(scores_mae)))\n# print('Mean MSE: %.3f (%.3f)' % (np.mean(scores_mse), np.std(scores_mse)))\n# # print('Mean R2: %.3f (%.3f)' % (np.mean(scores_r2), np.std(scores_r2)))\n\n# print(labels)\n# print(model.predict(train_data_feats))\n# # svr_results(labels[:test_num],  train_data_feats.iloc[:test_num,:], model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# test_num=10000\n\n# # model = make_pipeline(MinMaxScaler(), ElasticNetCV(random_state=99))\n# model =   make_pipeline(StandardScaler(),ElasticNet(alpha=5e-6, random_state=99, fit_intercept=True))\n# model.fit(train_data_feats, labels)\n\n# cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n# # evaluate model\n# scores_mae = cross_val_score(model, train_data_feats.iloc[:test_num,:] , labels[:test_num], scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1)\n# scores_mse = cross_val_score(model, train_data_feats.iloc[:test_num,:] , labels[:test_num], scoring='neg_mean_squared_error', cv=cv, n_jobs=-1)\n# # scores_r2 = cross_val_score(model, train_data_feats.iloc[:test_num,:] , labels[:test_num], scoring='r2', cv=cv, n_jobs=-1)\n\n# # force scores to be positive\n# scores_mae = np.absolute(scores_mae)\n# scores_mse = np.absolute(scores_mse)\n# # scores_r2 = np.absolute(scores_r2)\n# print('Mean MAE: %.3f (%.3f)' % (np.mean(scores_mae), np.std(scores_mae)))\n# print('Mean MSE: %.3f (%.3f)' % (np.mean(scores_mse), np.std(scores_mse)))\n# # print('Mean R2: %.3f (%.3f)' % (np.mean(scores_r2), np.std(scores_r2)))\n\n# print(labels)\n# print(model.predict(train_data_feats))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# test_num=1000\n\n# # model = make_pipeline(MinMaxScaler(), ElasticNetCV(random_state=99))\n# eps=0\n# model = make_pipeline(StandardScaler(), LinearSVR(epsilon=eps, C=5e-1, fit_intercept=True, intercept_scaling=1.0, loss=\"squared_epsilon_insensitive\"))\n# # model = LinearSVR(epsilon=eps, C=5e-4, fit_intercept=True)\n# model.fit(train_data_feats, labels)\n\n# cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n# # evaluate model\n# scores_mae = cross_val_score(model, train_data_feats.iloc[:test_num,:] , labels[:test_num], scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1)\n# scores_mse = cross_val_score(model, train_data_feats.iloc[:test_num,:] , labels[:test_num], scoring='neg_mean_squared_error', cv=cv, n_jobs=-1)\n# # scores_r2 = cross_val_score(model, train_data_feats.iloc[:test_num,:] , labels[:test_num], scoring='r2', cv=cv, n_jobs=-1)\n\n# # force scores to be positive\n# scores_mae = np.absolute(scores_mae)\n# scores_mse = np.absolute(scores_mse)\n# # scores_r2 = np.absolute(scores_r2)\n# print('Mean MAE: %.3f (%.3f)' % (np.mean(scores_mae), np.std(scores_mae)))\n# print('Mean MSE: %.3f (%.3f)' % (np.mean(scores_mse), np.std(scores_mse)))\n# # print('Mean R2: %.3f (%.3f)' % (np.mean(scores_r2), np.std(scores_r2)))\n\n# print(labels)\n# print(model.predict(train_data_feats))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# grid search - Dev Set Tuning\ndef estimator_tuning(X, y, estimator=SGDRegressor(), scaler=StandardScaler()):\n    model_grid = make_pipeline(scaler, estimator )\n\n    param_grid = {\n        \"sgdregressor__alpha\": [8.192e-10],\n        \"sgdregressor__tol\": [6.4e-5],\n        \"sgdregressor__epsilon\": [3.2e-4],\n        \"sgdregressor__loss\": [\"squared_error\"],\n        \"sgdregressor__penalty\": [\"elasticnet\"],\n        \"sgdregressor__learning_rate\": [\"adaptive\"]\n    }\n    # \"sgdregressor__alpha\": 5.0 ** -np.arange(2, 7)\n    # \"sgdregressor__tol\": 10.0 ** -np.arange(2, 7)\n    # \"sgdregressor__loss\": [\"squared_error\", \"huber\", \"epsilon_insensitive\"]\n    # sgdregressor__penalty\": [\"l2\", \"l1\", \"elasticnet\"]\n    # \"sgdregressor__learning_rate\": [\"constant\", \"optimal\", \"invscaling\", \"adaptive\"]\n\n    g_search = GridSearchCV(model_grid, param_grid, verbose=9, return_train_score=True, cv=2)\n    g_search.fit(X, y)\n    \n    with open(os.path.join(\"/kaggle/working/Data\",f\"{estimator}_grid_search_results.txt\"), \"w\", encoding=\"utf-8\" ) as writer:\n          writer.write(f\"Best ParametersL:\\n{g_search.best_params_}\\n\\n\\n{g_search.cv_results_}\")\n\n    print(f\"Best score: { g_search.best_score_}\\nParams: {g_search.best_params_}\")\n    return g_search.best_estimator_\n\n\nbest_model=estimator_tuning(dev_data_feats,labels_dev)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-08-02T13:20:04.697727Z","iopub.execute_input":"2022-08-02T13:20:04.698145Z","iopub.status.idle":"2022-08-02T13:20:09.145964Z","shell.execute_reply.started":"2022-08-02T13:20:04.698113Z","shell.execute_reply":"2022-08-02T13:20:09.144630Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"Fitting 2 folds for each of 1 candidates, totalling 2 fits\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_17/3506592224.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0mbest_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mestimator_tuning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdev_data_feats\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels_dev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipykernel_17/3506592224.py\u001b[0m in \u001b[0;36mestimator_tuning\u001b[0;34m(X, y, estimator, scaler)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mg_search\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_grid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_train_score\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mg_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/kaggle/working/Data\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34mf\"{estimator}_grid_search_results.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mwriter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    889\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 891\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    892\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1390\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1392\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1393\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1394\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    849\u001b[0m                     )\n\u001b[1;32m    850\u001b[0m                     for (cand_idx, parameters), (split_idx, (train, test)) in product(\n\u001b[0;32m--> 851\u001b[0;31m                         \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcandidate_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    852\u001b[0m                     )\n\u001b[1;32m    853\u001b[0m                 )\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1039\u001b[0m             \u001b[0;31m# remaining jobs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1040\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1041\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1042\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1043\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    857\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    860\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    775\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    776\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 777\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    778\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    570\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 572\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 263\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 263\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sklearn/utils/fixes.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[1;32m    678\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 680\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    681\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    682\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    392\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"passthrough\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m                 \u001b[0mfit_params_last_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_params_steps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params_last_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_stochastic_gradient.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, coef_init, intercept_init, sample_weight)\u001b[0m\n\u001b[1;32m   1544\u001b[0m             \u001b[0mcoef_init\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcoef_init\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1545\u001b[0m             \u001b[0mintercept_init\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mintercept_init\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1546\u001b[0;31m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1547\u001b[0m         )\n\u001b[1;32m   1548\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_stochastic_gradient.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, alpha, C, loss, learning_rate, coef_init, intercept_init, sample_weight)\u001b[0m\n\u001b[1;32m   1493\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1494\u001b[0m             \u001b[0mcoef_init\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1495\u001b[0;31m             \u001b[0mintercept_init\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1496\u001b[0m         )\n\u001b[1;32m   1497\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_stochastic_gradient.py\u001b[0m in \u001b[0;36m_partial_fit\u001b[0;34m(self, X, y, alpha, C, loss, learning_rate, max_iter, sample_weight, coef_init, intercept_init)\u001b[0m\n\u001b[1;32m   1414\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1415\u001b[0m         self._fit_regressor(\n\u001b[0;32m-> 1416\u001b[0;31m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1417\u001b[0m         )\n\u001b[1;32m   1418\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_stochastic_gradient.py\u001b[0m in \u001b[0;36m_fit_regressor\u001b[0;34m(self, X, y, alpha, C, loss, learning_rate, sample_weight, max_iter)\u001b[0m\n\u001b[1;32m   1645\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1646\u001b[0m             \u001b[0mintercept_decay\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1647\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maverage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1648\u001b[0m         )\n\u001b[1;32m   1649\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"markdown","source":"### Best sofar","metadata":{}},{"cell_type":"code","source":"def estimator_training(train_data_feats, labels, test_data_feats, labels_test):\n    model = make_pipeline(StandardScaler(), SGDRegressor(alpha=8.192e-10, max_iter=1000, tol=6.4e-5, epsilon=3.2e-4, learning_rate=\"adaptive\", loss=\"squared_error\", penalty=\"elasticnet\"))\n    # model = LinearSVR(epsilon=eps, C=5e-4, fit_intercept=True)\n    model.fit(train_data_feats, labels)\n\n    cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n    \n    # evaluate model\n    test_samples=1000\n    # MAE\n    scores_mae = cross_val_score(model, test_data_feats.iloc[:test_samples,:] , labels_test[:test_samples], scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1)\n    scores_mae = np.absolute(scores_mae)\n    print('Mean MAE: %.3f (%.3f)' % (np.mean(scores_mae), np.std(scores_mae)))\n    # MSE\n    scores_mse = cross_val_score(model, test_data_feats.iloc[:test_samples,:] , labels_test[:test_samples], scoring='neg_mean_squared_error', cv=cv, n_jobs=-1)\n    scores_mse = np.absolute(scores_mse)\n    print('Mean MSE: %.3f (%.3f)' % (np.mean(scores_mse), np.std(scores_mse)))\n    # R2    \n    scores_r2 = cross_val_score(model, test_data_feats.iloc[:test_samples,:] , labels_test[:test_samples], scoring='r2', cv=cv, n_jobs=-1)\n#     scores_r2 = np.absolute(scores_r2)\n    print('Mean R2: %.3f (%.3f)' % (np.mean(scores_r2), np.std(scores_r2)))\n\n#     print(labels_test)\n#     print(model.predict(test_data_feats))\n    \n    return model\n\n\nmodel=estimator_training(train_data_feats, labels, test_data_feats, labels_test)\n\npkl_filepath=os.path.join(\"/kaggle/working/Data\",\"SGD_model.pkl\")\nwith open(pkl_filepath,\"wb\") as model_writer:\n    pickle.dump(model, model_writer)","metadata":{"execution":{"iopub.status.busy":"2022-08-02T13:32:36.372374Z","iopub.execute_input":"2022-08-02T13:32:36.373164Z","iopub.status.idle":"2022-08-02T13:32:36.732023Z","shell.execute_reply.started":"2022-08-02T13:32:36.373128Z","shell.execute_reply":"2022-08-02T13:32:36.730185Z"},"trusted":true},"execution_count":25,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_17/414537041.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mestimator_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data_feats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data_feats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0mpkl_filepath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/kaggle/working/Data\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"SGD_model.pkl\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_17/414537041.py\u001b[0m in \u001b[0;36mestimator_training\u001b[0;34m(train_data_feats, labels, test_data_feats, labels_test)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mStandardScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSGDRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8.192e-10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m6.4e-5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3.2e-4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"adaptive\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"squared_error\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpenalty\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"elasticnet\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;31m# model = LinearSVR(epsilon=eps, C=5e-4, fit_intercept=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data_feats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mcv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRepeatedKFold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_splits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_repeats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    392\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"passthrough\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m                 \u001b[0mfit_params_last_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_params_steps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params_last_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_stochastic_gradient.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, coef_init, intercept_init, sample_weight)\u001b[0m\n\u001b[1;32m   1544\u001b[0m             \u001b[0mcoef_init\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcoef_init\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1545\u001b[0m             \u001b[0mintercept_init\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mintercept_init\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1546\u001b[0;31m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1547\u001b[0m         )\n\u001b[1;32m   1548\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_stochastic_gradient.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, alpha, C, loss, learning_rate, coef_init, intercept_init, sample_weight)\u001b[0m\n\u001b[1;32m   1493\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1494\u001b[0m             \u001b[0mcoef_init\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1495\u001b[0;31m             \u001b[0mintercept_init\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1496\u001b[0m         )\n\u001b[1;32m   1497\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_stochastic_gradient.py\u001b[0m in \u001b[0;36m_partial_fit\u001b[0;34m(self, X, y, alpha, C, loss, learning_rate, max_iter, sample_weight, coef_init, intercept_init)\u001b[0m\n\u001b[1;32m   1398\u001b[0m             \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1399\u001b[0m             \u001b[0maccept_large_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1400\u001b[0;31m             \u001b[0mreset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfirst_call\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1401\u001b[0m         )\n\u001b[1;32m   1402\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    579\u001b[0m                 \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 581\u001b[0;31m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    582\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m    979\u001b[0m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmulti_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_numeric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_numeric\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    980\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 981\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    982\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    983\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    332\u001b[0m         raise ValueError(\n\u001b[1;32m    333\u001b[0m             \u001b[0;34m\"Found input variables with inconsistent numbers of samples: %r\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 334\u001b[0;31m             \u001b[0;34m%\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    335\u001b[0m         )\n\u001b[1;32m    336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [2437027, 1035934]"],"ename":"ValueError","evalue":"Found input variables with inconsistent numbers of samples: [2437027, 1035934]","output_type":"error"}]},{"cell_type":"code","source":"# preds_df=pd.DataFrame({\"text_id\": train_data[\"text_id\"].values, \"text_position\": train_data[\"text_position\"].values, \"sentence\": train_data[\"sentence\"].values, \"pred_rougeL\": model.predict(train_data_feats)})\n\n# n_largest=preds_df.groupby([\"text_id\"])[\"text_position\",\"pred_rougeL\"].apply(lambda x: x.nlargest(3,columns=[\"pred_rougeL\"]).sort_index())","metadata":{"execution":{"iopub.status.busy":"2022-08-01T17:29:07.932684Z","iopub.execute_input":"2022-08-01T17:29:07.933515Z","iopub.status.idle":"2022-08-01T17:29:08.214945Z","shell.execute_reply.started":"2022-08-01T17:29:07.933472Z","shell.execute_reply":"2022-08-01T17:29:08.213571Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"train_data.loc[train_data[\"tot_sent\"].idxmax(),\"sentence\"]","metadata":{"execution":{"iopub.status.busy":"2022-08-02T13:45:50.424759Z","iopub.execute_input":"2022-08-02T13:45:50.425172Z","iopub.status.idle":"2022-08-02T13:45:50.444123Z","shell.execute_reply.started":"2022-08-02T13:45:50.425141Z","shell.execute_reply":"2022-08-02T13:45:50.443155Z"},"trusted":true},"execution_count":46,"outputs":[{"execution_count":46,"output_type":"execute_result","data":{"text/plain":"'12 Powdermill Rd.'"},"metadata":{}}]},{"cell_type":"code","source":"def scoring(pred_summary, ref_summary):\n    r_scorer=rouge_scorer.RougeScorer([\"rouge1\",\"rouge2\",\"rougeL\"], use_stemmer=True)\n#     rougeL=r_scorer.score(ref_summary,pred_summary)\n    rouge=r_scorer.score(ref_summary,pred_summary)\n    score_ind={\"precision\":0, \"recall\":1, \"fmeasure\":2}\n    return rouge[\"rouge1\"][score_ind[\"fmeasure\"]], rouge[\"rouge2\"][score_ind[\"fmeasure\"]], rouge[\"rougeL\"][score_ind[\"fmeasure\"]]\n\ndef summarization(sentences):\n    return \".\".join(sentences)\n\ndef create_summary(df,s_num,th=0.19):\n    n_largest=df.groupby([\"text_id\"])[\"text_position\",\"sentence\",\"pred_rougeL\"].apply(lambda x: x.nlargest(s_num,columns=[\"pred_rougeL\"]).sort_index())\n    max_rL=max(n_largest[\"pred_rougeL\"])\n    sent = [ n_largest.loc[n_largest[\"pred_rougeL\"].idxmax(),\"sentence\"] ]\n    sent.extend( [ s for s,r in zip(n_largest[\"sentence\"].values, n_largest[\"pred_rougeL\"].values) if r > th and r!=max_rL] )\n    summary = summarization( sent)   \n    return summary\n\ndef get_ref_summary(ref_summaries,doc_id):\n    return ref_summaries.loc[ref_summaries.text_id==doc_id, \"summary\"].values[0]\n\ndef get_preds(model,docX):\n    return model.predict(docX)\n\ndef single_doc_summarizer(model,X,ref_summaries,th,s_num=4):\n    global doc_p_bar\n    docs=X.groupby(\"text_id\")\n    tot_r1=[]\n    tot_r2=[]\n    tot_rL=[]\n    for i,d in enumerate(docs):\n        doc_p_bar.update(1)\n#         if i>10:\n#             break\n        document=d[1]\n        cols=[\"len\", \"s_position\", \"thematic_ratio\", \"s_pos_par\", \"num_ratio\"]\n        feats=document[cols]\n\n        preds=get_preds(model,feats)\n        document[\"pred_rougeL\"]=preds.tolist()\n        pred_summary=create_summary(document,s_num,th)\n        ref_summary=get_ref_summary(ref_summaries,i)\n        r1,r2,rL = scoring(pred_summary,ref_summary)\n        tot_r1.append(r1)\n        tot_r2.append(r2)\n        tot_rL.append(rL)\n        \n        \n        del document\n        del feats\n        del preds\n        del pred_summary\n        del ref_summary\n        gc.collect()\n\n    return tot_r1, tot_r2, tot_rL\n\n\n\ndef doc_summary(model,document,ref_summaries,doc_id,th,s_num=4) :\n    global doc_p_bar, tot_r1, tot_r2, tot_rL\n    \n#         if i>10:\n#             break\n    cols=[\"len\", \"s_position\", \"thematic_ratio\", \"s_pos_par\", \"num_ratio\"]\n    feats=document[cols]\n\n\n    preds=get_preds(model,feats)\n    document[\"pred_rougeL\"]=preds.tolist()\n    pred_summary=create_summary(document,s_num,th)\n    ref_summary=get_ref_summary(ref_summaries,doc_id)\n    r1,r2,rL = scoring(pred_summary,ref_summary)\n    tot_r1.append(r1)\n    tot_r2.append(r2)\n    tot_rL.append(rL)\n    doc_p_bar.postfix[1] = np.mean(tot_r1)\n    doc_p_bar.postfix[3] = np.mean(tot_r2)\n    doc_p_bar.postfix[5] = np.mean(tot_rL)\n    doc_p_bar.postfix[6][\"value\"] = doc_id\n    doc_p_bar.update(1)\n\n    del document\n    del feats\n    del preds\n    del pred_summary\n    del ref_summary\n    gc.collect()\n    \n    return r1, r2, rL","metadata":{"execution":{"iopub.status.busy":"2022-08-02T13:49:33.945656Z","iopub.execute_input":"2022-08-02T13:49:33.946073Z","iopub.status.idle":"2022-08-02T13:49:33.966083Z","shell.execute_reply.started":"2022-08-02T13:49:33.946043Z","shell.execute_reply":"2022-08-02T13:49:33.965179Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"code","source":"# sc_type=\"rougeL\"\n# data_type=\"train1\"\n# train_data=pd.read_csv(os.path.join(df_dir,f\"{data_type}_data_{sc_type}_tid.csv\"))\n# # train_data[\"text_position\"] = train_data.groupby(\"text_id\").cumcount().add(1)\n# data_stats(train_data)\n# labels=train_data[\"rougeL\"].values\n\n# sc_type=\"rougeL\"\n# data_type=\"test\"\n# test_data=pd.read_csv(os.path.join(df_dir,f\"{data_type}_data_{sc_type}_tid.csv\"))\n# test_data[\"text_position\"] = test_data.groupby(\"text_id\").cumcount().add(1)\n# data_stats(test_data)\n# labels=test_data[\"rougeL\"].values","metadata":{"execution":{"iopub.status.busy":"2022-08-02T13:32:36.736218Z","iopub.status.idle":"2022-08-02T13:32:36.736959Z","shell.execute_reply.started":"2022-08-02T13:32:36.736739Z","shell.execute_reply":"2022-08-02T13:32:36.736762Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# summaries=pd.read_csv(\"/kaggle/input/summarizer-data/train_summaries.csv\")\n# train_len=train_data.groupby(\"text_id\").size().shape[0]\n\n# summaries=summaries.iloc[:train_len,:]\n# train_data_feats[\"summary\"]=summaries[\"summary\"]\n# train_data_feats[\"sentence\"]=train_data[\"sentence\"]\n# train_data_feats[\"text_id\"]=train_data[\"text_id\"]\n# train_data_feats[\"text_position\"]=train_data[\"text_position\"]\n\n# X=train_data_feats\n# y=pd.DataFrame(labels)\n\n\n# score_type=\"rougeL\"\n# train_data, train_data_feats, dev_data, dev_data_feats, test_data, test_data_feats = load_datasets(df_dir, sc_type=score_type)\n\nsc_type=\"rougeL\"\ndata_type=\"test\"\ntest_data=pd.read_csv(os.path.join(df_dir,f\"{data_type}_data_{sc_type}_tid.csv\"))\ntest_data[\"text_position\"] = test_data.groupby(\"text_id\").cumcount().add(1)\ndata_stats(test_data)\nlabels=test_data[\"rougeL\"].values\n\nsummaries=pd.read_csv(\"/kaggle/input/summarizer-data/test_summaries_grouped.csv\")\ntest_len=test_data.groupby(\"text_id\").size().shape[0]\nsummaries[\"text_id\"]=summaries.index\nif \"summary\" not in summaries.columns:\n    summaries[\"summary\"]=summaries[\"sum_clean\"]\n    del summaries[\"sum_clean\"]\n\n# summaries=summaries.iloc[:dev_len,:]\n# dev_data_feats[\"summary\"]=summaries[\"sum_clean\"]\ntest_data_feats[\"sentence\"]=test_data[\"sentence\"]\ntest_data_feats[\"text_id\"]=test_data[\"text_id\"]\ntest_data_feats[\"text_position\"]=test_data[\"text_position\"]\n\nX=test_data_feats\ny=pd.DataFrame(labels_test)","metadata":{"execution":{"iopub.status.busy":"2022-08-02T13:42:13.806918Z","iopub.execute_input":"2022-08-02T13:42:13.807389Z","iopub.status.idle":"2022-08-02T13:42:17.030855Z","shell.execute_reply.started":"2022-08-02T13:42:13.807356Z","shell.execute_reply":"2022-08-02T13:42:17.029469Z"},"trusted":true},"execution_count":38,"outputs":[{"name":"stdout","text":"            count      mean       std  min       25%       50%       75%  max\nchosen                                                                       \n0       1006718.0  0.109932  0.127422  0.0  0.051948  0.088889  0.128205  1.0\n1         29216.0  0.456340  0.299931  0.0  0.210740  0.389262  0.666667  1.0\n","output_type":"stream"}]},{"cell_type":"code","source":"tot_r1=[]\ntot_r2=[]\ntot_rL=[]\nglobal doc_p_bar\ndoc_p_bar=tqdm(total=summaries.shape[0], desc=\"Dev Set Summary Scoring\", bar_format=\"{postfix[0]}: {postfix[1]:.10f} | {postfix[2]}: {postfix[3]:.10f} | {postfix[4]}: {postfix[5]:.10f} ( {postfix[6][value]}/{postfix[7]} )\", postfix=[\"Mean Rouge1\", np.mean(tot_r1), \"Mean Rouge2\", np.mean(tot_r2), \"Mean RougeL\", np.mean(tot_rL), dict(value=0) ,summaries.shape[0]], leave=True)\n\nthreshold=0.3\nscores=X.groupby(\"text_id\").apply(lambda x: doc_summary(model,x,summaries,x.text_id.values[0],threshold) )\n\nprint(f\"\\nMean Rouge1: {np.mean(r1)}\\nMean Rouge2: {np.mean(r2)}\\nMean RougeL: {np.mean(rL)}\\n\\n\")","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-08-02T13:49:41.239666Z","iopub.execute_input":"2022-08-02T13:49:41.240298Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"Mean Rouge1: 0.0903019697 | Mean Rouge2: 0.0031850259 | Mean RougeL: 0.0671548350 ( 73/36081 )\nMean Rouge1: 0.0885600291 | Mean Rouge2: 0.0030330175 | Mean RougeL: 0.0665844330 ( 125/36081 )","output_type":"stream"}]},{"cell_type":"code","source":"X.groupby(\"text_id\").apply(lambda x: x.text_id.values[0] )","metadata":{"execution":{"iopub.status.busy":"2022-08-02T01:11:19.034151Z","iopub.execute_input":"2022-08-02T01:11:19.034596Z","iopub.status.idle":"2022-08-02T01:11:20.842815Z","shell.execute_reply.started":"2022-08-02T01:11:19.034551Z","shell.execute_reply":"2022-08-02T01:11:20.841557Z"},"trusted":true},"execution_count":152,"outputs":[{"execution_count":152,"output_type":"execute_result","data":{"text/plain":"text_id\n0            0\n1            1\n2            2\n3            3\n4            4\n         ...  \n36305    36305\n36306    36306\n36307    36307\n36308    36308\n36309    36309\nLength: 36310, dtype: int64"},"metadata":{}}]},{"cell_type":"markdown","source":"___\n___\n# **Notes**\n\n\n1. scoring -> label rougeL -> sentence feats -> train\n2. grid search ?! (or manual fine tuning)\n3. test -> input doc -> predict score -> keep N first sentences or keep those over a threshold -> create summary -> calculate rouge1/2/L\n\n___\n### **References**\n\n1. [Named Entity Recognition (NER) with TensorflowNamed Entity Recognition (NER) with Tensorflow](https://www.kaggle.com/code/naseralqaydeh/named-entity-recognition-ner-with-tensorflow)\n2. [Extractive Summarization using Deep LearningExtractive Summarization using Deep Learning](https://arxiv.org/pdf/1708.04439v1.pdf)\n3. [NLTK](https://www.bogotobogo.com/python/NLTK/Stemming_NLTK.php)\n4. [Text Features Library](https://github.com/pmbaumgartner/text-feat-lib/tree/master/notebooks)\n5. []()\n\n\n\n### **Feats**\n1. [Feature extraction](https://arxiv.org/pdf/1708.04439v1.pdf)\n    1. Number of thematic words\n    2. Sentence position\n    3. Sentence length\n    4. Sentence position relative to paragraph\n    5. Number of proper nouns\n    6. Number of numerals\n    7. Number of named entities\n    8. Term Frequency-Inverse Sentence Frequency\n    9. Sentence to Centroid similarity\n    \n    \n2. [Text Summarization References](https://github.com/Tian312/awesome-text-summarization/blob/master/README.md)\n\n\n\n___\n### **Feature Base**\n\nThe feature base model extracts the features of the sentence, then evaluate its importance. Here is the representative research.\nSentence Extraction Based Single Document Summarization\nFollowing features are used in the above method.\n\n1. Position of the sentence in the input document\n2. Presence of the verb in the sentence\n3. Length of the sentence\n4. Term frequency\n5. Named entity tag NE\n6. Font style\n\netc. All the features are accumulated as the score.\nThe No.of coreferences are the number of pronouns to the previous sentence. It is simply calculated by counting the pronouns occurred in the first half of the sentence. So the Score represents the reference to the previous sentence.\nNow we can evaluate each sentence. Next is selecting the sentence to avoid the duplicate of the information. In this paper, the same word between the new and selected sentence is considered. And the refinement to connect the selected sentences are executed.\nLuhns Algorithm is also feature base. It evaluates the significance of the word that is calculated from the frequency.\nYou can try feature base text summarization by TextTeaser (PyTeaser is available for Python user).","metadata":{}},{"cell_type":"markdown","source":"# Unused","metadata":{}},{"cell_type":"code","source":"# train_set = f\"..{os.sep}Data{os.sep}release{os.sep}train.jsonl\"\n# dev_set = f\"..{os.sep}Data{os.sep}release{os.sep}dev.jsonl\"\n# test_set = f\"..{os.sep}Data{os.sep}release{os.sep}test.jsonl\"\n# load json files and convert them to dataframes to load faster next time\n# train_df = funs.json_to_df(train_set,\"train\")\n# dev_df = funs.json_to_df(dev_set,\"dev\")\n# test_df = funs.json_to_df(test_set,\"test\")\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# colab command to download the dataset\n!kaggle datasets download -d tkylafi/summarizer-data","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}