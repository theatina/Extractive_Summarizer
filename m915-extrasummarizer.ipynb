{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from heapq import heapify\nimport os\nimport json\nfrom typing import Counter\nimport pandas as pd\nimport re\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pickle\n\nimport math\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer, PorterStemmer, SnowballStemmer, LancasterStemmer\nfrom nltk.tokenize import sent_tokenize, word_tokenize\nfrom nltk import pos_tag\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport sklearn.model_selection\nfrom sklearn.model_selection import RepeatedKFold, cross_val_score, GridSearchCV\nimport sklearn.preprocessing as preproc\nfrom sklearn.feature_extraction import text\nfrom sklearn.svm import LinearSVR\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression, ElasticNet, ElasticNetCV, LassoLars, SGDRegressor\nfrom sklearn.metrics import confusion_matrix, mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, MaxAbsScaler\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.svm import SVR\n\nfrom IPython.display import FileLink, FileLinks\n\nfrom tqdm import tqdm\n\n!pip install rouge-score\nfrom rouge_score import rouge_scorer\n\nimport psutil\nimport gc\nfrom collections import Counter","metadata":{"execution":{"iopub.status.busy":"2022-08-03T20:27:07.556235Z","iopub.execute_input":"2022-08-03T20:27:07.556618Z","iopub.status.idle":"2022-08-03T20:27:24.312229Z","shell.execute_reply.started":"2022-08-03T20:27:07.556586Z","shell.execute_reply":"2022-08-03T20:27:24.310461Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Collecting rouge-score\n  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: absl-py in /opt/conda/lib/python3.7/site-packages (from rouge-score) (1.1.0)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.7/site-packages (from rouge-score) (3.7)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from rouge-score) (1.21.6)\nRequirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.7/site-packages (from rouge-score) (1.16.0)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from nltk->rouge-score) (1.0.1)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from nltk->rouge-score) (4.64.0)\nRequirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from nltk->rouge-score) (8.0.4)\nRequirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.7/site-packages (from nltk->rouge-score) (2021.11.10)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from click->nltk->rouge-score) (4.12.0)\nRequirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->click->nltk->rouge-score) (4.1.1)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->click->nltk->rouge-score) (3.8.0)\nBuilding wheels for collected packages: rouge-score\n  Building wheel for rouge-score (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24955 sha256=01d17d9cdbbea467a6100053a709f287b5273da6f331dd25f672f03d392ed2f0\n  Stored in directory: /root/.cache/pip/wheels/84/ac/6b/38096e3c5bf1dc87911e3585875e21a3ac610348e740409c76\nSuccessfully built rouge-score\nInstalling collected packages: rouge-score\nSuccessfully installed rouge-score-0.1.2\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"\ndef json_to_df_oldv(json_path,data_type):\n    with open(json_path, \"r\", encoding=\"utf-8\") as f: \n        lines = [eval(l) for l in f.readlines()]\n\n    # exclude lines with surrogates in their text/summary\n    surr = [ i for i,l in enumerate(lines) for k in l.keys() if k in [\"text\",\"summary\"] and re.search(r'[\\uD800-\\uDFFF]', l[k])!=None ]\n\n    lines = [ l for i,l in zip( range(len(lines)),lines ) if i not in surr ]\n\n    cols=[ \"title\",\t\"date\",\t\"text\",\t\"summary\", \"compression\", \"coverage\", \"density\", \"compression_bin\", \"coverage_bin\"]\n\n    # we need only the extractive summaries as we are building an extractive summarizer\n    data=[ [ l[k] for k in l.keys() if k in cols ] for l in lines if l[\"density_bin\"]==\"extractive\" ]\n    df = pd.DataFrame(data,columns=cols)\n\n    df.to_csv(f\"..{os.sep}Data{os.sep}DataFrames{os.sep}{data_type}_set.csv\", header=True, index=False )\n\n    return df\n\ndef json_to_df(json_path,data_type):\n    data=[]\n    for ln in open(json_path,\"r\"):\n        obj = json.loads(ln)\n        data.append(obj)\n    df=pd.DataFrame(data)\n    \n    cols=[ \"title\",\t\"date\",\t\"text\",\t\"summary\", \"compression\", \"coverage\", \"density\", \"compression_bin\", \"coverage_bin\"]\n    df=df.loc[df.density_bin==\"extractive\"].reset_index()\n    \n    df.to_csv(f\"/kaggle/working/Data{os.sep}DataFrames{os.sep}{data_type}_set.csv\", header=True, index=False )\n    df[\"summary\"].to_csv(f\"/kaggle/working/Data{os.sep}DataFrames{os.sep}{data_type}_summaries.csv\", header=True, index=False )\n    \n    return df\n\n\n# text processing functions\n\n# A list of contractions from http://stackoverflow.com/questions/19790188/expanding-english-language-contractions-in-python\ncontractions = { \"ain't\": \"am not\", \"aren't\": \"are not\", \"can't\": \"cannot\", \"can't've\": \"cannot have\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \"couldn't've\": \"could not have\", \"didn't\": \"did not\", \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hadn't've\": \"had not have\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\", \"he'd've\": \"he would have\", \"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'll\": \"how will\", \"how's\": \"how is\", \"i'd\": \"i would\", \"i'll\": \"i will\", \"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'll\": \"it will\", \"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\", \"mightn't\": \"might not\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"needn't\": \"need not\", \"oughtn't\": \"ought not\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"she'd\": \"she would\", \"she'll\": \"she will\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"that'd\": \"that would\", \"that's\": \"that is\", \"there'd\": \"there had\", \"there's\": \"there is\", \"they'd\": \"they would\", \"they'll\": \"they will\", \"they're\": \"they are\", \"they've\": \"they have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'll\": \"we will\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what're\": \"what are\", \"what's\": \"what is\", \"what've\": \"what have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"who'll\": \"who will\", \"who's\": \"who is\", \"won't\": \"will not\", \"wouldn't\": \"would not\", \"you'd\": \"you would\", \"you'll\": \"you will\", \"you're\": \"you are\" }\n\n\ndef sentence_cleaning(text, remove_stopwords = True, sub_contractions=True, stemming=True):\n    global pbar_cleaning\n    pbar_cleaning.update(1)\n    \n    # Convert words to lower case\n    text = text.lower()\n    toks = word_tokenize(text)\n      \n    # Replace contractions with their longer forms \n    if sub_contractions:\n        new_text = []\n        for word in toks:\n            if word in contractions:\n                new_text.append(contractions[word])\n            else:\n                new_text.append(word)\n    \n    text = \" \".join(new_text)\n\n    # Format words and remove unwanted characters\n    text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n    text = re.sub(r'\\<a href', ' ', text)\n    text = re.sub(r'&amp;', '', text) \n    text = re.sub(r'[_\"\\-;%()|+&=*%.,!?:#$@\\[\\]/]', ' ', text)\n    text = re.sub(r'<br />', ' ', text)\n    text = re.sub(r'\\'', ' ', text)\n\n    toks_clean = word_tokenize(text)\n    \n    if remove_stopwords:\n        stops = set(stopwords.words(\"english\"))\n        toks_clean = [w for w in toks_clean if not w in stops]\n\n    \n    if stemming: \n        stemmer=SnowballStemmer(language=\"english\")\n        toks_clean=[ stemmer.stem(w) for w in toks_clean ]\n\n    text = \" \".join(toks_clean)\n    \n    return text, toks_clean\n\n\ndef rouge_scoring(sentence,summary,sc_type=\"rougeL\",score=\"fmeasure\"):\n    global pbar\n    pbar.update(1)\n    r_scorer=rouge_scorer.RougeScorer([\"rouge1\",\"rouge2\",\"rougeL\"])\n    r_L=r_scorer.score(summary,sentence)\n    score_ind={\"precision\":0, \"recall\":1, \"fmeasure\":2}\n    \n    return r_L[sc_type][score_ind[score]]\n\n\ndef text_processing(df,data_type,df_dir,sc_type=\"rougeL\"):\n    global pbar\n    cols=[\"sentence\", \"summary\", \"text\", \"text_id\"] \n    \n    df[\"summary\"].to_csv(os.path.join(df_dir,f\"{data_type}_data_{sc_type}_summaries_grouped.csv\"), header=True, index=False)\n#     print(f\"Summary df len: {df['summary'].shape}\")\n    # new_df=pd.DataFrame()\n\n    # sentence split \n    sentences=[ sent_tokenize(t) for t in df[\"text\"].values ]\n#     print(f\"Sentence Len: {len(sentences)}\")\n\n    summaries=df[\"summary\"].values\n    sent_sum_text=[ [ s,summary,t,i  ] for i, (s_list,summary,t) in enumerate(zip( sentences, summaries, df[\"text\"] )) for s in s_list ]\n    new_df=pd.DataFrame(sent_sum_text, columns=cols)\n#     new_df[\"text_id\"]=new_df[\"text\"].factorize()[0]\n    \n#     print(len(sent_sum_text))\n#     print(set(new_df[\"text_id\"].values), set(new_df[\"t_id\"].values))\n#     print(new_df[\"text_id\"].values[-1])\n    \n    new_df[\"chosen\"]= 0\n    ind = new_df[[ s in t for s,t in zip( new_df[\"sentence\"], new_df[\"summary\"] ) ]].index\n    new_df.loc[ind,\"chosen\"]=1\n    del new_df[\"text\"]\n    # for c in new_df.columns:\n    #   new_df[c]=new_df[c].astype(str)\n\n    # labels\n    # columns -> sentence: 0, summary: 1, text: 2\n    pbar = tqdm(total=new_df.shape[0] )\n    new_df[\"rougeL\"]= new_df.apply(lambda row: rouge_scoring(row[\"sentence\"],row[\"summary\"], sc_type=sc_type, score=\"fmeasure\" ), axis=1)\n#     print(new_df[\"rougeL\"])\n\n    new_df[\"summary\"].to_csv(os.path.join(df_dir,f\"{data_type}_data_{sc_type}_summaries.csv\"), header=True, index=False)\n    del new_df[\"summary\"]\n    \n    new_df.to_csv(os.path.join(df_dir,f\"{data_type}_data_{sc_type}.csv\"), header=True, index=False)\n\n    return new_df\n\n\ndef add_chosen_text_id(df):\n    cols=[\"sentence\", \"summary\", \"text\"] \n    # new_df=pd.DataFrame()\n\n    # sentence split \n    sentences=[ sent_tokenize(t) for t in df[\"text\"].values ]\n    summaries=df[\"summary\"].values\n    \n    sent_sum_text=[ [ s,summary,t  ] for s_list,summary,t in zip( sentences, summaries, df[\"text\"] ) for s in s_list  ]\n    new_df=pd.DataFrame(sent_sum_text, columns=cols)\n    \n    new_df[\"text_id\"]=new_df[\"text\"].factorize()[0]\n    new_df[\"chosen\"]= 0\n    ind = new_df[[ s in t for s,t in zip( new_df[\"sentence\"], new_df[\"summary\"] ) ]].index\n    new_df.loc[ind,\"chosen\"]=1\n    \n    del new_df[\"text\"]\n    del new_df[\"summary\"]\n    del new_df[\"sentence\"]\n    \n    return new_df\n\n\ndef text_proc_labels(data_dir,df_dir=\"/kaggle/input/summarizer-data/\"):\n#     train_df = pd.read_csv(os.path.join(df_dir,\"train_set.csv\"))\n    dev_df = pd.read_csv(os.path.join(df_dir,\"dev_set.csv\"))\n    test_df = pd.read_csv(os.path.join(df_dir,\"test_set.csv\"))\n\n#     train_df1, train_df2, train_df3, train_df4 = np.array_split(train_df, 4)\n    splits=4\n#     train_data_list=[]\n#     for i,train_df in enumerate(np.array_split(train_df, splits)):\n#         train_data = text_processing(train_df,f\"train{i+1}\",data_dir,\"rougeL\")\n#         train_data_list.append(train_data)\n    \n    test_data = text_processing(test_df,\"test\",data_dir,\"rougeL\")\n    dev_data = text_processing(dev_df,\"dev\",data_dir,\"rougeL\")\n    \n    \n    return train_data_list, dev_data, test_data\n\n\ndef df_add_tid(data_dir, df_dir):\n    sc_type=\"rougeL\"\n    data_labels_dir=\"/kaggle/input/summarizer-data\"\n\n#     data_type=\"train\"\n#     train_df = pd.read_csv(os.path.join(df_dir,f\"{data_type}_set.csv\"))\n#     splits=4\n#     for i,df in enumerate(np.array_split(train_df, 4)):\n#         if i==2:\n#             data_type=f\"train{i+1}\"\n#             print(data_type+\"\\n\")\n#             train_ch_tid_df=add_chosen_text_id(df)\n#             train_rougeL=pd.read_csv(os.path.join(data_labels_dir,f\"{data_type}_data_{sc_type}.csv\"))\n#             print(train_rougeL.columns, train_rougeL.shape)\n#             train_rougeL[\"text_id\"]=train_ch_tid_df[\"text_id\"]\n#             train_rougeL[\"chosen\"]=train_ch_tid_df[\"chosen\"]\n#             print(train_rougeL.columns, train_rougeL.shape)\n#             train_rougeL.to_csv(os.path.join(data_dir,f\"{data_type}_data_{sc_type}_tid.csv\"), header=True, index=False)\n\n    data_type=\"dev\" \n    dev_df = pd.read_csv(os.path.join(df_dir,f\"{data_type}_set.csv\"))\n    dev_ch_tid_df=add_chosen_text_id(dev_df)\n    dev_rougeL=pd.read_csv(os.path.join(data_labels_dir,f\"{data_type}_data_{sc_type}.csv\"))\n    print(dev_rougeL.columns, dev_rougeL.shape)\n    dev_rougeL[\"text_id\"]=dev_ch_tid_df[\"text_id\"]\n    dev_rougeL[\"chosen\"]=dev_ch_tid_df[\"chosen\"]\n    print(dev_rougeL.columns, dev_rougeL.shape)\n    dev_rougeL.to_csv(os.path.join(data_dir,f\"{data_type}_data_{sc_type}_tid.csv\"), header=True, index=False)\n\n    data_type=\"test\"\n    test_df = pd.read_csv(os.path.join(df_dir,f\"{data_type}_set.csv\"))\n    test_ch_tid_df=add_chosen_text_id(test_df)\n    test_rougeL=pd.read_csv(os.path.join(data_labels_dir,f\"{data_type}_data_{sc_type}.csv\"))\n    print(test_rougeL.columns, test_rougeL.shape)\n    test_rougeL[\"text_id\"]=test_ch_tid_df[\"text_id\"]\n    test_rougeL[\"chosen\"]=test_ch_tid_df[\"chosen\"]\n    print(test_rougeL.columns, test_rougeL.shape)\n    test_rougeL.to_csv(os.path.join(data_dir,f\"{data_type}_data_{sc_type}_tid.csv\"), header=True, index=False)\n\n    \ndef data_stats(df):\n    groups = df.groupby(\"chosen\")\n    print(f\"DataFrame shape: {df.shape}\\n\\n\")\n    print(groups.describe()[\"rougeL\"])\n\n    \ndef thematic_ratio(them_words, word_list):\n    them_occ= sum( [ word_list.count(w) for w in set(them_words)&set(word_list)])\n    them_ratio=them_occ/len(word_list)\n    return them_ratio\n\n\ndef s_position(t_position,tot_sent):\n    N=tot_sent\n    th=0.2*N\n    min_p= th*N\n    max_p= 2*th*N\n    \n    if t_position==tot_sent or t_position==1:\n        pos=1.0\n    else: \n        pos=math.cos((t_position - min_p)*((1/max_p) - min_p))\n        \n    return pos\n    \n    \ndef prop_nouns(tokens):\n    if type(tokens)!=list:\n        tokens=eval(tokens)\n    pos= nltk.pos_tag(tokens)\n    tags_count=Counter(tag for _, tag in pos if tag==\"NNP\" or tag==\"NNPS\")\n    return tags_count[\"NNP\"]+tags_count[\"NNPS\"] \n\n\ndef feature_df(df, data_dir, data_type):\n    global pbar_cleaning\n    feat_df=pd.DataFrame()\n    \n    # tokenize\n    pbar_cleaning=tqdm(total=train_data.shape[0], leave=True)\n    df[\"tokens\"] = df[\"sentence\"].apply(lambda x: sentence_cleaning(x)[1]) \n    \n\n    # create sentence features\n    # 1. thematic words\n    col = df.groupby(\"text_id\")[\"tokens\"].apply(sum)\n    thematic_cols= pd.DataFrame({\"text_id\": col.index, \"thematic\": [ [ t[0] for t in Counter(x).most_common(10) ] for x in col ]})  \n    df=df.join(thematic_cols[\"thematic\"], on='text_id' )\n    feat_df[\"thematic_ratio\"] = df.apply(lambda row: thematic_ratio(row.thematic, row.tokens) if len(row.tokens)>0 else 0.0, axis=1)\n\n    # 2. sentence position in the text\n    feat_df[\"text_position\"] = df.groupby(\"text_id\").cumcount().add(1)\n    df[\"tot_sent\"] = df.groupby(\"text_id\")[\"sentence\"].transform(len)\n    feat_df[\"s_position\"] = df.apply(lambda row: s_position(row.text_position,row.tot_sent), axis=1)\n\n    # 3. sentence length - threshold=3\n    threshold=3\n    feat_df[\"len\"]= df[\"tokens\"].apply(lambda x: 0 if len(x)<threshold else len(x))\n\n    # 4. sentence position - paragraph relative\n    feat_df['s_pos_par'] = feat_df[\"s_position\"].values\n    feat_df.loc[feat_df.s_pos_par!=1.0, 's_pos_par']=0.0\n\n    # 5. numerals\n    feat_df[\"num_ratio\"]=df[\"tokens\"].apply(lambda x: sum( [ 1 for t in x if t.isnumeric() ] )/len(x) if len(x)>0 else 0 )\n\n\n    # ?. Term Frequency-Inverse Sentence Frequency\n\n\n    # ?. proper nouns - not so useful\n    # train_data_feats[\"NNPs\"]=train_data_feats[\"tokens\"].apply(lambda x: prop_nouns(x) )\n\n\n\n\n    # train_data_feats=train_data[[\"len\",\"text_position\"]]\n    feat_df.to_csv(os.path.join(data_dir,f\"{data_type}_set_feats.csv\"), header=True, index=False)\n\n    return feat_df\n\n\n\n# SVM_scaler =  StandardScaler()\n# LR_scaler =  MinMaxScaler()\n# KNN_scaler =  StandardScaler()\n# # classifier parameters\n# KNN_n_num = 9\n# LR_C = 1.0\n# SVM_C = 0.001\n# algos={\n#   \"ElNet\": make_pipeline(SVM_scaler, SVC(C=SVM_C)),\n#   \"LR\":  make_pipeline(LR_scaler, LogisticRegression(C=LR_C)),\n#   \"KNN\": make_pipeline(KNN_scaler, KNeighborsClassifier(n_neighbors = KNN_n_num)),\n# }\n\n\ndef classifier_training(model,X_train,y_train):\n    model.fit(X_train,y_train)\n    preds=model.predict(X_train)\n    c_rep=classification_report(y_train,preds)\n    c_rep_dict=classification_report(y_train,preds,output_dict=True)\n    return model, c_rep, c_rep_dict\n\ndef classifier_validation(model,X_dev,y_dev):\n    preds=model.predict(X_dev)\n    c_rep = classification_report(y_dev,preds)\n    c_rep_dict=classification_report(y_dev,preds,output_dict=True)\n    return c_rep, c_rep_dict\n\ndef classifier_test(model,X_test,y_test):\n    preds=model.predict(X_test)\n    c_rep = classification_report(y_test,preds)\n    c_rep_dict=classification_report(y_test,preds,output_dict=True)\n    return c_rep, c_rep_dict\n\n\n# Training - Validation - Test pipeline\ndef classifier_T_V_T(X_train, y_train, X_dev, y_dev, X_test, y_test, algo_type=\"LR\"):\n    model=algos[algo_type]\n\n    model,c_rep_train,c_rep_dict_train=classifier_training(model,X_train,y_train)\n    c_rep_dev,c_rep_dict_dev=classifier_validation(model,X_dev,y_dev)\n    c_rep_test,c_rep_dict_test=classifier_validation(model,X_test,y_test)\n\n    return model, c_rep_train, c_rep_dict_train, c_rep_dev, c_rep_dict_dev, c_rep_test, c_rep_dict_test\n\n","metadata":{"execution":{"iopub.status.busy":"2022-08-03T20:46:39.007129Z","iopub.execute_input":"2022-08-03T20:46:39.008935Z","iopub.status.idle":"2022-08-03T20:46:39.151432Z","shell.execute_reply.started":"2022-08-03T20:46:39.008681Z","shell.execute_reply":"2022-08-03T20:46:39.141640Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"# memory clean\n# for v in globals():\n#     print(v)\n#     if str(v) not \"__name__\":\n#         del v\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-08-03T20:27:24.387119Z","iopub.execute_input":"2022-08-03T20:27:24.388196Z","iopub.status.idle":"2022-08-03T20:27:24.556020Z","shell.execute_reply.started":"2022-08-03T20:27:24.388150Z","shell.execute_reply":"2022-08-03T20:27:24.555185Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"23"},"metadata":{}}]},{"cell_type":"code","source":"df_dir=\"/kaggle/input/summarizer-data\"\n\ndata_dir= \"/kaggle/working/Data/DataFrames\"\nif not os.path.exists(data_dir):\n    os.makedirs(data_dir)\n    \n# test_json_path=\"/kaggle/input/summarizer-data/test.jsonl\"\n# json_to_df(test_json_path,\"test\")\n\n# dev_json_path=\"/kaggle/input/summarizer-data/dev.jsonl\"\n# json_to_df(dev_json_path,\"dev\")\n\n# load saved dataframes and create labels\n# train_data_list, dev_data, test_data = text_proc_labels(data_dir)\n\n# load dataframes with labels\n# df_add_tid(data_dir, df_dir)\n\n\nFileLinks(\".\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-output":false,"scrolled":true,"execution":{"iopub.status.busy":"2022-08-03T20:27:24.557981Z","iopub.execute_input":"2022-08-03T20:27:24.558915Z","iopub.status.idle":"2022-08-03T20:27:24.573702Z","shell.execute_reply.started":"2022-08-03T20:27:24.558875Z","shell.execute_reply":"2022-08-03T20:27:24.572611Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"./\n  __notebook_source__.ipynb","text/html":"./<br>\n&nbsp;&nbsp;<a href='./__notebook_source__.ipynb' target='_blank'>__notebook_source__.ipynb</a><br>"},"metadata":{}}]},{"cell_type":"code","source":"# # load training dataset\n# sc_type=\"rougeL\"\n# data_type=\"train1\"\n# train_data=pd.read_csv(os.path.join(df_dir,f\"{data_type}_data_{sc_type}_tid.csv\"))\n# data_stats(train_data)\n# labels=train_data[\"rougeL\"].values\n# del train_data[\"rougeL\"]\n\n\n# sc_type=\"rougeL\"\n# data_type=\"dev\"\n# dev_data=pd.read_csv(os.path.join(df_dir,f\"{data_type}_data_{sc_type}_tid.csv\"))\n# data_stats(dev_data)\n# labels_dev=dev_data[\"rougeL\"].values\n# del dev_data[\"rougeL\"]\n\n\n# sc_type=\"rougeL\"\n# data_type=\"test\"\n# test_data=pd.read_csv(os.path.join(df_dir,f\"{data_type}_data_{sc_type}_tid.csv\"))\n# data_stats(test_data)\n# labels_test=test_data[\"rougeL\"].values\n# del test_data[\"rougeL\"]","metadata":{"execution":{"iopub.status.busy":"2022-08-03T20:27:24.575396Z","iopub.execute_input":"2022-08-03T20:27:24.576320Z","iopub.status.idle":"2022-08-03T20:27:24.581716Z","shell.execute_reply.started":"2022-08-03T20:27:24.576272Z","shell.execute_reply":"2022-08-03T20:27:24.580800Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"def load_datasets(df_dir, sc_type=\"rougeL\"):\n    # train_data_feats=pd.read_csv(\"/kaggle/input/summarizer-data/train1_set_feats.csv\")\n    data_type=\"train1\"\n    print(f\"\\nLoading {data_type.capitalize()} Data . . \")\n    # train_data=feature_df(train_data, data_dir, data_type)\n    train_data=pd.read_csv(f\"/kaggle/input/summarizer-data/{data_type}_set_feats.csv\") \n    \n    train_data_rougeLTid=pd.read_csv(os.path.join(df_dir,f\"{data_type}_data_{sc_type}_tid.csv\"))\n    train_data_rougeLTid[\"text_position\"] = train_data_rougeLTid.groupby(\"text_id\").cumcount().add(1)\n    \n    data_stats(train_data_rougeLTid)\n    labels_train=train_data_rougeLTid[\"rougeL\"].values\n#     del train_data[\"rougeL\"]\n    \n    train_data_feats=train_data\n    train_data[\"sentence\"]=train_data_rougeLTid[\"sentence\"]\n\n    train_data_feats=train_data_feats[[\"len\", \"s_position\", \"thematic_ratio\", \"s_pos_par\", \"num_ratio\"]]\n\n    # lens= StandardScaler().fit_transform(np.array(train_data_feats[\"len\"]).reshape(-1,1) )\n    # train_data_feats[\"len\"]=lens\n    # train_data_feats\n\n\n    data_type=\"dev\"\n    print(f\"\\n\\nLoading {data_type.capitalize()} Data . . \")\n    # dev_data=feature_df(dev_data, data_dir, data_type)\n    dev_data=pd.read_csv(f\"/kaggle/input/summarizer-data/{data_type}_set_feats.csv\") \n    \n    dev_data_rougeLTid=pd.read_csv(os.path.join(df_dir,f\"{data_type}_data_{sc_type}.csv\"))\n    dev_data_rougeLTid[\"text_position\"] = dev_data_rougeLTid.groupby(\"text_id\").cumcount().add(1)\n    \n    data_stats(dev_data_rougeLTid)\n    labels_dev=dev_data_rougeLTid[\"rougeL\"].values\n    \n    dev_data_feats=dev_data\n    dev_data[\"sentence\"]=dev_data_rougeLTid[\"sentence\"]\n    dev_data_feats=dev_data_feats[[\"len\", \"s_position\", \"thematic_ratio\", \"s_pos_par\", \"num_ratio\"]]\n\n\n    data_type=\"test\"\n    print(f\"\\n\\nLoading {data_type.capitalize()} Data . . \")\n    # test_data=feature_df(test_data, data_dir, data_type)\n    test_data=pd.read_csv(f\"/kaggle/input/summarizer-data/{data_type}_set_feats.csv\") \n    \n    test_data_rougeLTid=pd.read_csv(os.path.join(df_dir,f\"{data_type}_data_{sc_type}.csv\"))\n    test_data_rougeLTid[\"text_position\"] = test_data_rougeLTid.groupby(\"text_id\").cumcount().add(1)\n    \n    data_stats(test_data_rougeLTid)\n    labels_test=test_data_rougeLTid[\"rougeL\"].values\n    \n    test_data_feats=test_data\n    test_data[\"sentence\"]=test_data_rougeLTid[\"sentence\"]\n    test_data_feats=test_data_feats[[\"len\", \"s_position\", \"thematic_ratio\", \"s_pos_par\", \"num_ratio\"]]\n    \n    return train_data_rougeLTid, train_data_feats, labels_train, dev_data_rougeLTid, dev_data_feats, labels_dev, test_data_rougeLTid, test_data_feats, labels_test\n\n\ndef load_dataset_test(df_dir, sc_type=\"rougeL\"):\n    \n    data_type=\"test\"\n    print(f\"\\nLoading {data_type.capitalize()} Data . . \")\n    # test_data=feature_df(test_data, data_dir, data_type)\n    test_data=pd.read_csv(f\"/kaggle/input/summarizer-data/{data_type}_set_feats.csv\") \n    \n\n    test_data_rougeLTid=pd.read_csv(os.path.join(df_dir,f\"{data_type}_data_{sc_type}.csv\"))\n    test_data_rougeLTid[\"text_position\"] = test_data_rougeLTid.groupby(\"text_id\").cumcount().add(1)\n                                                 \n    data_stats(test_data_rougeLTid)\n    labels_test=test_data_rougeLTid[\"rougeL\"].values\n    \n    test_data_feats=test_data\n    test_data[\"sentence\"]=test_data_rougeLTid[\"sentence\"]\n    test_data_feats=test_data_feats[[\"len\", \"s_position\", \"thematic_ratio\", \"s_pos_par\", \"num_ratio\"]]\n    \n    return test_data_rougeLTid, test_data_feats, labels_test\n","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-08-03T21:47:13.371295Z","iopub.execute_input":"2022-08-03T21:47:13.372362Z","iopub.status.idle":"2022-08-03T21:47:13.389179Z","shell.execute_reply.started":"2022-08-03T21:47:13.372306Z","shell.execute_reply":"2022-08-03T21:47:13.388081Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"code","source":"score_type=\"rougeL\"\ntrain_data, train_data_feats, labels, dev_data, dev_data_feats, labels_dev, test_data, test_data_feats, labels_test = load_datasets(df_dir, sc_type=score_type)\n\nFileLinks(\".\")","metadata":{"execution":{"iopub.status.busy":"2022-08-03T21:46:26.963206Z","iopub.execute_input":"2022-08-03T21:46:26.965695Z","iopub.status.idle":"2022-08-03T21:46:57.024339Z","shell.execute_reply.started":"2022-08-03T21:46:26.965607Z","shell.execute_reply":"2022-08-03T21:46:57.023170Z"},"trusted":true},"execution_count":51,"outputs":[{"name":"stdout","text":"\nLoading Train1 Data . . \nDataFrame shape: (2437027, 5)\n\n\n            count      mean       std  min       25%       50%       75%  max\nchosen                                                                       \n0       2310412.0  0.105404  0.120294  0.0  0.050000  0.083333  0.122449  1.0\n1        126615.0  0.349349  0.234293  0.0  0.177295  0.306569  0.461538  1.0\n\nLoading Dev Data . . \nDataFrame shape: (1048562, 5)\n\n\n            count      mean       std  min       25%       50%       75%  max\nchosen                                                                       \n0       1019122.0  0.109952  0.127384  0.0  0.051948  0.088889  0.128205  1.0\n1         29440.0  0.457427  0.297148  0.0  0.216498  0.392638  0.666667  1.0\n\nLoading Test Data . . \nDataFrame shape: (1037722, 5)\n\n\n            count      mean       std  min       25%       50%       75%  max\nchosen                                                                       \n0       1008495.0  0.109895  0.127409  0.0  0.051948  0.088889  0.128205  1.0\n1         29227.0  0.456524  0.300030  0.0  0.210927  0.389610  0.666667  1.0\n","output_type":"stream"},{"execution_count":51,"output_type":"execute_result","data":{"text/plain":"./\n  __notebook_source__.ipynb\n./Data/\n  test_set_feats.csv\n  SGD_model.pkl","text/html":"./<br>\n&nbsp;&nbsp;<a href='./__notebook_source__.ipynb' target='_blank'>__notebook_source__.ipynb</a><br>\n./Data/<br>\n&nbsp;&nbsp;<a href='./Data/test_set_feats.csv' target='_blank'>test_set_feats.csv</a><br>\n&nbsp;&nbsp;<a href='./Data/SGD_model.pkl' target='_blank'>SGD_model.pkl</a><br>"},"metadata":{}}]},{"cell_type":"code","source":"# feature_df(test_data,data_dir,\"test\")\nfeature_df(dev_data,data_dir,\"dev\")\nfeature_df(train_data,data_dir,\"train1\")","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # tokenize\n# global pbar_cleaning\n# pbar_cleaning=tqdm(total=train_data.shape[0], leave=True)\n# train_data[\"tokens\"] = train_data[\"sentence\"].apply(lambda x: sentence_cleaning(x)[1]) \n\n# train_data_feats=pd.DataFrame()\n# # create sentence features\n# # 1. thematic words\n# col = train_data.groupby(\"text_id\").tokens.apply(sum)\n# thematic_cols= pd.DataFrame({\"text_id\": col.index, \"thematic\": [ [ t[0] for t in Counter(x).most_common(10) ] for x in col ]})  \n# train_data=train_data.join(thematic_cols[\"thematic\"], on='text_id' )\n# train_data_feats[\"thematic_ratio\"] = train_data.apply(lambda row: thematic_ratio(row.thematic, row.tokens) if len(row.tokens)>0 else 0.0, axis=1)\n\n# # 2. sentence position in the text\n# train_data[\"text_position\"] = train_data.groupby(\"text_id\").cumcount().add(1)\n# train_data[\"tot_sent\"] = train_data.groupby(\"text_id\")[\"sentence\"].transform(len)\n# train_data_feats[\"s_position\"] = train_data.apply(lambda row: s_position(row.text_position,row.tot_sent), axis=1)\n\n# # 3. sentence length - threshold=3\n# threshold=3\n# train_data_feats[\"len\"]= train_data[\"tokens\"].apply(lambda x: 0 if len(x)<threshold else len(x))\n\n# # 4. sentence position - paragraph relative\n# train_data_feats['s_pos_par'] = train_data_feats[\"s_position\"].values\n# train_data_feats.loc[train_data_feats.s_pos_par!=1.0, 's_pos_par']=0.0\n\n# # 5. numerals\n# train_data_feats[\"num_ratio\"]=train_data_feats[\"tokens\"].apply(lambda x: sum( [ 1 for t in eval(x) if t.isnumeric() ] )/len(x) )\n\n\n# # ?. Term Frequency-Inverse Sentence Frequency\n\n\n# # ?. proper nouns - not so useful\n# # train_data_feats[\"NNPs\"]=train_data_feats[\"tokens\"].apply(lambda x: prop_nouns(x) )\n\n\n\n\n# # train_data_feats=train_data[[\"len\",\"text_position\"]]\n# train_data_feats.to_csv(os.path.join(data_dir,f\"{data_type}_set_feats.csv\"), header=True, index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train_data_feats=train_data[[\"len\",\"text_position\",\"thematic_words\", ]]\n# data_type=\"train1\"\n# train_data.to_csv(os.path.join(data_dir,f\"{data_type}_set_feats.csv\"), header=True, index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# FileLinks(\".\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sc_type=\"rougeL\"\n# data_type=\"train1\"\n# train_data=pd.read_csv(os.path.join(df_dir,f\"{data_type}_data_{sc_type}_tid.csv\"))\n# data_stats(train_data)\n# labels=train_data[\"rougeL\"].values\n\n# sc_type=\"rougeL\"\n# data_type=\"dev\"\n# dev_data=pd.read_csv(os.path.join(df_dir,f\"{data_type}_data_{sc_type}_tid.csv\"))\n# data_stats(dev_data)\n# labels=dev_data[\"rougeL\"].values","metadata":{"execution":{"iopub.status.busy":"2022-08-02T00:11:56.581636Z","iopub.execute_input":"2022-08-02T00:11:56.582151Z","iopub.status.idle":"2022-08-02T00:12:13.593052Z","shell.execute_reply.started":"2022-08-02T00:11:56.582104Z","shell.execute_reply":"2022-08-02T00:12:13.591998Z"},"trusted":true},"execution_count":59,"outputs":[{"name":"stdout","text":"            count      mean       std  min       25%       50%       75%  max\nchosen                                                                       \n0       2310412.0  0.105404  0.120294  0.0  0.050000  0.083333  0.122449  1.0\n1        126615.0  0.349349  0.234293  0.0  0.177295  0.306569  0.461538  1.0\n            count      mean       std  min       25%       50%       75%  max\nchosen                                                                       \n0       1019122.0  0.109952  0.127384  0.0  0.051948  0.088889  0.128205  1.0\n1         29440.0  0.457427  0.297148  0.0  0.216498  0.392638  0.666667  1.0\n","output_type":"stream"}]},{"cell_type":"code","source":"# test_num=1000\n\n# model = make_pipeline(MinMaxScaler(), ElasticNet(alpha=5e-3, warm_start=True, random_state=99, fit_intercept=True))\n# model.fit(train_data_feats, labels)\n\n# cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n# # evaluate model\n# scores_mae = cross_val_score(model, train_data_feats.iloc[:test_num,:] , labels[:test_num], scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1)\n# scores_mse = cross_val_score(model, train_data_feats.iloc[:test_num,:] , labels[:test_num], scoring='neg_mean_squared_error', cv=cv, n_jobs=-1)\n# # scores_r2 = cross_val_score(model, train_data_feats.iloc[:test_num,:] , labels[:test_num], scoring='r2', cv=cv, n_jobs=-1)\n\n# # force scores to be positive\n# scores_mae = np.absolute(scores_mae)\n# scores_mse = np.absolute(scores_mse)\n# # scores_r2 = np.absolute(scores_r2)\n# print('Mean MAE: %.3f (%.3f)' % (np.mean(scores_mae), np.std(scores_mae)))\n# print('Mean MSE: %.3f (%.3f)' % (np.mean(scores_mse), np.std(scores_mse)))\n# # print('Mean R2: %.3f (%.3f)' % (np.mean(scores_r2), np.std(scores_r2)))\n\n# print(labels)\n# print(model.predict(train_data_feats))\n# # svr_results(labels[:test_num],  train_data_feats.iloc[:test_num,:], model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# test_num=10000\n\n# # model = make_pipeline(MinMaxScaler(), ElasticNetCV(random_state=99))\n# model =   make_pipeline(StandardScaler(),ElasticNet(alpha=5e-6, random_state=99, fit_intercept=True))\n# model.fit(train_data_feats, labels)\n\n# cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n# # evaluate model\n# scores_mae = cross_val_score(model, train_data_feats.iloc[:test_num,:] , labels[:test_num], scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1)\n# scores_mse = cross_val_score(model, train_data_feats.iloc[:test_num,:] , labels[:test_num], scoring='neg_mean_squared_error', cv=cv, n_jobs=-1)\n# # scores_r2 = cross_val_score(model, train_data_feats.iloc[:test_num,:] , labels[:test_num], scoring='r2', cv=cv, n_jobs=-1)\n\n# # force scores to be positive\n# scores_mae = np.absolute(scores_mae)\n# scores_mse = np.absolute(scores_mse)\n# # scores_r2 = np.absolute(scores_r2)\n# print('Mean MAE: %.3f (%.3f)' % (np.mean(scores_mae), np.std(scores_mae)))\n# print('Mean MSE: %.3f (%.3f)' % (np.mean(scores_mse), np.std(scores_mse)))\n# # print('Mean R2: %.3f (%.3f)' % (np.mean(scores_r2), np.std(scores_r2)))\n\n# print(labels)\n# print(model.predict(train_data_feats))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# test_num=1000\n\n# # model = make_pipeline(MinMaxScaler(), ElasticNetCV(random_state=99))\n# eps=0\n# model = make_pipeline(StandardScaler(), LinearSVR(epsilon=eps, C=5e-1, fit_intercept=True, intercept_scaling=1.0, loss=\"squared_epsilon_insensitive\"))\n# # model = LinearSVR(epsilon=eps, C=5e-4, fit_intercept=True)\n# model.fit(train_data_feats, labels)\n\n# cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n# # evaluate model\n# scores_mae = cross_val_score(model, train_data_feats.iloc[:test_num,:] , labels[:test_num], scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1)\n# scores_mse = cross_val_score(model, train_data_feats.iloc[:test_num,:] , labels[:test_num], scoring='neg_mean_squared_error', cv=cv, n_jobs=-1)\n# # scores_r2 = cross_val_score(model, train_data_feats.iloc[:test_num,:] , labels[:test_num], scoring='r2', cv=cv, n_jobs=-1)\n\n# # force scores to be positive\n# scores_mae = np.absolute(scores_mae)\n# scores_mse = np.absolute(scores_mse)\n# # scores_r2 = np.absolute(scores_r2)\n# print('Mean MAE: %.3f (%.3f)' % (np.mean(scores_mae), np.std(scores_mae)))\n# print('Mean MSE: %.3f (%.3f)' % (np.mean(scores_mse), np.std(scores_mse)))\n# # print('Mean R2: %.3f (%.3f)' % (np.mean(scores_r2), np.std(scores_r2)))\n\n# print(labels)\n# print(model.predict(train_data_feats))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# grid search - Dev Set Tuning\ndef estimator_tuning(X, y, estimator=SGDRegressor(), scaler=StandardScaler()):\n    model_grid = make_pipeline(scaler, estimator )\n\n    param_grid = {\n        \"sgdregressor__alpha\": [8.192e-10],\n        \"sgdregressor__tol\": [6.4e-5],\n        \"sgdregressor__epsilon\": [3.2e-4],\n        \"sgdregressor__loss\": [\"squared_error\"],\n        \"sgdregressor__penalty\": [\"elasticnet\"],\n        \"sgdregressor__learning_rate\": [\"adaptive\"]\n    }\n    # \"sgdregressor__alpha\": 5.0 ** -np.arange(2, 7)\n    # \"sgdregressor__tol\": 10.0 ** -np.arange(2, 7)\n    # \"sgdregressor__loss\": [\"squared_error\", \"huber\", \"epsilon_insensitive\"]\n    # sgdregressor__penalty\": [\"l2\", \"l1\", \"elasticnet\"]\n    # \"sgdregressor__learning_rate\": [\"constant\", \"optimal\", \"invscaling\", \"adaptive\"]\n\n    g_search = GridSearchCV(model_grid, param_grid, verbose=9, return_train_score=True, cv=2)\n    g_search.fit(X, y)\n    \n    with open(os.path.join(\"/kaggle/working/Data\",f\"{estimator}_grid_search_results.txt\"), \"w\", encoding=\"utf-8\" ) as writer:\n          writer.write(f\"Best ParametersL:\\n{g_search.best_params_}\\n\\n\\n{g_search.cv_results_}\")\n\n    print(f\"Best score: { g_search.best_score_}\\nParams: {g_search.best_params_}\")\n    return g_search.best_estimator_\n\n\nbest_model=estimator_tuning(dev_data_feats,labels_dev)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-08-02T14:41:33.573323Z","iopub.status.idle":"2022-08-02T14:41:33.574259Z","shell.execute_reply.started":"2022-08-02T14:41:33.573972Z","shell.execute_reply":"2022-08-02T14:41:33.574000Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Best sofar","metadata":{}},{"cell_type":"code","source":"def estimator_training(train_data_feats, labels, test_data_feats, labels_test):\n    model = make_pipeline(StandardScaler(), SGDRegressor(alpha=8.192e-10, max_iter=1000, tol=6.4e-5, epsilon=3.2e-4, learning_rate=\"adaptive\", loss=\"squared_error\", penalty=\"elasticnet\"))\n    # model = LinearSVR(epsilon=eps, C=5e-4, fit_intercept=True)\n    model.fit(train_data_feats, labels)\n\n    cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n    \n    # evaluate model\n    test_samples=1000\n    # MAE\n    scores_mae = cross_val_score(model, test_data_feats.iloc[:test_samples,:] , labels_test[:test_samples], scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1)\n    scores_mae = np.absolute(scores_mae)\n    print('Mean MAE: %.3f (%.3f)' % (np.mean(scores_mae), np.std(scores_mae)))\n    # MSE\n    scores_mse = cross_val_score(model, test_data_feats.iloc[:test_samples,:] , labels_test[:test_samples], scoring='neg_mean_squared_error', cv=cv, n_jobs=-1)\n    scores_mse = np.absolute(scores_mse)\n    print('Mean MSE: %.3f (%.3f)' % (np.mean(scores_mse), np.std(scores_mse)))\n    # R2    \n    scores_r2 = cross_val_score(model, test_data_feats.iloc[:test_samples,:] , labels_test[:test_samples], scoring='r2', cv=cv, n_jobs=-1)\n#     scores_r2 = np.absolute(scores_r2)\n    print('Mean R2: %.3f (%.3f)' % (np.mean(scores_r2), np.std(scores_r2)))\n\n#     print(labels_test)\n#     print(model.predict(test_data_feats))\n    \n    return model\n\n\nmodel=estimator_training(train_data_feats, labels, test_data_feats, labels_test)\n\npkl_filepath=os.path.join(\"/kaggle/working/Data\",\"SGD_model.pkl\")\nwith open(pkl_filepath,\"wb\") as model_writer:\n    pickle.dump(model, model_writer)","metadata":{"execution":{"iopub.status.busy":"2022-08-03T20:28:33.338105Z","iopub.execute_input":"2022-08-03T20:28:33.338881Z","iopub.status.idle":"2022-08-03T20:28:58.375316Z","shell.execute_reply.started":"2022-08-03T20:28:33.338839Z","shell.execute_reply":"2022-08-03T20:28:58.374104Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"Mean MAE: 0.069 (0.007)\nMean MSE: 0.016 (0.005)\nMean R2: 0.156 (0.158)\n","output_type":"stream"}]},{"cell_type":"code","source":"# preds_df=pd.DataFrame({\"text_id\": train_data[\"text_id\"].values, \"text_position\": train_data[\"text_position\"].values, \"sentence\": train_data[\"sentence\"].values, \"pred_rougeL\": model.predict(train_data_feats)})\n\n# n_largest=preds_df.groupby([\"text_id\"])[\"text_position\",\"pred_rougeL\"].apply(lambda x: x.nlargest(3,columns=[\"pred_rougeL\"]).sort_index())","metadata":{"execution":{"iopub.status.busy":"2022-08-01T17:29:07.932684Z","iopub.execute_input":"2022-08-01T17:29:07.933515Z","iopub.status.idle":"2022-08-01T17:29:08.214945Z","shell.execute_reply.started":"2022-08-01T17:29:07.933472Z","shell.execute_reply":"2022-08-01T17:29:08.213571Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"data_dir=\"/kaggle/working/Data\"\ndef scoring(pred_summary, ref_summary,doc_id):\n    pred_summary = re.sub(r'\\n+', '', pred_summary)\n    \n    score_ind={\"precision\":0, \"recall\":1, \"fmeasure\":2}\n    r_scorer=rouge_scorer.RougeScorer([\"rouge1\",\"rouge2\",\"rougeL\"], use_stemmer=True)\n\n    rouge=r_scorer.score(ref_summary,pred_summary)\n    \n    r1=rouge[\"rouge1\"][score_ind[\"fmeasure\"]]\n    r2=rouge[\"rouge2\"][score_ind[\"fmeasure\"]]\n    rL=rouge[\"rougeL\"][score_ind[\"fmeasure\"]]\n    \n    with open(os.path.join(data_dir,\"summaResults.txt\"), \"a\", encoding=\"utf-8\") as writer:\n        file_str=f\"> Document #{doc_id}\\n\\nPredicted Summary\\n{pred_summary}\\n\\nReference Summary\\n{ref_summary}\\n\\nR1: {r1:.9f} | R2: {r2:.9f} | RL: {rL:.9f}\\n\\n\\n\\n\"\n        writer.write(file_str)\n        \n    return r1,r2,rL\n\n\ndef summarization(sentences):\n    sentences = [ str(s) for s in sentences if len(s)>0 ]\n    if len(sentences)==0:\n        return \" \"\n    \n    summary = \".\".join(sentences)\n    if summary[-1] is not \".\":\n        summary+=\".\"\n    \n    return summary\n\n\ndef create_summary(df,s_num,th=0.19):\n    n_largest=df.groupby([\"text_id\"])[\"text_position\",\"sentence\",\"pred_rougeL\"].apply(lambda x: x.nlargest(s_num,columns=[\"pred_rougeL\"]).sort_index())\n    max_rL=max(n_largest[\"pred_rougeL\"])\n    sent = [ n_largest.loc[n_largest[\"pred_rougeL\"].idxmax(),\"sentence\"] ]\n    sent.extend( [ s for s,r in zip(n_largest[\"sentence\"].values, n_largest[\"pred_rougeL\"].values) if r > th and r!=max_rL] )\n    summary = summarization( sent)   \n    return summary\n\n\ndef get_ref_summary(ref_summaries,doc_id):\n    ref_sum=ref_summaries.loc[ref_summaries.text_id==doc_id, \"summary\"].values\n    \n    if type(ref_sum)!=str:\n        ref_sum=ref_sum[0]\n#     if type(ref_sum)!=str:\n#         ref_sum=\" \"\n    return ref_sum\n\n\ndef get_preds(model,docX):\n    return model.predict(docX)\n\n\ndef doc_summary(model,document,ref_summaries,doc_id,th,s_num=4) :\n    global doc_p_bar, tot_r1, tot_r2, tot_rL\n    \n#         if i>10:\n#             break\n    cols=[\"len\", \"s_position\", \"thematic_ratio\", \"s_pos_par\", \"num_ratio\"]\n    feats=document[cols]\n\n\n    preds=get_preds(model,feats)\n    document[\"pred_rougeL\"]=preds.tolist()\n    pred_summary=create_summary(document,s_num,th)\n    ref_summary=get_ref_summary(ref_summaries,doc_id)\n    r1,r2,rL = scoring(pred_summary,ref_summary,doc_id)\n    tot_r1.append(r1)\n    tot_r2.append(r2)\n    tot_rL.append(rL)\n    doc_p_bar.postfix[1] = np.mean(tot_r1)\n    doc_p_bar.postfix[3] = np.mean(tot_r2)\n    doc_p_bar.postfix[5] = np.mean(tot_rL)\n    doc_p_bar.postfix[6][\"value\"] = doc_id\n    doc_p_bar.update(1)\n\n    del document\n    del feats\n    del preds\n    del pred_summary\n    del ref_summary\n    gc.collect()\n    \n    return r1, r2, rL","metadata":{"execution":{"iopub.status.busy":"2022-08-03T20:28:58.377266Z","iopub.execute_input":"2022-08-03T20:28:58.377829Z","iopub.status.idle":"2022-08-03T20:28:58.397663Z","shell.execute_reply.started":"2022-08-03T20:28:58.377791Z","shell.execute_reply":"2022-08-03T20:28:58.396313Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# sc_type=\"rougeL\"\n# data_type=\"train1\"\n# train_data=pd.read_csv(os.path.join(df_dir,f\"{data_type}_data_{sc_type}_tid.csv\"))\n# # train_data[\"text_position\"] = train_data.groupby(\"text_id\").cumcount().add(1)\n# data_stats(train_data)\n# labels=train_data[\"rougeL\"].values\n\n# sc_type=\"rougeL\"\n# data_type=\"test\"\n# test_data=pd.read_csv(os.path.join(df_dir,f\"{data_type}_data_{sc_type}_tid.csv\"))\n# test_data[\"text_position\"] = test_data.groupby(\"text_id\").cumcount().add(1)\n# data_stats(test_data)\n# labels=test_data[\"rougeL\"].values","metadata":{"execution":{"iopub.status.busy":"2022-08-02T23:59:45.163539Z","iopub.execute_input":"2022-08-02T23:59:45.164850Z","iopub.status.idle":"2022-08-02T23:59:45.168883Z","shell.execute_reply.started":"2022-08-02T23:59:45.164804Z","shell.execute_reply":"2022-08-02T23:59:45.167989Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# summaries=pd.read_csv(\"/kaggle/input/summarizer-data/train_summaries.csv\")\n# train_len=train_data.groupby(\"text_id\").size().shape[0]\n\n# summaries=summaries.iloc[:train_len,:]\n# train_data_feats[\"summary\"]=summaries[\"summary\"]\n# train_data_feats[\"sentence\"]=train_data[\"sentence\"]\n# train_data_feats[\"text_id\"]=train_data[\"text_id\"]\n# train_data_feats[\"text_position\"]=train_data[\"text_position\"]\n\n# X=train_data_feats\n# y=pd.DataFrame(labels)\n\n\nscore_type=\"rougeL\"\ntest_data, test_data_feats, labels_test = load_dataset_test(df_dir, sc_type=score_type)\n\nsc_type=\"rougeL\"\ndata_type=\"test\"\n# test_data=pd.read_csv(os.path.join(df_dir,f\"{data_type}_data_{sc_type}.csv\"))\n# test_data[\"text_position\"] = test_data.groupby(\"text_id\").cumcount().add(1)\nlabels=test_data[\"rougeL\"].values\n\nsummaries=pd.read_csv(\"/kaggle/input/summarizer-data/test_data_rougeL_summaries_grouped.csv\")\nsummaries=pd.DataFrame(summaries[\"summary\"])\nsummaries[\"text_id\"]=summaries.index\n\n# summaries.to_csv(\"/kaggle/working/Data/DataFrames/test_summaries.csv\", header=True, index=False)\n# test_len=test_data.groupby(\"text_id\").size().shape[0]\n# summaries[\"text_id\"]=summaries.index\n# if \"summary\" not in summaries.columns:\n#     summaries[\"summary\"]=summaries[\"sum_clean\"]\n#     del summaries[\"sum_clean\"]\n\n# summaries=summaries.iloc[:dev_len,:]\n# dev_data_feats[\"summary\"]=summaries[\"sum_clean\"]\ntest_data_feats[\"sentence\"]=test_data[\"sentence\"]\ntest_data_feats[\"text_id\"]=test_data[\"text_id\"]\ntest_data_feats[\"text_position\"]=test_data[\"text_position\"]\n\nX=test_data_feats\ny=pd.DataFrame(labels_test)","metadata":{"execution":{"iopub.status.busy":"2022-08-03T21:47:19.936273Z","iopub.execute_input":"2022-08-03T21:47:19.937350Z","iopub.status.idle":"2022-08-03T21:47:23.717506Z","shell.execute_reply.started":"2022-08-03T21:47:19.937303Z","shell.execute_reply":"2022-08-03T21:47:23.716545Z"},"trusted":true},"execution_count":53,"outputs":[{"name":"stdout","text":"\nLoading Test Data . . \nDataFrame shape: (1037722, 5)\n\n\n            count      mean       std  min       25%       50%       75%  max\nchosen                                                                       \n0       1008495.0  0.109895  0.127409  0.0  0.051948  0.088889  0.128205  1.0\n1         29227.0  0.456524  0.300030  0.0  0.210927  0.389610  0.666667  1.0\n","output_type":"stream"}]},{"cell_type":"code","source":"# X.loc[X.text_id==10000,\"sentence\"].values, \"\\n\\n\", summaries.loc[10000,\"summary\"]","metadata":{"execution":{"iopub.status.busy":"2022-08-03T18:02:06.635935Z","iopub.execute_input":"2022-08-03T18:02:06.636342Z","iopub.status.idle":"2022-08-03T18:02:06.645844Z","shell.execute_reply.started":"2022-08-03T18:02:06.636310Z","shell.execute_reply":"2022-08-03T18:02:06.644960Z"},"trusted":true},"execution_count":240,"outputs":[{"execution_count":240,"output_type":"execute_result","data":{"text/plain":"(array(['updated 8:10 AM EDT, Tue July 29, 2014\\n\\nMike \"The Situation\" Sorrentino got into a fight inside a tanning salon he co-owns with his brother.',\n        '(CNN) -- Former \"Jersey Shore\" star Mike \"The Situation\" Sorrentino will get anger management counseling to settle an assault charge connected to a tanning salon fight with a brother, his lawyer said.',\n        'Sorrentino, 31, reached a plea deal with the Middletown, New Jersey, prosecutor allowing him to face a reduced municipal code charge if he completes 12 weeks of private counseling, attorney Edward Fradkin said.',\n        'The reality show star appeared in court Monday, when the judge approved the deal, Fradkin said.',\n        'He will return in three months and enter a plea to a non-criminal ordinance violation after the counseling, he said.',\n        'The fight happened inside a tanning salon Sorrentino co-owns with his brother.',\n        '\"The Situation\" told a photographer after his arrest that the fight happened because his brother \"was not running the store correctly, so I had to go in there and clean house, in more ways than one.\"',\n        'Sorrentino, who appeared in six seasons of the MTV reality series \"Jersey Shore,\" emerged from a stint in drug rehab two years ago declaring that he was a changed man.',\n        \"The Situation: I'm not the same person I was before\\n\\nCNN's Jane Caffrey contributed to this report.\"],\n       dtype=object),\n '\\n\\n',\n '\"Jersey Shore\" star Mike \"The Situation\" Sorrentino gets anger management counseling to settle an assault charge connected to a tanning salon fight.')"},"metadata":{}}]},{"cell_type":"code","source":"open(os.path.join(data_dir,\"summaResults.txt\"),\"w\")\n%cd /kaggle/working\nFileLinks(\".\")","metadata":{"execution":{"iopub.status.busy":"2022-08-03T18:15:06.591814Z","iopub.execute_input":"2022-08-03T18:15:06.592250Z","iopub.status.idle":"2022-08-03T18:15:06.605654Z","shell.execute_reply.started":"2022-08-03T18:15:06.592216Z","shell.execute_reply":"2022-08-03T18:15:06.604426Z"},"trusted":true},"execution_count":252,"outputs":[{"name":"stdout","text":"/kaggle/working\n","output_type":"stream"},{"execution_count":252,"output_type":"execute_result","data":{"text/plain":"./\n  __notebook_source__.ipynb\n./Data/\n  SGD_model.pkl\n  summaResults.txt\n./Data/DataFrames/\n  dev_summaries.csv\n  test_data_rougeL_tid.csv\n  dev_data_rougeL_summaries.csv\n  dev_data_rougeL_tid.csv\n  test_data_rougeL_summaries.csv\n  test_data_rougeL_summaries_grouped.csv\n  dev_data_rougeL.csv\n  test_data_rougeL.csv\n  test_set.csv\n  test_summaries.csv\n  dev_data_rougeL_summaries_grouped.csv\n  dev_set.csv","text/html":"./<br>\n&nbsp;&nbsp;<a href='./__notebook_source__.ipynb' target='_blank'>__notebook_source__.ipynb</a><br>\n./Data/<br>\n&nbsp;&nbsp;<a href='./Data/SGD_model.pkl' target='_blank'>SGD_model.pkl</a><br>\n&nbsp;&nbsp;<a href='./Data/summaResults.txt' target='_blank'>summaResults.txt</a><br>\n./Data/DataFrames/<br>\n&nbsp;&nbsp;<a href='./Data/DataFrames/dev_summaries.csv' target='_blank'>dev_summaries.csv</a><br>\n&nbsp;&nbsp;<a href='./Data/DataFrames/test_data_rougeL_tid.csv' target='_blank'>test_data_rougeL_tid.csv</a><br>\n&nbsp;&nbsp;<a href='./Data/DataFrames/dev_data_rougeL_summaries.csv' target='_blank'>dev_data_rougeL_summaries.csv</a><br>\n&nbsp;&nbsp;<a href='./Data/DataFrames/dev_data_rougeL_tid.csv' target='_blank'>dev_data_rougeL_tid.csv</a><br>\n&nbsp;&nbsp;<a href='./Data/DataFrames/test_data_rougeL_summaries.csv' target='_blank'>test_data_rougeL_summaries.csv</a><br>\n&nbsp;&nbsp;<a href='./Data/DataFrames/test_data_rougeL_summaries_grouped.csv' target='_blank'>test_data_rougeL_summaries_grouped.csv</a><br>\n&nbsp;&nbsp;<a href='./Data/DataFrames/dev_data_rougeL.csv' target='_blank'>dev_data_rougeL.csv</a><br>\n&nbsp;&nbsp;<a href='./Data/DataFrames/test_data_rougeL.csv' target='_blank'>test_data_rougeL.csv</a><br>\n&nbsp;&nbsp;<a href='./Data/DataFrames/test_set.csv' target='_blank'>test_set.csv</a><br>\n&nbsp;&nbsp;<a href='./Data/DataFrames/test_summaries.csv' target='_blank'>test_summaries.csv</a><br>\n&nbsp;&nbsp;<a href='./Data/DataFrames/dev_data_rougeL_summaries_grouped.csv' target='_blank'>dev_data_rougeL_summaries_grouped.csv</a><br>\n&nbsp;&nbsp;<a href='./Data/DataFrames/dev_set.csv' target='_blank'>dev_set.csv</a><br>"},"metadata":{}}]},{"cell_type":"code","source":"# %cd /kaggle/working\n# FileLinks(\".\")\n\ntot_r1=[]\ntot_r2=[]\ntot_rL=[]\n\nglobal doc_p_bar\ndoc_p_bar=tqdm(summaries[\"summary\"], desc=f\"{data_type} Set Summary Scoring\", bar_format=\"{postfix[0]}: {postfix[1]:.10f} | {postfix[2]}: {postfix[3]:.10f} | {postfix[4]}: {postfix[5]:.10f} ( {postfix[6][value]}/{postfix[7]} ) {elapsed}<{remaining}\", postfix=[\"Mean Rouge1\", np.mean(tot_r1), \"Mean Rouge2\", np.mean(tot_r2), \"Mean RougeL\", np.mean(tot_rL), dict(value=0) ,summaries.shape[0]], leave=True)\n\nopen(os.path.join(data_dir,\"summaResults.txt\"),\"w\")\n# FileLinks(\".\")\nthreshold=0.3\nFileLinks(\"\")\nscores=X.groupby(\"text_id\").apply(lambda x: doc_summary(model,x,summaries,int(x.text_id.values[0]),threshold) )\n\nfinal_res=f\"\\nMean Rouge1: {np.mean(r1)}\\nMean Rouge2: {np.mean(r2)}\\nMean RougeL: {np.mean(rL)}\\n\\n\"\nprint(final_res)\n\nwith open(os.path.join(data_dir,\"summaResults.txt\"), \"a\", encoding=\"utf-8\") as writer:\n        writer.write(\"\\n\\n---> Final Results\\n\\n\"+final_res)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-08-03T18:30:29.209049Z","iopub.execute_input":"2022-08-03T18:30:29.209775Z","iopub.status.idle":"2022-08-03T18:33:36.101441Z","shell.execute_reply.started":"2022-08-03T18:30:29.209735Z","shell.execute_reply":"2022-08-03T18:33:36.099787Z"},"trusted":true},"execution_count":258,"outputs":[{"name":"stderr","text":"Mean Rouge1: 0.4003270509 | Mean Rouge2: 0.3276676304 | Mean RougeL: 0.3823655083 ( 16/36166 ) 00:11<6:09:12\nMean Rouge1: 0.2427534269 | Mean Rouge2: 0.1389274289 | Mean RougeL: 0.2081940562 ( 516/36166 ) 03:06<3:34:12","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/groupby/groupby.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1274\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1275\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_python_apply_general\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_selected_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1276\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/groupby/groupby.py\u001b[0m in \u001b[0;36m_python_apply_general\u001b[0;34m(self, f, data)\u001b[0m\n\u001b[1;32m   1308\u001b[0m         \"\"\"\n\u001b[0;32m-> 1309\u001b[0;31m         \u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmutated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrouper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1310\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/groupby/ops.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, f, data, axis)\u001b[0m\n\u001b[1;32m    851\u001b[0m             \u001b[0mgroup_axes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 852\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    853\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_is_indexed_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup_axes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_17/3919624635.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mFileLinks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mscores\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"text_id\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdoc_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msummaries\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_id\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mthreshold\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_17/2102844157.py\u001b[0m in \u001b[0;36mdoc_summary\u001b[0;34m(model, document, ref_summaries, doc_id, th, s_num)\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0mpred_summary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocument\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ms_num\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m     \u001b[0mref_summary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mget_ref_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mref_summaries\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdoc_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m     \u001b[0mr1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mr2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_summary\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mref_summary\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdoc_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_17/2102844157.py\u001b[0m in \u001b[0;36mget_ref_summary\u001b[0;34m(ref_summaries, doc_id)\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdoc_id\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m517\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Doc id: #\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mdoc_id\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mref_sum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mref_sum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: can only concatenate str (not \"int\") to str","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_17/3919624635.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mFileLinks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mscores\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"text_id\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdoc_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msummaries\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_id\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mthreshold\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mfinal_res\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf\"\\nMean Rouge1: {np.mean(r1)}\\nMean Rouge2: {np.mean(r2)}\\nMean RougeL: {np.mean(rL)}\\n\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/groupby/groupby.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1285\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mgroup_selection_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1286\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_python_apply_general\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_selected_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1288\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/groupby/groupby.py\u001b[0m in \u001b[0;36m_python_apply_general\u001b[0;34m(self, f, data)\u001b[0m\n\u001b[1;32m   1307\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mapplying\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m         \"\"\"\n\u001b[0;32m-> 1309\u001b[0;31m         \u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmutated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrouper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1310\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1311\u001b[0m         return self._wrap_applied_output(\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/groupby/ops.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, f, data, axis)\u001b[0m\n\u001b[1;32m    813\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    814\u001b[0m                 \u001b[0msdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplitter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msorted_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 815\u001b[0;31m                 \u001b[0mresult_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmutated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplitter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfast_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    816\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    817\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/groupby/ops.py\u001b[0m in \u001b[0;36mfast_apply\u001b[0;34m(self, f, sdata, names)\u001b[0m\n\u001b[1;32m   1358\u001b[0m         \u001b[0;31m# must return keys::list, values::list, mutated::bool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1359\u001b[0m         \u001b[0mstarts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mends\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_slices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mslabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mngroups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1360\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlibreduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_frame_axis0\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstarts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mends\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1362\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_chop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msdata\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslice_obj\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/_libs/reduction.pyx\u001b[0m in \u001b[0;36mpandas._libs.reduction.apply_frame_axis0\u001b[0;34m()\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_17/3919624635.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mFileLinks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mscores\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"text_id\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdoc_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msummaries\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_id\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mthreshold\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mfinal_res\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf\"\\nMean Rouge1: {np.mean(r1)}\\nMean Rouge2: {np.mean(r2)}\\nMean RougeL: {np.mean(rL)}\\n\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5485\u001b[0m         ):\n\u001b[1;32m   5486\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5487\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5488\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5489\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'text_id'"],"ename":"AttributeError","evalue":"'DataFrame' object has no attribute 'text_id'","output_type":"error"}]},{"cell_type":"code","source":"cols=[\"len\", \"s_position\", \"thematic_ratio\", \"s_pos_par\", \"num_ratio\"]\nfeats=test_data_feats[cols]\npreds=model.predict(feats).tolist()","metadata":{"execution":{"iopub.status.busy":"2022-08-03T21:47:39.785163Z","iopub.execute_input":"2022-08-03T21:47:39.785581Z","iopub.status.idle":"2022-08-03T21:47:39.917511Z","shell.execute_reply.started":"2022-08-03T21:47:39.785543Z","shell.execute_reply":"2022-08-03T21:47:39.916261Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"code","source":"preds_df=pd.DataFrame({\"sentence\": test_data_feats[\"sentence\"], \"text_id\": test_data_feats[\"text_id\"], \"text_position\": test_data_feats[\"text_position\"], \"pred_rougeL\": preds})","metadata":{"execution":{"iopub.status.busy":"2022-08-03T21:47:45.461087Z","iopub.execute_input":"2022-08-03T21:47:45.461503Z","iopub.status.idle":"2022-08-03T21:47:45.605692Z","shell.execute_reply.started":"2022-08-03T21:47:45.461467Z","shell.execute_reply":"2022-08-03T21:47:45.604659Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"code","source":"threshold=0.3\ns_num_max=5\npred_sums=preds_df.groupby(\"text_id\").apply(lambda gr: create_summary(gr,s_num_max,threshold))","metadata":{"execution":{"iopub.status.busy":"2022-08-03T21:50:25.196483Z","iopub.execute_input":"2022-08-03T21:50:25.196896Z","iopub.status.idle":"2022-08-03T21:52:51.045343Z","shell.execute_reply.started":"2022-08-03T21:50:25.196862Z","shell.execute_reply":"2022-08-03T21:52:51.044105Z"},"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"code","source":"r_scorer=rouge_scorer.RougeScorer([\"rouge1\",\"rouge2\",\"rougeL\"], use_stemmer=True)\n\nscores=[ r_scorer.score(ref,pred) for ref,pred in zip(summaries[\"summary\"].values,pred_sums) ]","metadata":{"execution":{"iopub.status.busy":"2022-08-03T21:59:55.814809Z","iopub.execute_input":"2022-08-03T21:59:55.815877Z","iopub.status.idle":"2022-08-03T22:01:32.085812Z","shell.execute_reply.started":"2022-08-03T21:59:55.815822Z","shell.execute_reply":"2022-08-03T22:01:32.084649Z"},"trusted":true},"execution_count":76,"outputs":[]},{"cell_type":"code","source":"score_df=pd.DataFrame(scores)","metadata":{"execution":{"iopub.status.busy":"2022-08-03T22:01:32.087818Z","iopub.execute_input":"2022-08-03T22:01:32.088190Z","iopub.status.idle":"2022-08-03T22:01:32.130965Z","shell.execute_reply.started":"2022-08-03T22:01:32.088149Z","shell.execute_reply":"2022-08-03T22:01:32.130075Z"},"trusted":true},"execution_count":77,"outputs":[]},{"cell_type":"code","source":"score_df[\"ref_sum\"]=summaries[\"summary\"].values\nscore_df[\"pred_sum\"]=pred_sums\nscore_df[\"text_id\"]=score_df.index\n\nscore_df[\"rouge1_precision\"]=score_df[\"rouge1\"].apply(lambda x: x[0])\nscore_df[\"rouge1_recall\"]=score_df[\"rouge1\"].apply(lambda x: x[1])\nscore_df[\"rouge1_fmeasure\"]=score_df[\"rouge1\"].apply(lambda x: x[2])\ndel score_df[\"rouge1\"]\n\nscore_df[\"rouge2_precision\"]=score_df[\"rouge2\"].apply(lambda x: x[0])\nscore_df[\"rouge2_recall\"]=score_df[\"rouge2\"].apply(lambda x: x[1])\nscore_df[\"rouge2_fmeasure\"]=score_df[\"rouge2\"].apply(lambda x: x[2])\ndel score_df[\"rouge2\"]\n\nscore_df[\"rougeL_precision\"]=score_df[\"rougeL\"].apply(lambda x: x[0])\nscore_df[\"rougeL_recall\"]=score_df[\"rougeL\"].apply(lambda x: x[1])\nscore_df[\"rougeL_fmeasure\"]=score_df[\"rougeL\"].apply(lambda x: x[2])\ndel score_df[\"rougeL\"]","metadata":{"execution":{"iopub.status.busy":"2022-08-03T22:02:26.800185Z","iopub.execute_input":"2022-08-03T22:02:26.800615Z","iopub.status.idle":"2022-08-03T22:02:26.945635Z","shell.execute_reply.started":"2022-08-03T22:02:26.800580Z","shell.execute_reply":"2022-08-03T22:02:26.944559Z"},"trusted":true},"execution_count":80,"outputs":[]},{"cell_type":"code","source":"def log_to_file(row):\n    pred_summary = re.sub(r'\\n+', '', row['pred_sum'])\n    doc_str=f\"> Document #{row['text_id']}\\n\\n#Predicted Summary\\n{pred_summary}\\n\\n#Reference Summary\\n{row['ref_sum']}\\n\\nR1: {row['rouge1_fmeasure']}  |  R2: {row['rouge2_fmeasure']}  |  RL:  {row['rougeL_fmeasure']}\\n\\n\\n\\n\"\n    return doc_str\n\nfile_str=score_df.apply(lambda row: log_to_file(row), axis=1)\n\nwith open(os.path.join(data_dir,f\"summaResults_{data_type}.txt\"),\"w\",encoding=\"utf-8\") as log_writer:\n    for row_str in file_str:\n        log_writer.write(row_str)\n        \nFileLinks(\".\")","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-08-03T22:20:31.721124Z","iopub.execute_input":"2022-08-03T22:20:31.721548Z","iopub.status.idle":"2022-08-03T22:20:33.335092Z","shell.execute_reply.started":"2022-08-03T22:20:31.721516Z","shell.execute_reply":"2022-08-03T22:20:33.334118Z"},"trusted":true},"execution_count":101,"outputs":[]},{"cell_type":"code","source":"FileLinks(\".\")","metadata":{"execution":{"iopub.status.busy":"2022-08-03T22:20:57.682964Z","iopub.execute_input":"2022-08-03T22:20:57.684113Z","iopub.status.idle":"2022-08-03T22:20:57.692418Z","shell.execute_reply.started":"2022-08-03T22:20:57.684053Z","shell.execute_reply":"2022-08-03T22:20:57.691223Z"},"trusted":true},"execution_count":102,"outputs":[{"execution_count":102,"output_type":"execute_result","data":{"text/plain":"./\n  __notebook_source__.ipynb\n./Data/\n  summaResults_test.txt\n  test_set_feats.csv\n  SGD_model.pkl","text/html":"./<br>\n&nbsp;&nbsp;<a href='./__notebook_source__.ipynb' target='_blank'>__notebook_source__.ipynb</a><br>\n./Data/<br>\n&nbsp;&nbsp;<a href='./Data/summaResults_test.txt' target='_blank'>summaResults_test.txt</a><br>\n&nbsp;&nbsp;<a href='./Data/test_set_feats.csv' target='_blank'>test_set_feats.csv</a><br>\n&nbsp;&nbsp;<a href='./Data/SGD_model.pkl' target='_blank'>SGD_model.pkl</a><br>"},"metadata":{}}]},{"cell_type":"markdown","source":"___\n___\n# **Notes**\n\n\n1. scoring -> label rougeL -> sentence feats -> train\n2. grid search ?! (or manual fine tuning)\n3. test -> input doc -> predict score -> keep N first sentences or keep those over a threshold -> create summary -> calculate rouge1/2/L\n\n___\n### **References**\n\n1. [Named Entity Recognition (NER) with TensorflowNamed Entity Recognition (NER) with Tensorflow](https://www.kaggle.com/code/naseralqaydeh/named-entity-recognition-ner-with-tensorflow)\n2. [Extractive Summarization using Deep LearningExtractive Summarization using Deep Learning](https://arxiv.org/pdf/1708.04439v1.pdf)\n3. [NLTK](https://www.bogotobogo.com/python/NLTK/Stemming_NLTK.php)\n4. [Text Features Library](https://github.com/pmbaumgartner/text-feat-lib/tree/master/notebooks)\n5. []()\n\n\n\n### **Feats**\n1. [Feature extraction](https://arxiv.org/pdf/1708.04439v1.pdf)\n    1. Number of thematic words\n    2. Sentence position\n    3. Sentence length\n    4. Sentence position relative to paragraph\n    5. Number of proper nouns\n    6. Number of numerals\n    7. Number of named entities\n    8. Term Frequency-Inverse Sentence Frequency\n    9. Sentence to Centroid similarity\n    \n    \n2. [Text Summarization References](https://github.com/Tian312/awesome-text-summarization/blob/master/README.md)\n\n\n\n___\n### **Feature Base**\n\nThe feature base model extracts the features of the sentence, then evaluate its importance. Here is the representative research.\nSentence Extraction Based Single Document Summarization\nFollowing features are used in the above method.\n\n1. Position of the sentence in the input document\n2. Presence of the verb in the sentence\n3. Length of the sentence\n4. Term frequency\n5. Named entity tag NE\n6. Font style\n\netc. All the features are accumulated as the score.\nThe No.of coreferences are the number of pronouns to the previous sentence. It is simply calculated by counting the pronouns occurred in the first half of the sentence. So the Score represents the reference to the previous sentence.\nNow we can evaluate each sentence. Next is selecting the sentence to avoid the duplicate of the information. In this paper, the same word between the new and selected sentence is considered. And the refinement to connect the selected sentences are executed.\nLuhns Algorithm is also feature base. It evaluates the significance of the word that is calculated from the frequency.\nYou can try feature base text summarization by TextTeaser (PyTeaser is available for Python user).","metadata":{}},{"cell_type":"markdown","source":"# Unused","metadata":{}},{"cell_type":"code","source":"# train_set = f\"..{os.sep}Data{os.sep}release{os.sep}train.jsonl\"\n# dev_set = f\"..{os.sep}Data{os.sep}release{os.sep}dev.jsonl\"\n# test_set = f\"..{os.sep}Data{os.sep}release{os.sep}test.jsonl\"\n# load json files and convert them to dataframes to load faster next time\n# train_df = funs.json_to_df(train_set,\"train\")\n# dev_df = funs.json_to_df(dev_set,\"dev\")\n# test_df = funs.json_to_df(test_set,\"test\")\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# colab command to download the dataset\n# !kaggle datasets download -d tkylafi/summarizer-data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for items in os.walk(\"/kaggle/input\"):\n#     print(items)\n    \n\n# test_json=json.loads(test_json_path)","metadata":{"execution":{"iopub.status.busy":"2022-08-03T09:50:32.238730Z","iopub.execute_input":"2022-08-03T09:50:32.239653Z","iopub.status.idle":"2022-08-03T09:50:53.156609Z","shell.execute_reply.started":"2022-08-03T09:50:32.239611Z","shell.execute_reply":"2022-08-03T09:50:53.155512Z"},"trusted":true},"execution_count":27,"outputs":[{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"        index                                                url  \\\n0           1  http://www.nytimes.com/2006/06/05/technology/0...   \n1           2  http://www.nydailynews.com/archives/news/1995/...   \n2           4  http://www.reuters.com/article/2011/01/31/us-i...   \n3           5  http://www.reuters.com/article/2007/08/17/us-c...   \n4          10  http://www.bostonglobe.com/arts/music/2014/01/...   \n...       ...                                                ...   \n36367  108828  http://www.washingtonpost.com/wp-dyn/content/a...   \n36368  108829  http://www.washingtonpost.com/wp-dyn/content/d...   \n36369  108831  http://www.washingtonpost.com/wp-dyn/content/a...   \n36370  108834  http://www.washingtonpost.com/wp-dyn/content/a...   \n36371  108836  http://www.washingtonpost.com/wp-dyn/content/a...   \n\n                                                 archive  \\\n0      http://web.archive.org/web/20060620021852id_/h...   \n1      http://web.archive.org/web/20110210093603id_/h...   \n2      http://web.archive.org/web/20120321005702id_/h...   \n3      http://web.archive.org/web/20120606165550id_/h...   \n4      http://web.archive.org/web/20140131020936id_/h...   \n...                                                  ...   \n36367  https://web.archive.org/web/2005060119id_/http...   \n36368  https://web.archive.org/web/2005060119id_/http...   \n36369  https://web.archive.org/web/2005052419id_/http...   \n36370  https://web.archive.org/web/2005052019id_/http...   \n36371  https://web.archive.org/web/2005052019id_/http...   \n\n                                                   title            date  \\\n0      India Becoming a Crucial Cog in the Machine at...  20060620021852   \n1              NEW YORKERS' ONLY REGRET WAS STAYING HOME  20110210093603   \n2       Freed American hiker summoned back by Iran court  20120321005702   \n3          Breast cancer vaccine looks safe, study shows  20120606165550   \n4          Music review: Jake Bugg at the House of Blues  20140131020936   \n...                                                  ...             ...   \n36367             Prisoners' Religious Rights Law Upheld      2005060119   \n36368                               Arthur Andersen Case      2005060119   \n36369  Corcoran Director Quits; Trustees Shelve Gehry...      2005052419   \n36370                 In L.A., a Pol for a Polyglot City      2005052019   \n36371  Montgomery Mother's Stand On Sex-Ed Begins at ...      2005052019   \n\n                                                    text  \\\n0      BANGALORE, India, June 4  The world's biggest...   \n1      This story was reported by: NICK CHARLES, AUST...   \n2      TEHRAN | Mon Jan 31, 2011 9:17am EST\\n\\nTEHRAN...   \n3      By Maggie Fox, Health and Science Editor\\n\\nWA...   \n4      As the lights went down at the nearly sold-out...   \n...                                                  ...   \n36367  The Supreme Court upheld a federal religious f...   \n36368  The Supreme Court's unanimous rebuff of the 20...   \n36369  David C. Levy resigned yesterday as president ...   \n36370  LOS ANGELES -- The politics of the great Ameri...   \n36371  Michelle Turner, mother of four public school ...   \n\n                                                 summary  compression  \\\n0      India provides I.B.M. with its fastest-growing...    56.045455   \n1      As many black men marched on Washington yester...     6.152941   \n2      TEHRAN (Reuters) - An American woman who was f...     7.902439   \n3      WASHINGTON (Reuters) - A vaccine designed to t...    17.162162   \n4      As the lights went down at the nearly sold-out...     2.153061   \n...                                                  ...          ...   \n36367  The Supreme Court upheld a federal religious f...    13.605263   \n36368  The Supreme Court's unanimous rebuff of the 20...    62.765957   \n36369  David C. Levy resigned yesterday as president ...    15.000000   \n36370  LOS ANGELES -- The politics of the great Ameri...    17.905660   \n36371  Michelle Turner, mother of four public school ...    16.142857   \n\n       coverage    density compression_bin coverage_bin density_bin  \n0      0.954545  16.500000            high         high  extractive  \n1      0.976471  24.600000             low         high  extractive  \n2      1.000000  39.048780             low         high  extractive  \n3      1.000000  35.054054          medium         high  extractive  \n4      0.994898  55.658163             low         high  extractive  \n...         ...        ...             ...          ...         ...  \n36367  1.000000  38.000000             low         high  extractive  \n36368  1.000000  47.000000            high         high  extractive  \n36369  1.000000  48.000000             low         high  extractive  \n36370  1.000000  53.000000          medium         high  extractive  \n36371  1.000000  49.000000          medium         high  extractive  \n\n[36372 rows x 13 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>index</th>\n      <th>url</th>\n      <th>archive</th>\n      <th>title</th>\n      <th>date</th>\n      <th>text</th>\n      <th>summary</th>\n      <th>compression</th>\n      <th>coverage</th>\n      <th>density</th>\n      <th>compression_bin</th>\n      <th>coverage_bin</th>\n      <th>density_bin</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>http://www.nytimes.com/2006/06/05/technology/0...</td>\n      <td>http://web.archive.org/web/20060620021852id_/h...</td>\n      <td>India Becoming a Crucial Cog in the Machine at...</td>\n      <td>20060620021852</td>\n      <td>BANGALORE, India, June 4  The world's biggest...</td>\n      <td>India provides I.B.M. with its fastest-growing...</td>\n      <td>56.045455</td>\n      <td>0.954545</td>\n      <td>16.500000</td>\n      <td>high</td>\n      <td>high</td>\n      <td>extractive</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>http://www.nydailynews.com/archives/news/1995/...</td>\n      <td>http://web.archive.org/web/20110210093603id_/h...</td>\n      <td>NEW YORKERS' ONLY REGRET WAS STAYING HOME</td>\n      <td>20110210093603</td>\n      <td>This story was reported by: NICK CHARLES, AUST...</td>\n      <td>As many black men marched on Washington yester...</td>\n      <td>6.152941</td>\n      <td>0.976471</td>\n      <td>24.600000</td>\n      <td>low</td>\n      <td>high</td>\n      <td>extractive</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4</td>\n      <td>http://www.reuters.com/article/2011/01/31/us-i...</td>\n      <td>http://web.archive.org/web/20120321005702id_/h...</td>\n      <td>Freed American hiker summoned back by Iran court</td>\n      <td>20120321005702</td>\n      <td>TEHRAN | Mon Jan 31, 2011 9:17am EST\\n\\nTEHRAN...</td>\n      <td>TEHRAN (Reuters) - An American woman who was f...</td>\n      <td>7.902439</td>\n      <td>1.000000</td>\n      <td>39.048780</td>\n      <td>low</td>\n      <td>high</td>\n      <td>extractive</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>5</td>\n      <td>http://www.reuters.com/article/2007/08/17/us-c...</td>\n      <td>http://web.archive.org/web/20120606165550id_/h...</td>\n      <td>Breast cancer vaccine looks safe, study shows</td>\n      <td>20120606165550</td>\n      <td>By Maggie Fox, Health and Science Editor\\n\\nWA...</td>\n      <td>WASHINGTON (Reuters) - A vaccine designed to t...</td>\n      <td>17.162162</td>\n      <td>1.000000</td>\n      <td>35.054054</td>\n      <td>medium</td>\n      <td>high</td>\n      <td>extractive</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>10</td>\n      <td>http://www.bostonglobe.com/arts/music/2014/01/...</td>\n      <td>http://web.archive.org/web/20140131020936id_/h...</td>\n      <td>Music review: Jake Bugg at the House of Blues</td>\n      <td>20140131020936</td>\n      <td>As the lights went down at the nearly sold-out...</td>\n      <td>As the lights went down at the nearly sold-out...</td>\n      <td>2.153061</td>\n      <td>0.994898</td>\n      <td>55.658163</td>\n      <td>low</td>\n      <td>high</td>\n      <td>extractive</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>36367</th>\n      <td>108828</td>\n      <td>http://www.washingtonpost.com/wp-dyn/content/a...</td>\n      <td>https://web.archive.org/web/2005060119id_/http...</td>\n      <td>Prisoners' Religious Rights Law Upheld</td>\n      <td>2005060119</td>\n      <td>The Supreme Court upheld a federal religious f...</td>\n      <td>The Supreme Court upheld a federal religious f...</td>\n      <td>13.605263</td>\n      <td>1.000000</td>\n      <td>38.000000</td>\n      <td>low</td>\n      <td>high</td>\n      <td>extractive</td>\n    </tr>\n    <tr>\n      <th>36368</th>\n      <td>108829</td>\n      <td>http://www.washingtonpost.com/wp-dyn/content/d...</td>\n      <td>https://web.archive.org/web/2005060119id_/http...</td>\n      <td>Arthur Andersen Case</td>\n      <td>2005060119</td>\n      <td>The Supreme Court's unanimous rebuff of the 20...</td>\n      <td>The Supreme Court's unanimous rebuff of the 20...</td>\n      <td>62.765957</td>\n      <td>1.000000</td>\n      <td>47.000000</td>\n      <td>high</td>\n      <td>high</td>\n      <td>extractive</td>\n    </tr>\n    <tr>\n      <th>36369</th>\n      <td>108831</td>\n      <td>http://www.washingtonpost.com/wp-dyn/content/a...</td>\n      <td>https://web.archive.org/web/2005052419id_/http...</td>\n      <td>Corcoran Director Quits; Trustees Shelve Gehry...</td>\n      <td>2005052419</td>\n      <td>David C. Levy resigned yesterday as president ...</td>\n      <td>David C. Levy resigned yesterday as president ...</td>\n      <td>15.000000</td>\n      <td>1.000000</td>\n      <td>48.000000</td>\n      <td>low</td>\n      <td>high</td>\n      <td>extractive</td>\n    </tr>\n    <tr>\n      <th>36370</th>\n      <td>108834</td>\n      <td>http://www.washingtonpost.com/wp-dyn/content/a...</td>\n      <td>https://web.archive.org/web/2005052019id_/http...</td>\n      <td>In L.A., a Pol for a Polyglot City</td>\n      <td>2005052019</td>\n      <td>LOS ANGELES -- The politics of the great Ameri...</td>\n      <td>LOS ANGELES -- The politics of the great Ameri...</td>\n      <td>17.905660</td>\n      <td>1.000000</td>\n      <td>53.000000</td>\n      <td>medium</td>\n      <td>high</td>\n      <td>extractive</td>\n    </tr>\n    <tr>\n      <th>36371</th>\n      <td>108836</td>\n      <td>http://www.washingtonpost.com/wp-dyn/content/a...</td>\n      <td>https://web.archive.org/web/2005052019id_/http...</td>\n      <td>Montgomery Mother's Stand On Sex-Ed Begins at ...</td>\n      <td>2005052019</td>\n      <td>Michelle Turner, mother of four public school ...</td>\n      <td>Michelle Turner, mother of four public school ...</td>\n      <td>16.142857</td>\n      <td>1.000000</td>\n      <td>49.000000</td>\n      <td>medium</td>\n      <td>high</td>\n      <td>extractive</td>\n    </tr>\n  </tbody>\n</table>\n<p>36372 rows  13 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}